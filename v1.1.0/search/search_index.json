{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the MKE 4k docs","text":""},{"location":"#mke-4-bingular","title":"MKE 4 Bingular","text":"<p>The Mirantis Kubernetes Engine 4k documentation set is provided to help system administrators and DevOps professionals to configure, install, and operate MKE 4k, covering key concepts and functionalities.</p> <p>Like the system software it seeks to represent, the MKE 4k documentation is constantly evolving. As such, feedback on the comprehensiveness and quality of the content herein is both welcome and essential.</p> <p> </p> <p>{{&lt; hextra/hero-button   text=\"File an issue for MKE 4k documentation\"   style=\"background-color: #214666;\"   link=\"https://github.com/Mirantis/mke-docs/issues/new\"</p> <p>}}</p>"},{"location":"oss-download/","title":"Open Source Components and Licenses","text":"<p>Click any product component license below to download a text file of that license to your local system.</p> <ul> <li>Mirantis Kubernetes Engine License Report</li> <li>Mirantis Secure Registry FrontEnd License Report</li> <li>Mirantis Secure Registry BackEnd License Report</li> <li>Mirantis Container Runtime License Report</li> <li>Mirantis Container Runtime CLI License Report</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>This section delves into the foundational concepts that underpin the architecture and configuration of MKE. Understanding these concepts is important for effectively deploying, managing, and scaling applications within a Kubernetes environment.</p> <p>{{&lt; cards &gt;}}   {{&lt; card link=\"architecture\" title=\"Architecture\" icon=\"cube\" &gt;}}   {{&lt; card link=\"configuration\" title=\"Configurations\" icon=\"adjustments\" &gt;}}   {{&lt; card link=\"k0rdent-templates\" title=\"k0rdent templates\" icon=\"template\" &gt;}}   {{&lt; card link=\"cni\" title=\"Container Network Interface (CNI)\" icon=\"cube-transparent\" &gt;}} {{&lt; /cards &gt;}}</p>"},{"location":"concepts/architecture/","title":"Architecture","text":"<p>Mirantis Kubernetes Engine (MKE) 4k is an enterprise-grade, production-ready Kubernetes platform that is designed to be secure, scalable, and reliable.</p> <p>You can manage the entire MKE 4k cluster through the mke4.yaml configuration file. Refer to Configuration for details.</p>"},{"location":"concepts/architecture/#components","title":"Components","text":"<p>The following components constitute an MKE 4k cluster:</p> <ul> <li>k0s Kubernetes distribution</li> <li>Container Network Interface (CNI)</li> <li>MKE 4k Control Plane</li> <li><code>mke-operator</code></li> <li><code>kordent</code></li> <li>System services</li> <li>User services</li> </ul>"},{"location":"concepts/architecture/#k0s-kubernetes-distribution","title":"k0s Kubernetes distribution","text":"<p>k0s is a lightweight and open-source Kubernetes distribution that acts as the foundational runtime for MKE 4k clusters. For more information, refer to the k0s documentation.</p>"},{"location":"concepts/architecture/#container-network-interface-cni","title":"Container Network Interface (CNI)","text":"<p>MKE 4k supports Container Network Interface (CNI) plugins. Such components provide networking functionality within a cluster that is secure, scalable, and policy-driven.</p> <p>MKE 4k installs with the Calico CNI by default. Other CNI options, though, are also available. Refer to Container Network Interface for the network configuration details.</p>"},{"location":"concepts/architecture/#mke-4k-control-plane","title":"MKE 4k Control Plane","text":"<p>The MKE 4k Control Plane, which comprises <code>mke-operator</code> and <code>kordent</code>, is used to orchestrate, manage, and maintain the lifecycle of the MKE 4k cluster and services.</p> <ul> <li><code>mke-operator</code>: A Kubernetes-native operator that automates installation,   upgrades, system configuration, and user services.</li> <li><code>k0rdent</code>: An open-source component that performs lifecycle management for   the services that <code>mke-operator</code> configures.</li> </ul>"},{"location":"concepts/architecture/#system-services","title":"System services","text":"<p>A key benefit of MKE 4k is that it includes various services that are typically standard to a cluster, thus ensuring that they integrate well and are easy to use and configure. These services are Kubernetes workloads that run within the cluster, which you can manage using the MKE 4k control plane. They include authorization, ingress, monitoring, and backups, among others.</p>"},{"location":"concepts/architecture/#user-services","title":"User Services","text":"<p>Typically, user services are user-defined applications that run within the MKE 4k cluster, which you manage using the MKE 4k control plane.</p>"},{"location":"concepts/architecture/#architecture-diagram","title":"Architecture Diagram","text":"\u00d7"},{"location":"concepts/cni/","title":"Container Network Interfaces","text":"<p>MKE 4k supports Container Network Interface (CNI) plugins to enable the networking functionalities needed for container communication and management within a cluster. These plugins are responsible for setting up, modifying, and tearing down network interfaces when containers are created or removed. They provide MKE 4k with a wide range of networking functionalities, including IP address management, routing, NAT, and network isolation.</p> <p>CNI plugins that MKE 4k currently supports include:</p> <ul> <li>Calico OSS</li> </ul> <p>{{&lt; callout type=\"important\" &gt;}}</p> <p>MKE 4k supports upgrades from MKE 3 clusters that are using Calico OSS or an unmanaged CNI. For more information, refer to Upgrade from MKE 3.7 or 3.8.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"concepts/configuration/","title":"Configuration","text":"<p>The Mirantis Kubernetes Engine (MKE) 4k configuration file contains an opinionated configuration on how to set up an MKE 4k cluster.</p> <p>With the <code>mke4.yaml</code> configuration file, you can:</p> <ul> <li>Define the number of nodes in the cluster.</li> <li>Define ways to access the nodes.</li> <li>Enable or disable certain MKE 4k components.</li> <li>Configure MKE 4k component features</li> </ul> <p>Once set, the <code>mke4.yaml</code> configuration file is translated into a more complex k0rdent template that contains the granular details on how to set up the cluster.</p>"},{"location":"concepts/configuration/#create-configuration","title":"Create configuration","text":"<ol> <li> <p>Generate the default <code>mke4.yaml</code> configuration file by running:</p> <pre><code>mkectl init &gt; mke4.yaml\n</code></pre> </li> <li> <p>Modify the <code>hosts</code> section of the <code>mke4.yaml</code> configuration file, to apply    the configuration to a set of pre-existing machines that you have set up in    advance:</p> <pre><code>hosts:\n- ssh:\n    address: 18.224.23.158\n    keyPath: \"/absolute/path/to/private/key.pem\"\n    port: 22\n    user: root\n  role: controller+worker\n- ssh:\n    address: 18.224.23.158\n    keyPath: \"/absolute/path/to/private/key.pem\"\n    port: 22\n    user: ubuntu\n  role: worker\n- ssh:\n    address: 18.117.87.45\n    keyPath: \"/absolute/path/to/private/key.pem\"\n    port: 22\n    user: ubuntu\n  role: worker\n</code></pre> </li> </ol>"},{"location":"concepts/configuration/#choose-services","title":"Choose services","text":"<p>A core component of MKE 4k is a default set of curated and tested services that you can install by running <code>mkectl init</code> and subsequently applying the generated configuration file.</p> <p>Using the <code>mke4.yaml</code> configuration file, you can enable and disable a number of the available MKE 4k services, as well as modify the settings of those services.</p>"},{"location":"concepts/k0rdent-templates/","title":"k0rdent templates","text":"<p>{{&lt; callout type=\"info\" &gt;}}</p> <p>k0rdent is a Mirantis-initiated open source project with which platform engineers can build, automate, and manage Kubernetes platforms at scale. MKE 4k is adopting k0rdent technology to enable installation and management of services and components in a template-driven manner. k0rdent is available in MKE 4k as a technical preview only.</p> <p>For more information on MKE 4k and k0rdent, contact Mirantis Sales.</p> <p>{{&lt; /callout &gt;}}</p> <p>k0rdent templates are re-usable text definitions of components that you can use to install services. These templates provide a declarative way for you to install and manage the lifecycles of components while greatly reducing the number of parameters that require hands-on configuration.</p>"},{"location":"concepts/k0rdent-templates/#benefits","title":"Benefits","text":"<p>k0rdent templates are:</p> <ul> <li> <p>Human readable and editable: k0rdent templates use YAML as an abstraction to represent the target state.</p> </li> <li> <p>Usable in multiple contexts by way of runtime parameterization: Through the use of placeholders, you can customize templates at runtime without having to directly edit the template.</p> </li> <li> <p>Capable of limited scope deployment: You can set restrictions over which k0rdent templates can be deployed and by whom. For example, a platform manager can configure a template so that non-admin users can only execute templates that deploy a particular set of controllers.</p> </li> </ul>"},{"location":"configuration/_index/","title":"Configuration","text":"<p>The information herein is designed to guide you in setting up, maintaining, and running MKE 4k.</p> <p>{{&lt; cards &gt;}} {{&lt; card link=\"authentication\" title=\"Authentication\" icon=\"cog\" &gt;}} {{&lt; card link=\"backup-restore\" title=\"Backup and restore\" icon=\"cog\" &gt;}} {{&lt; card link=\"kubernetes\" title=\"Kubernetes Components\" icon=\"cog\" &gt;}} {{&lt; card link=\"ingress\" title=\"Ingress controller\" icon=\"cog\" &gt;}} {{&lt; card link=\"metallb\" title=\"MetalLB load balancer\" icon=\"cog\" &gt;}} {{&lt; card link=\"add-services\" title=\"Add services\" icon=\"cog\" &gt;}} {{&lt; card link=\"monitoring\" title=\"Monitoring\" icon=\"cog\" &gt;}} {{&lt; card link=\"k0rdent-capi-providers\" title=\"k0rdent CAPI providers\" icon=\"cog\" &gt;}} {{&lt; card link=\"support-bundle\" title=\"Support bundle\" icon=\"cog\" &gt;}} {{&lt; card link=\"telemetry\" title=\"Telemetry\" icon=\"cog\" &gt;}} {{&lt; card link=\"coredns-lameduck\" title=\"CoreDNS Lameduck\" icon=\"cog\" &gt;}} {{&lt; card link=\"mke4k-dashboard\" title=\"MKE 4k Dashboard\" icon=\"cog\" &gt;}} {{&lt; card link=\"nvidia-gpu\" title=\"NVIDIA GPU Workloads\" icon=\"cog\"&gt;}} {{&lt; card link=\"policycontroller\" title=\"Policy Controller\" icon=\"cog\" &gt;}} {{&lt; card link=\"node-feature-discovery\" title=\"Node Feature Discovery (NFD)\" icon=\"cog\" &gt;}} {{&lt; card link=\"cloudproviders\" title=\"Cloud Providers\" icon=\"cog\" &gt;}}</p> <p>{{&lt; card link=\"multus\" title=\"Multus\" icon=\"cog\" &gt;}} {{&lt; card link=\"configuration-drift-detection\" title=\"Configuration Drift Detection\" icon=\"cog\" &gt;}} {{&lt; card link=\"container-network-interface\" title=\"Container Network Interface\" icon=\"cog\" &gt;}} {{&lt; /cards &gt;}}</p>"},{"location":"configuration/add-services/","title":"Add services","text":"<p>{{&lt; callout type=\"info\" &gt;}}</p> <p>k0rdent is a Mirantis-initiated open source project that enables lifecycle management for services on MKE 4k clusters.</p> <p>{{&lt; /callout &gt;}}</p> <p>Through the use of the <code>services</code> section of the <code>mke4.yaml</code> configuration file, you can add additional components to your MKE 4k installation. This section will pass the input values through to k0rdent, following the typical k0rdent services format.</p> <p>Example:</p> <pre><code>services:\n   - template: project-ingress-nginx-4.11.0\n     name: ingress-nginx\n     namespace: ingress\n</code></pre> <p>To learn more about the services sanctioned by k0rdent, refer to the k0rdent catalog.</p>"},{"location":"configuration/configuration-drift-detection/","title":"Configuration Drift Detection","text":"<p>Configuration drift is when operating environments deviate from a baseline or standard configuration over time, causing these environments to become inconsistent and unpredictable.</p> <p>In MKE 4k clusters, drift detection is enabled by default for system services and services that are specified in the cluster configuration. When any change is detected in a specified resource, such as a label or specification, the service state is automatically synced back to its original state as described in the <code>mke4.yaml</code> configuration file.</p> <p>By design, you cannot fully disable configuration drift detection, as doing so could cause your MKE 4k cluster to become unstable. You can, however, configure the system to ignore certain resources and specify certain resource fields for exclusion from the detection process.</p>"},{"location":"configuration/configuration-drift-detection/#ignore-specified-resources","title":"Ignore specified resources","text":"<p>To stop certain resources from being tracked for configuration drift, add the drift ignore patch selectors to the MKE 4k configuration. For example, the following configuration will disable drift detection for the <code>ingress-nginx-controller</code> deployment object in <code>ingress-nginx</code> namespace:</p> <pre><code>spec:\n  driftDetection:\n    driftIgnore:\n      - group: apps\n        version: v1\n        kind: Deployment\n        name: ingress-nginx-controller\n        namespace: ingress-nginx\n</code></pre> <p>Thereafter, any changes made to resources deployed by MKE 4k itself will be flagged as a configuration drift. Any modifications made directly to the <code>ingress-nginx-controller</code> deployment, however, will not be detected as drift.</p> <p>All patch selector parameters are optional, and you can specify any or all of them to indicate which resources drift detection is to ignore.</p> <p>The patch selector parameters are detailed in the following table:</p> Parameter Description <code>group</code> API group from which to select resources. <code>version</code> Version of the API group from which to select resources. <code>kind</code> Type of API group from which to select resources. <code>name</code> Name with which to match resources. <code>namespace</code> The namespace from which to select resources. <code>annotationSelector</code> A string that follows the label selection expression, which matches with resource annotations. <code>labelSelector</code> A string that follows the label selection expression, which matches with resource labels. <p>{{&lt; callout type=\"warning\" &gt;}} You cannot filter resources by service using patch selector parameters. Thus, if you decide to disable drift detection for specific resources, ensure that the patch selector is set to filter only required cluster resources. {{&lt; /callout &gt;}}</p>"},{"location":"configuration/configuration-drift-detection/#exclude-specified-fields","title":"Exclude specified fields","text":"<p>In addition to setting drift detection to ignore specified resources, you can also use JSON Pointers to specify that only certain fields be excluded from drift detection.</p> <p>Example MKE 4k configuration:</p> <pre><code>spec:\n  driftDetection:\n    driftExclusions:\n      - paths:\n           - \"/spec/replicas\"\n        target:\n          group: apps\n          version: v1\n          kind: Deployment\n          name: ingress-nginx-controller\n          namespace: ingress-nginx\n</code></pre>"},{"location":"configuration/coredns-lameduck/","title":"CoreDNS Lameduck","text":"<p>Adding lameduck to the health plugin minimizes DNS resolution failures during a CoreDNS pod restart or deployment rollout.  Mirantis Kubernetes Engine (MKE) 4k supports enabling lameduck for the default server block.</p>"},{"location":"configuration/coredns-lameduck/#configuration","title":"Configuration","text":"<p>CoreDNS lameduck support is disabled by default. To enable lameduck, configure the <code>lameduck</code> section of the <code>mke4.yaml</code> configuration file under <code>dns</code>:</p> <pre><code>  dns:\n    lameduck:\n      enabled: true\n      duration: \"7s\"\n</code></pre> <p>Configuration parameters</p> Field Description Default enabled Enables the lameduck health function.  Valid values: true, false. false duration Length of time during which lameduck will run, expressed with integers and time suffixes, such as s for seconds and m for minutes. 7s <p> MKE 4k does not support the use of the <code>kubectl edit</code> command to edit the CoreDNS config map outside of MKE 4k for the purpose of configuring the lameduck function. Any such changes will be overwritten by the values set in the <code>mke4.yaml</code> configuration file whenever you execute the <code>mkectl apply</code> command.</p>"},{"location":"configuration/coredns-lameduck/#applying-configuration","title":"Applying configuration","text":"<ol> <li>Enable or adjust the lameduck configuration.</li> <li>Wait for the CoreDNS pods to apply the changes.</li> <li>Check the CoreDNS logs to verify that the Pod is running and that it has    applied the configuration without any errors.</li> </ol> <pre><code>kubectl logs -f deployment/coredns -n kube-system\n</code></pre> <p>Example output:</p> <p><pre><code>Found 2 pods, using pod/coredns-5d78c9869d-7qfnd\n.:53\n[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908\nCoreDNS-1.10.1\ngo1.20, 055b2c3\n\n[INFO] Reloading\n[INFO] plugin/reload: Running configuration SHA512 = 26fe33ee13757f04c8c9a1caebd7c6f0614306c92089ea215f1a8663f95ff1e673d4fa5de544b31492231923d4679370ce8735823ce3b5e65e5c23a9029c4512\n[INFO] Reloading complete\n</code></pre> 4. View the ConfigMap to verify the configuration that was applied:</p> <pre><code>kubectl describe configmap coredns -n kube-system\n</code></pre> <p>Example output:</p> <pre><code>Name:         coredns\nNamespace:    kube-system\nLabels:       k0s.k0sproject.io/stack=coredns\nAnnotations:  k0s.k0sproject.io/last-applied-configuration:\n                {\"apiVersion\":\"v1\",\"data\":{\"Corefile\":\".:53 {\\n    errors\\n    health\\n    ready\\n    kubernetes cluster.local in-addr.arpa ip6.arpa {\\n  ...\n              k0s.k0sproject.io/stack-checksum: df0a18b174e12f166824f894f447d08f\n\nData\n====\nCorefile:\n----\n.:53 {\n  cache 30\n  errors\n  forward . /etc/resolv.conf\n  health {\n    lameduck 7s\n  }\n  kubernetes cluster.local in-addr.arpa ip6.arpa {\n    pods insecure\n    ttl 30\n    fallthrough in-addr.arpa ip6.arpa\n  }\n  loadbalance\n  loop\n  prometheus :9153\n  ready\n  reload\n}\n\n\nBinaryData\n====\n\nEvents:  &lt;none&gt;\n</code></pre>"},{"location":"configuration/coredns-lameduck/#lameduck-configuration-parameters","title":"Lameduck configuration parameters","text":"Parameter Description dns.lameduck.enabled Set to <code>true</code> to enable lameduck functionality. dns.lameduck.duration Amount of time in which to delay the shutdown of the CoreDNS Pod."},{"location":"configuration/k0rdent-capi-providers/","title":"k0rdent CAPI providers","text":"<p>{{&lt; callout type=\"info\" &gt;}}</p> <p>k0rdent is a Mirantis-initiated open source project with which platform engineers can build, automate, and manage Kubernetes platforms at scale. MKE 4k is adopting k0rdent technology to enable installation and management of services and components in a template-driven manner. k0rdent is available in MKE 4k as a technical preview only.</p> <p>For more information on MKE 4k and k0rdent, contact Mirantis Sales.</p> <p>{{&lt; /callout &gt;}}</p> <p>k0rdent CAPI (Cluster API) providers allow for the creation and management of child clusters directly in popular infrastructures.</p> <p>Most k0rdent CAPI providers are disabled in MKE 4k by default. To enable these providers, create a <code>k0rdent.providers</code> section in the <code>mke4.yaml</code> configuration file and add them to it. The CAPI providers use the same strings/names as those found in the k0rdent product.</p> <p>Example:</p> <pre><code>k0rdent:\n  enabled: true\n  providers:\n      - name: cluster-api-provider-azure\n</code></pre> <p>The offline configuration differs somewhat. </p> Supported k0rdent CAPI providers with additional airgap-specific settings  {{&lt; callout type=\"important\" &gt;}}  For the providers detailed herein, `/` must be the same registry and path to which you uploaded the offline bundle when [preparing your offline environment](../../getting-started/offline-installation/#preparation).  {{&lt; /callout &gt;}}   <pre><code>k0rdent:\n  enabled: true\n  providers:\n    - name: k0smotron-bootstrap\n      config:\n        images:\n          k0smotronManager:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n          kubeRbacProxyKubeRbacProxy:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n    - name: k0smotron-control-plane\n      config:\n        images:\n          k0smotronManager:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n          kubeRbacProxyKubeRbacProxy:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n    - name: k0smotron-infrastructure\n      config:\n        images:\n          k0smotronManager:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n          kubeRbacProxyKubeRbacProxy:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n    - name: cluster-api-provider-aws\n      config:\n        images:\n          clusterApiAwsControllerManager:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n    - name: cluster-api-provider-azure\n      config:\n        images:\n          azureserviceoperatorManager:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n          clusterApiAzureControllerManager:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n    - name: cluster-api-provider-openstack\n      config:\n        images:\n          capiOpenstackControllerManager:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n    - name: cluster-api-provider-vsphere\n      config:\n        images:\n          clusterApiVsphereControllerManager:\n            repo: &lt;registry-address&gt;/&lt;registry-project-path&gt;\n</code></pre> <p>Currently, all CAPI providers are installed at cluster creation; however, once the cluster is up and running, these providers are disabled. A number of CAPI providers are MKE 4k dependencies and are thus always added and enabled, including Sveltos, CAPI, KCM, and mke-operator.</p> <p>For information on each of the k0rdent CAPI providers, refer to the official k0rdent documentation Prepare k0rdent to create child clusters on multiple providers.</p>"},{"location":"configuration/mke4k-dashboard/","title":"MKE 4k Dashboard","text":"<p>The MKE 4k Dashboard add-on provides a web UI that you can use to manage Kubernetes resources:</p> <p></p> \u00d7 <p>To access the MKE 4k Dashboard, which is enabled by default, navigate to the address of the load balancer endpoint from a freshly installed cluster. Refer to Load balancer requirements for detailed information.</p>"},{"location":"configuration/monitoring/","title":"Monitoring","text":"<p>The MKE 4k monitoring setup is based on the kube-prometheus-stack, offering a comprehensive solution for collecting, storing, and visualizing metrics.</p>"},{"location":"configuration/monitoring/#monitoring-tools","title":"Monitoring tools","text":"<p>Detail for the MKE 4k monitor tools is provided in the following table:</p> Monitoring tool Default state Configuration key Description Prometheus enabled - Collects and stores metrics Grafana enabled <code>monitoring.enableGrafana</code> Provides a web interface for viewing metrics and logs collected by Prometheus cAdvisor disabled <code>monitoring.enableCAdvisor</code> Provides additional container level metrics OpsCare disabled <code>monitoring.enableOpscare</code> (Under development) Supplies additional monitoring capabilities, such as Alertmanager"},{"location":"configuration/monitoring/#prometheus","title":"Prometheus","text":"<p>Prometheus is an open-source monitoring and alerting toolkit, designed for reliability and scalability, that collects and stores metrics as time series data. It offers powerful query capabilities and a flexible alerting system.</p> <p>The Prometheus API is available at <code>https://&lt;mke4_url&gt;/prometheus/</code></p> <p>To access the Prometheus dashboard:</p> <ol> <li> <p>Port forward Prometheus:</p> <pre><code>kubectl --namespace mke port-forward svc/prometheus-operated 9090\n</code></pre> </li> <li> <p>Navigate to <code>http://localhost:9090</code>.</p> </li> </ol>"},{"location":"configuration/monitoring/#grafana","title":"Grafana","text":"<p>Grafana is an open-source monitoring platform that provides a rich set of tools for visualizing time-series data. It includes a variety of graph types and dashboards.</p> <p>Grafana is enabled in MKE 4k by default and may be disabled through the <code>mke4.yaml</code> configuration file:</p> <pre><code>monitoring:\n  enableGrafana: true\n</code></pre> <p>To access the Grafana dashboard:</p> <ol> <li>Obtain the <code>admin</code> user password for the Grafana dashboard from the <code>monitoring-grafana</code> secret in the <code>mke</code> namespace.</li> </ol> <pre><code>kubectl get secret monitoring-grafana -n mke -o jsonpath=\"{.data.admin-password}\" | base64 --decode\n</code></pre> <ol> <li> <p>Port forward Grafana:</p> <pre><code>kubectl --namespace mke port-forward svc/monitoring-grafana 3000:80\n</code></pre> </li> <li> <p>Navigate to <code>http://localhost:3000</code> to access the Welcome to Grafana login page.</p> </li> <li> <p>Enter the default username admin into the Email or username field and type the password you retrieved from the <code>monitoring-grafana</code> secret into the Password field.</p> </li> <li> <p>Click Log In.</p> </li> </ol>"},{"location":"configuration/monitoring/#cadvisor","title":"cAdvisor","text":"<p>cAdvisor is an open-source tool that collects, aggregates, processes,  and exports information in reference to running containers.</p> <p>cAdvisor is disabled in MKE 4k by default. You can enable the tool through the <code>mke4.yaml</code> configuration file:</p> <pre><code>monitoring:\n  enableCAdvisor: true\n</code></pre>"},{"location":"configuration/monitoring/#opscare-under-development","title":"OpsCare (Under development)","text":"<p>Mirantis OpsCare is an advanced monitoring and alerting solution. Once it is integrated, Mirantis OpsCare will enhance the monitoring capabilities of MKE 4k by incorporating additional tools and features, such as Prometheus Alertmanager.</p> <p>Disabled by default, you can enable Mirantis OpsCare through the <code>mke4.yaml</code> configuration file.</p> <pre><code>monitoring:\n  enableOpscare: true\n</code></pre>"},{"location":"configuration/multus/","title":"Multus","text":"<p>Multus is a container network interface (CNI) plugin that enables the attachment of multiple network interfaces to a single Pod.</p> <p>By default, Pods in Kubernetes are connected to a single network interface, which is the default network. With Multus CNI, though, Pods can have multiple network interfaces for multi-homed connectivity. For more information, refer to the Multus CNI GitHub repository.</p>"},{"location":"configuration/multus/#enable-multus","title":"Enable Multus","text":"<p>Multus is disabled in MKE 4k by default. To enable the function, you must obtain the <code>mke4.yaml</code> configuration file, locate the <code>network.multus.enabled</code> section, set the <code>enabled</code> parameter to <code>true</code>, and apply the new configuration.</p> <ol> <li>Obtain the default <code>mke4.yaml</code> configuration file:</li> </ol> <pre><code>mkectl init\n</code></pre> <ol> <li>Navigate to the <code>network</code> section of the configuration file, and set the    <code>enabled</code> parameter for multus to <code>true</code>.</li> </ol> <pre><code>network:\n  multus:\n    enabled: true\n</code></pre> <ol> <li>Apply the configuration:</li> </ol> <pre><code>mkectl apply -f mke4.yaml\n</code></pre> <ol> <li>Verify the successful deployment of Multus in the cluster:</li> </ol> <pre><code>kubectl get daemonset,pods -n kube-system -l app=multus\n</code></pre> <p>Example output:</p> <pre><code>NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/kube-multus-ds   3         3         3       3            3           &lt;none&gt;          50s\n\nNAME                       READY   STATUS    RESTARTS   AGE\npod/kube-multus-ds-8psck   1/1     Running   0          36s\npod/kube-multus-ds-dltjh   1/1     Running   0          36s\npod/kube-multus-ds-m2bsz   1/1     Running   0          36s\n</code></pre> <p>{{&lt; callout type=\"tip\" &gt;}} In MKE 4k, you can disable Multus at any time, as opposed to MKE 3 where once Multus is installed it cannot be disabled. {{&lt; /callout &gt;}}</p>"},{"location":"configuration/multus/#add-a-network-interface","title":"Add a network interface","text":"<ol> <li>SSH in to each cluster node to install the CNI and to determine the primary    network:</li> </ol> <p>To download and extract the CNI plugin:</p> <pre><code>  ```\n  CNI_PLUGIN_VERSION=v1.3.0\n  CNI_ARCH=amd64\n  curl -sL https://github.com/containernetworking/plugins/releases/download/${CNI_PLUGIN_VERSION}/cni-plugins linux-${CNI_ARCH}-${CNI_PLUGIN_VERSION}.tgz | sudo tar xvz -C /opt/cni/bin/\n  ```\n</code></pre> <p>To determine the primary network interface for the node:</p> <p>You will use the primary network interface information to create the <code>NetworkAttachmentDefinitions</code> file.</p> <pre><code>  {{&lt; callout type=\"info\" &gt;}}\n  The name of the primary interface can vary with the underlying network adapter.\n  {{&lt; /callout &gt;}}\n\n  ```\n  route\n  ```\n\n  {{&lt; callout type=\"info\" &gt;}}\n  eth0 is the primary network interface for most Linux distributions.\n  {{&lt; /callout &gt;}}\n\n  Example output:\n\n  ```\n  Kernel IP routing table\n  Destination     Gateway         Genmask         Flags Metric Ref    Use Iface\n  default         ip-172-31-0-1.u 0.0.0.0         UG    100    0        0 ens5\n  172.31.0.0      0.0.0.0         255.255.0.0     U     100    0        0 ens5\n  ip-172-31-0-1.u 0.0.0.0         255.255.255.255 UH    100    0        0 ens5\n  192.168.17.0    0.0.0.0         255.255.255.192 U     0      0        0 *\n  ```\n</code></pre> <ol> <li>Create the <code>NetworkAttachmentDefinitions</code> file, to specify other networks:</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: ens5-network\nspec:\n  config: |\n    {\n      \"cniVersion\": \"0.3.1\",\n      \"type\": \"macvlan\",\n      \"master\": \"ens5\",\n      \"mode\": \"bridge\",\n      \"mtu\": 9001,\n      \"ipam\": {\n        \"type\": \"host-local\",\n        \"subnet\": \"172.31.0.0/16\",\n        \"rangeStart\": \"172.31.2.150\",\n        \"rangeEnd\": \"172.31.2.200\",\n        \"routes\": [\n          { \"dst\": \"0.0.0.0/0\" }\n        ],\n        \"gateway\": \"172.31.2.1\"\n      }\n    }\nEOF\n</code></pre> <ol> <li>Verify the creation of the the network attachment definition:</li> </ol> <pre><code>kubectl get network-attachment-definition\n</code></pre> <p>Example output:</p> <pre><code>NAME           AGE\nens5-network   44s\n</code></pre> <ol> <li>Create a multi-homed Pod:</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-additional-network\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ens5-network\nspec:\n  containers:\n    - command:\n        - sleep\n        - \"3600\"\n      image: busybox\n      name: pods-simple-container\nEOF\n</code></pre> <ol> <li>Verify the network interfaces of the Pod:</li> </ol> <pre><code>kubectl exec -it pod-additional-network -- ip a\n</code></pre> <p>Example output:</p> <pre><code>LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 8951 qdisc noqueue qlen 1000\n    link/ether 26:36:4c:44:9c:80 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.23.138/32 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::2436:4cff:fe44:9c80/64 scope link\n       valid_lft forever preferred_lft forever\n3: net1@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc noqueue\n    link/ether 0e:a3:f7:e8:50:85 brd ff:ff:ff:ff:ff:ff\n    inet 172.31.2.150/16 brd 172.31.255.255 scope global net1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::ca3:f7ff:fee8:5085/64 scope link\n       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"configuration/multus/#uninstall-multus","title":"Uninstall Multus","text":"<ol> <li> <p>Obtain the MKE 4k configuration file.</p> </li> <li> <p>Set the enabled field to false to disable Multus.</p> </li> </ol> <pre><code>network:\n  multus:\n    enabled: false\n</code></pre> <ol> <li>Apply the configuration:</li> </ol> <pre><code>mkectl apply -f mke4.yaml\n</code></pre>"},{"location":"configuration/nvidia-gpu/","title":"NVIDIA GPU Workloads","text":"<p>Mirantis Kubernetes Engine (MKE) 4k supports running workloads on NVIDIA GPU nodes. Current support is limited to NVIDIA GPUs.</p> <p>{{&lt; callout type=\"info\" &gt;}} GPU Feature Discovery (GFD) is enabled by default. {{&lt; /callout &gt;}}</p> <p>To manage your GPU resources and enable GPU support, MKE 4k installs the NVIDIA GPU Operator on your cluster. The use of this resource causes the following resources to be installed and configured on each node:</p> <ul> <li>GPU device driver</li> <li>NVIDIA GPU Toolkit</li> <li>NVIDIA container runtime</li> </ul> <p>{{&lt; callout type=\"info\" &gt;}} Though it is not required, you can run the following command at any point to verify your GPU specifications:</p> <pre><code>sudo lspci | grep -i nvidia\n</code></pre> <p>Example output:</p> <pre><code>00:1e.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)\n</code></pre> <p>{{&lt; /callout &gt;}}</p> <p>{{&lt; callout type=\"important\" &gt;}}</p> <p>For air-gapped MKE 4k clusters, you must deploy a package registry and mirror the drivers to it, as described in the official NVIDIA documentation, Install NVIDIA GPU Operator in Air-Gapped Environments - Local Package Repository.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"configuration/nvidia-gpu/#configuration","title":"Configuration","text":"<p>NVIDIA GPU support is disabled in MKE 4k by default.</p> <p>To enable NVIDIA GPU support:</p> <ol> <li>Obtain the mke4.yaml configuration file:</li> </ol> <pre><code>mkectl init &gt; mke4.yaml\n</code></pre> <ol> <li>Navigate to the <code>devicePlugins.nvidiaGPU</code> section of the mke4.yaml    configuration file, and set the <code>enabled</code> parameter to <code>true</code>.</li> </ol> <pre><code>devicePlugins:\n  nvidiaGPU:\n    enabled: true\n</code></pre> <ol> <li>Apply the new configuration setting:</li> </ol> <pre><code>mkectl apply -f mke4.yaml\n</code></pre> <p>{{&lt; callout type=\"important\" &gt;}} Pod startup time can vary depending on node performance, during which the Pods will seem to be in a state of failure. {{&lt; /callout &gt;}}</p>"},{"location":"configuration/nvidia-gpu/#verification","title":"Verification","text":"<p>Once your NVIDIA GPU support configuration has completed, you can verify your setup using the tests detailed below:</p>"},{"location":"configuration/nvidia-gpu/#detect-nvidia-gpu-devices","title":"Detect NVIDIA GPU Devices","text":"<ol> <li>Run a simple GPU workload that reports detected NVIDIA GPU devices:</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: cuda-container\n      image: nvcr.io/nvidia/cloud-native/gpu-operator-validator:v22.9.0\n      resources:\n        limits:\n          nvidia.com/gpu: 1 # requesting 1 GPU\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\nEOF\n</code></pre> <ol> <li>Verify the successful completion of the Pod:</li> </ol> <pre><code>kubectl get pods | grep \"gpu-pod\"\n</code></pre> <p>Example output:</p> <pre><code>NAME                        READY   STATUS    RESTARTS   AGE\ngpu-pod                     0/1     Completed 0          7m56s\n</code></pre>"},{"location":"configuration/nvidia-gpu/#run-a-gpu-workload","title":"Run a GPU Workload","text":"<ol> <li>Create the workload:</li> </ol> <p><code>cat &lt;&lt;EOF | kubectl apply -f -    apiVersion: v1    kind: Pod    metadata:      name: cuda-vectoradd    spec:      restartPolicy: OnFailure      containers:      - name: cuda-vectoradd        image: \"nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04\"        resources:          limits:            nvidia.com/gpu: 1    EOF</code></p> <ol> <li>Run the following command once the Pod has reached <code>Completed</code> status:</li> </ol> <pre><code>kubectl logs pod/cuda-vectoradd\n</code></pre> <p>Example output:</p> <pre><code>[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone\n</code></pre> <ol> <li>Clean up the Pod:</li> </ol> <pre><code>kubectl delete -f cuda-vectoradd.yaml\n</code></pre>"},{"location":"configuration/nvidia-gpu/#count-gpus","title":"Count GPUs","text":"<p>Run the following command once you have enabled the NVIDIA GPU Device Plugin and the Pods have stabilized:</p> <pre><code>kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPUs:.metadata.labels.nvidia\\.com/gpu\\.count\"\n</code></pre> <p>Example results, showing a cluster with 3 control-plane nodes and 3 worker nodes:</p> <pre><code>NAME                                           GPUs\nip-172-31-174-195.us-east-2.compute.internal   1\nip-172-31-228-160.us-east-2.compute.internal   &lt;none&gt;\nip-172-31-231-180.us-east-2.compute.internal   1\nip-172-31-26-15.us-east-2.compute.internal     &lt;none&gt;\nip-172-31-3-198.us-east-2.compute.internal     1\nip-172-31-99-105.us-east-2.compute.internal    &lt;none&gt;\n</code></pre>"},{"location":"configuration/support-bundle/","title":"Support bundle","text":"<p>To generate support bundles directly from the command line, you must have the kubectl extension support bundle plugin installed.</p>"},{"location":"configuration/support-bundle/#install-the-support-bundle-plugin","title":"Install the support bundle plugin","text":"<p>You can install the support bundle plugin using Krew plugin manager, or you can obtain the support bundle plugin from the release archives and install it manually.</p> <p>{{&lt; tabs items=\"Krew installation,Manual installation\" &gt;}}</p> <pre><code>{{&lt; tab &gt;}}\n1. Optional. Install the Krew plugin manager if is not yet installed on your\n   system. For detailed instruction, refer to the official Krew documentaiton\n   [Installing](https://krew.sigs.k8s.io/docs/user-guide/setup/install/).\n\n2. Install the support bundle plugin:\n\n   ```commandline\n   kubectl krew install support-bundle\n\n\n3. Append the ``$HOME/.krew/bin`` directory to your ``$PATH`` environment variable:\n\n   ```commandline\n   export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n   ```\n\n4. Verify the support bundle plugin installation:\n\n   1. Restart your shell.\n\n   2. Run the ``kubectl krew`` command.\n\n{{&lt; /tab &gt;}}\n\n{{&lt; tab &gt;}}\n\nYou can manually install the support bundle plugin from the release archives.\nA key advantage this method offers is that it facilitates the installation\nin air gap environments.\n\nRun the following command to download and unarchive the latest release,\nand move the plugin to your ``$PATH``:\n\n```commandline\ncurl -L https://github.com/replicatedhq/troubleshoot/releases/latest/download/support-bundle_linux_amd64.tar.gz | tar xzvf -\nsudo mv ./support-bundle /usr/local/bin/kubectl-support_bundle\n    ```\n{{&lt; /tab &gt;}}\n</code></pre> <p>{{&lt; /tabs &gt;}}</p>"},{"location":"configuration/support-bundle/#upgrade-a-support-bundle","title":"Upgrade a support bundle","text":"<p>The instruction for upgrading the support bundle plugin corresponds to the method that was used to install the plugin. </p> <p>{{&lt; tabs items=\"Krew installation,Manual installation\" &gt;}}</p> <pre><code>{{&lt; tab &gt;}}\nRun the following krew command to upgrade your existing support-bundle:\n\n```commandline\nkubectl krew upgrade support-bundle\n```\n{{&lt; /tab &gt;}}\n\n{{&lt; tab &gt;}}\nRun the following commands to manually upgrade your existing support-bundle:\n\n1. Download the latest version of the plugin:\n\n```commandline\ncurl -L https://github.com/replicatedhq/troubleshoot/releases/latest/download/support-bundle_linux_amd64.tar.gz | tar xzvf -\n```\n\n2. Replace the existing plugin with the new version:\n\n```commandline\nsudo mv ./support-bundle /usr/local/bin/kubectl-support_bundle\n```\n\n{{&lt; /tab &gt;}}\n</code></pre> <p>{{&lt; /tabs &gt;}}</p>"},{"location":"configuration/support-bundle/#uninstall-a-support-bundle","title":"Uninstall a support bundle","text":"<p>The instruction for uninstalling the support bundle plugin corresponds to the method that was used to install the plugin. </p> <p>{{&lt; tabs items=\"Krew installation,Manual installation\" &gt;}}</p> <pre><code>{{&lt; tab &gt;}}\nRun the following command to remove the support bundle plugin:\n\n```commandline\nkubectl krew uninstall support-bundle\n```\n{{&lt; /tab &gt;}}\n{{&lt; tab &gt;}}\n1. Delete the `support-bundle` binary file from where it was placed\n  at installation. \n2. Remove the support bundle:\n\n  ```commandline\n  sudo rm /usr/local/bin/kubectl-support_bundle\n  ```\n{{&lt; /tab &gt;}}\n</code></pre> <p>{{&lt; /tabs &gt;}}</p>"},{"location":"configuration/support-bundle/#create-a-support-bundle","title":"Create a support bundle","text":"<ol> <li> <p>Construct a YAML file to set the support bundle configuration.</p> <p>The example <code>your-support-bundle.yaml</code> file that follows:</p> <ul> <li>Collects basic information about the cluster.</li> <li>Enumerates all available resources in the cluster.</li> <li>Collects logs from the <code>mke-controller-manager</code> pods, in the <code>logs/</code> directory of the output.</li> </ul> <pre><code>apiVersion: troubleshoot.sh/v1beta2\n kind: SupportBundle\n metadata:\n   name: sample\n spec:\n   collectors:\n     - logs:\n         selector:\n           - control-plane=controller-manager\n         namespace: mke\n         name: logs/mke\n</code></pre> </li> <li> <p>Generate the support bundle:</p> <pre><code>kubectl support-bundle ./path-to-your-support-bundle.yaml\n</code></pre> </li> </ol> <p>By default, the support bundle collects cluster information and cluster resources.</p> <p>For a comprehensive list of available in-cluster collectors, refer to the official    Troubleshoot All Collectors    documentation.</p>"},{"location":"configuration/support-bundle/#collect-host-information-using-the-k0s-provided-yaml-file","title":"Collect host information using the k0s-provided YAML file","text":"<ol> <li> <p>Obtain the k0s-provided YAML    file.</p> </li> <li> <p>Run the <code>support-bundle</code> tool:</p> <pre><code>./support-bundle --kubeconfig /var/lib/k0s/pki/admin.conf &lt;support-bundle-worker.yaml&gt;\n</code></pre> </li> </ol> <p>{{&lt; callout type=\"info\" &gt;}}      The <code>support-bundle</code> tool requires that the <code>kubeconfig</code> file be passed as      an argument. The <code>kubeconfig</code> file is located at      <code>/var/lib/k0s/pki/admin.conf</code>.    {{&lt; /callout &gt;}}</p> <p>Now, you can find the support bundle with the collected host information at <code>support-bundle-&lt;timestamp&gt;.tar.gz</code>.</p>"},{"location":"configuration/telemetry/","title":"Telemetry","text":"<p>You can set MKE 4k to automatically record and transmit data to Mirantis through an encrypted channel for monitoring and analysis purposes. This data helps the Mirantis Customer Success Organization to better understand how customers use MKE. It also provides product usage statistics, which is key feedback that helps product teams in their efforts to enhance Mirantis products and services.</p> <p>{{&lt; callout type=\"info\" &gt;}}    The MKE 4k telemetry enablement setting is automatically applied to the k0s    configuration.  {{&lt; /callout &gt;}}</p> <p>{{&lt; callout type=\"info\" &gt;}}    Telemetry is automatically enabled for MKE 4k clusters that are running    without a license, with a license that has expired, or with an invalid    license. In all of such scenarios, you can only disable    telemetry once a valid license has been applied to the cluster. {{&lt; /callout &gt;}}</p>"},{"location":"configuration/telemetry/#enable-telemetry-through-the-mke-4k-cli","title":"Enable telemetry through the MKE 4k CLI","text":"<ol> <li>Access the <code>mke4.yaml</code> configuration file.</li> <li>Set the <code>spec.tracking.enabled</code> field to <code>true</code>.</li> </ol> <pre><code>spec:\n  tracking:\n    enabled: true\n    clusterLabel: &lt;optional-label-to-include-with-analytics&gt;\n</code></pre> <ol> <li>Run the  <code>mkectl apply</code> command to apply the new settings.</li> </ol> <p>After a few moments, the change will reconcile in the configuration. From this point onward, MKE 4k will transmit key usage data to Mirantis by way of a secure Segment endpoint.</p>"},{"location":"configuration/telemetry/#enable-telemetry-through-the-mke-4k-web-ui","title":"Enable telemetry through the MKE 4k web UI","text":"<ol> <li> <p>Log in to the MKE 4k web UI as an administrator.</p> </li> <li> <p>Click Admin Settings to display the available options.</p> </li> <li> <p>Click Telemetry to call the Telemetry screen.</p> </li> <li> <p>Click Enable Telemetry.</p> </li> <li> <p>Click Save.</p> </li> </ol>"},{"location":"configuration/workload-node-deployment/","title":"Workload node deployment","text":"<p>By default, user workloads cannot run on manager nodes. This is to ensure that cluster management functionality remains performant and to enhance cluster security. You can, however, override this behavior.</p> <p>{{&lt; callout type=\"warning\" &gt;}}</p> <p>If a user deploys a malicious workload that can affect the node on which it is running, that workload will not be able to strike any other nodes in the cluster or have any impact on cluster management functionality.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"configuration/workload-node-deployment/#restrict-workload-deployment-to-kubernetes-worker-nodes","title":"Restrict workload deployment to Kubernetes worker nodes","text":"<p>By default, MKE 4k clusters use Kubernetes taints and tolerations to prevent user workloads from deploying to MKE 4k manager nodes.</p> <p>To view the taints, run the following command:</p> <pre><code>$ kubectl get nodes &lt;mkemanager&gt; -o json | jq -r '.spec.taints | .[]'\n</code></pre> <p>Example of system response:</p> <pre><code>{\n  \"effect\": \"NoSchedule\",\n  \"key\": \"node-role.kubernetes.io/master\"\n}\n</code></pre>"},{"location":"configuration/workload-node-deployment/#allow-workload-deployment-on-kubernetes-mke-4k-manager","title":"Allow workload deployment on Kubernetes MKE 4k manager","text":"<p>You can circumvent the protections put in place by Kubernetes taints and tolerations.</p> <p>To add a toleration to the Pod specification for existing workloads:</p> <ol> <li>Add the following toleration to the Pod specification, either through the    MKE 4k web UI or using the <code>kubectl edit &lt;resource&gt; &lt;name&gt;</code> command:</li> </ol> <pre><code>tolerations:\n- key: \"node-role.kubernetes.io/master\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n</code></pre> <ol> <li>Inspect the modified object to verify the successful application of the    toleration.</li> </ol>"},{"location":"configuration/authentication/LDAP/","title":"LDAP","text":"<p>You can configure LDAP (Lightweight Directory Access Protocol) for MKE 4k through the <code>authentication.ldap</code> section of the <code>mke4.yaml</code> configuration file.</p> <p>LDAP example configuration:</p> <pre><code>authentication:\n  ldap:\n    enabled: true\n    host: ldap.example.org:389\n    insecureNoSSL: true\n    bindDN: cn=admin,dc=example,dc=org\n    bindPW: password\n    usernamePrompt: Email Address\n    userSearch:\n      baseDN: ou=People,dc=example,dc=org\n      filter: \"(objectClass=person)\"\n      username: mail\n      idAttr: DN\n      emailAttr: mail\n      nameAttr: cn\n</code></pre>"},{"location":"configuration/authentication/LDAP/#configure-ldap-service-for-mke","title":"Configure LDAP service for MKE","text":"<p>In the <code>mke4.yaml</code> configuration file <code>authentication.ldap</code> section, enable your LDAP service by setting <code>enabled</code> to <code>true</code>. Use the remaining fields, which are defined in the following table, to configure the interactions with your LDAP server.</p> <p>{{&lt; callout type=\"info\" &gt;}} For information on how to obtain the field values, refer to setting up OpenLDAP as an LDAP provider. {{&lt; /callout &gt;}}</p> Field Description <code>host</code> Host and optional port of the LDAP server, in the <code>host:port</code> format. <code>rootCA</code> Path to a trusted root certificate file. <code>bindDN</code> Distinguished Name (DN) for an application service account. <code>bindPW</code> Password for an application service account. <code>usernamePrompt</code> Attribute to display in the password prompt. <code>userSearch</code> Settings to map user-entered username and password to an LDAP entry. <code>userSearch.baseDN</code> BaseDN from which to start the search. <code>userSearch.filter</code> Optional filter to apply for a user search of the directory. <code>userSearch.username</code> Username attribute to use for user entry comparison. <code>userSearch.idAttr</code> String representation of the user. <code>userSearch.emailAttr</code> Attribute to map to email. <code>userSearch.nameAttr</code> Attribute to map to display name of a user. <code>userSearch.preferredUsernameAttr</code> Attribute to map to preferred usernames. <code>groupSearch</code> Group search queries for groups given a user entry. <code>groupSearch.baseDN</code> BaseDN from which to start the search. <code>groupSearch.filter</code> Optional filter to apply for a group search of the directory. <code>groupSearch.userMatchers</code> Field pairs list to use to match a user to a group. <code>groupSearch.nameAttr</code> Group name. <p>For more information, refer to the official DEX documentation LDAP configuration.</p>"},{"location":"configuration/authentication/OIDC/","title":"OIDC","text":"<p>You configure OIDC (OpenID Connect) for MKE 4k through the <code>authentication.oidc</code> section of the <code>mke4.yaml</code> configuration file.</p> <p>OIDC example configuration:</p> <pre><code>authentication:\n  oidc:\n    enabled: true\n    issuer: https://dev-94406016.okta.com\n    clientID: 0oedtjcjrjWab3zlD5d4\n    clientSecret: DFA9NYLfE1QxwCSFkZunssh2HCx16kDl41k9tIBtFZaNcqyEGle8yZPtMBesyomD\n</code></pre>"},{"location":"configuration/authentication/OIDC/#configure-oidc-service-for-mke","title":"Configure OIDC service for MKE","text":"<p>In the <code>mke4.yaml</code> configuration file <code>authentication.oidc</code> section, enable your OIDC service by setting <code>enabled</code> to <code>true</code>. Use the remaining fields, which are defined in the following table, to configure your chosen OIDC provider.</p> <p>{{&lt; callout type=\"info\" &gt;}} For information on how to obtain the field values, refer to Setting up Okta as an OIDC provider. {{&lt; /callout &gt;}}</p> Field Description <code>issuer</code> OIDC provider root URL. <code>clientID</code> ID from the IdP application configuration. <code>clientSecret</code> Secret from the IdP application configuration. <p>For more information, refer to the official DEX documentation OIDC configuration.</p>"},{"location":"configuration/authentication/SAML/","title":"SAML","text":"<p>You can configure SAML (Security Assertion Markup Language) for MKE 4k through the <code>authentication.saml</code> section of the <code>mke4.yaml</code> configuration file.</p> <p>SAML example configuration:</p> <pre><code>authentication:\n  saml:\n    enabled: true\n    ssoURL: https://dev64105006.okta.com/app/dev64105006_mke4saml_1/epkdtszgindywD6mF5s7/sso/saml\n    usernameAttr: name\n    emailAttr: email\n</code></pre>"},{"location":"configuration/authentication/SAML/#configure-saml-service-for-mke","title":"Configure SAML service for MKE","text":"<p>In the <code>mke4.yaml</code> configuration file <code>authentication.saml</code> section, enable your SAML service by setting <code>enabled</code> to <code>true</code>. Use the remaining fields, which are defined in the following table, to configure your chosen SAML provider.</p> <p>{{&lt; callout type=\"info\" &gt;}} For information on how to obtain the field values, refer to Setting up Okta as a SAML provider. {{&lt; /callout &gt;}}</p> Field Description <code>enabled</code> Enable authentication through dex. <code>ssoMetadataURL</code> Metadata URL provided by some IdPs, with which MKE 4k can retrieve information for all other SAML configurations.When a URL is provided for <code>ssoMetadataURL</code>, the other SAML fields are not required. <code>ca</code> Certificate Authority (CA) alternative to <code>caData</code> to use when validating the signature of the SAML response. Must be manually mounted in a local accessible by dex. <code>caData</code> CA alternative to <code>ca</code>, which you can use to place the certificate data directly into the config file. <code>ssoURL</code> URL to provide to users to sign into MKE 4k with SAML. Provided by the IdP. <code>insecureSkipSignatureValidation</code> Optional. Use to skip the signature validation. For testing purposes only. <code>usernameAttr</code> Username attribute in the returned assertions, to map to ID token claims. <code>emailAttr</code> Email attribute in the returned assertions, to map to ID token claims. <code>groupsAttr</code> Optional. Groups attribute in the returned assertions, to map to ID token claims. <code>entityIssuer</code> Optional. Include as the Issuer value during authentication requests. <code>ssoIssuer</code> Optional. Issuer value that is expected in the SAML response. <code>groupsDelim</code> Optional. If groups are assumed to be represented as a single attribute, this delimiter splits the attribute value into multiple groups. <code>nameIDPolicyFormat</code> Requested name ID format."},{"location":"configuration/authentication/SAML/#test-authentication-flow","title":"Test authentication flow","text":"<ol> <li>Navigate to <code>http://&lt;MKE 4k hostname&gt;/dex/login</code>.</li> <li>Click Login to display the login page.</li> <li>Select Log in with SAML.</li> <li>Enter your credentials and click Sign In. If authentication is    successful, you will be redirected to the client applications home page.</li> </ol>"},{"location":"configuration/authentication/_index/","title":"Authentication","text":"<p>MKE 4k uses Dex for authentication. Dex serves as a proxy between your MKE 4k cluster and your authentication providers, combining the configuration of multiple authentication providers into a single configuration while also handling the complexity of the various protocols.</p> <p>Mirantis Kubernetes Engine (MKE) 4k supports the following authentication protocols:</p> <ul> <li>OpenID Connect (OIDC)</li> <li>Security Assertion Markup Language (SAML)</li> <li>Lightweight Directory Access Protocol (LDAP)</li> </ul>"},{"location":"configuration/authentication/_index/#prerequisites","title":"Prerequisites","text":"<p>You must have certain dependencies in place before you can configure authentication. These dependencies differ, depending on which authentication protocol you choose to deploy.</p> <ul> <li> <p>Identity Provider (IdP): To use OIDC or SAML, you must configure an identity provider. For examples of how to use Okta as an authentication service provider for either of these protocols, refer to OIDC or SAML.</p> </li> <li> <p>LDAP Server: To use LDAP, you must have an LDAP server configured. A setup example for an OpenLDAP server is available at LDAP.</p> </li> </ul>"},{"location":"configuration/authentication/_index/#configuration","title":"Configuration","text":"<p>You configure authentication for MKE 4k through the <code>authentication</code> section of the <code>mke4.yaml</code> configuration file.</p> <p>Authentication is always enabled, however, the settings for each of the individual authentication protocols are disabled. To enable and install an authentication protocol, set its <code>enabled</code> configuration option to <code>true</code>.</p> <pre><code>authentication:\n  ldap:\n    enabled: false\n  oidc:\n    enabled: false\n  saml:\n    enabled: false\n</code></pre>"},{"location":"configuration/authentication/_index/#set-expiration","title":"Set Expiration","text":"<p>You can use the <code>expiry</code> section of the configuration file to set the expiration time for refresh and id tokens, in the format of number + time unit format. For example, <code>1h</code> to designate one hour.</p> <pre><code>authentication:\n  expiry:\n    refreshTokens: {}\n</code></pre> <p>The following table shows all of the available fields for the <code>expiry</code> section.</p> Field Description <code>expiry</code> Section for the various expiry settings. <code>expiry.idTokens</code> Lifetime of the ID tokens. <code>expiry.authRequests</code> Time frame that a code can be exchanged for a token. <code>expiry.deviceRequests</code> Time frame in which users can authorize a device to receive a token. <code>expiry.signingKeys</code> Time period after which the signing keys are rotated. <code>expiry.refreshTokens</code> Section for the various refresh token settings. <code>expiry.refreshTokens.validIfNotUsedFor</code> Invalidate a refresh token if it is not used for the specified time. <code>expiry.refreshTokens.absoluteLifetime</code> Absolute time frame of a refresh token. <code>expiry.refreshTokens.disableRotation</code> Disable every-request rotation. <code>expiry.refreshTokens.reuseInterval</code> Interval for obtaining the same refresh token from the refresh endpoint."},{"location":"configuration/authentication/_index/#unsupported-functions","title":"Unsupported functions","text":"<p>Authentication functions that are not supported in MKE 4k include:</p> <ul> <li>OIDC proxies</li> <li>SAML proxies</li> <li>LDAP disablement of referral chasing</li> <li>LDAP JIT provisioning</li> <li>LDAP SAML logins</li> <li>LDAP simple pagination</li> <li>LDAP user sync</li> <li>MFA (Multi-Factor Authentication)</li> </ul>"},{"location":"configuration/authentication/basic-authentication/","title":"Basic authentication","text":"<p>By default, whenever you create a fresh cluster, an admin user is created, with the username <code>admin</code> and a randomly generated password. This password is printed out following the successful creation of the cluster.</p> <p>{{&lt; callout type=\"info\" &gt;}} The password is printed following the successful creation of the cluster. It is not stored anywhere on the cluster, and it is not possible to retrieve it later. Thus, if the password is not saved in the install output logs, you must recreate the cluster to gain access. {{&lt; /callout &gt;}}</p> <p>An admin user can create users, change user passwords, and delete users from the MKE 4k system.</p>"},{"location":"configuration/authentication/basic-authentication/#create-users","title":"Create users","text":"<ol> <li> <p>Log in to the MKE 4k Dashboard as an administrator.</p> </li> <li> <p>Navigate to Access Control --&gt; Users to access the user page.</p> </li> <li> <p>Click the Create User button at the far right to access the User Creation page.</p> </li> <li> <p>Enter the pertinent information into the following fields:</p> </li> <li> <p>username</p> </li> <li>password</li> <li>confirm password</li> </ol> <p>{{&lt; callout type=\"info\" &gt;}}    You can enter the name of the user into the name field, however this is    optional and the input is not in use in MKE 4k.    {{&lt; /callout &gt;}}</p> <ol> <li>Optional. If the user you are creating is to be an admin, tick the    admin checkbox.</li> </ol>"},{"location":"configuration/authentication/basic-authentication/#change-a-password","title":"Change a password","text":"<ol> <li> <p>Log in to the MKE 4k Dashboard as an administrator.</p> </li> <li> <p>Navigate to Access Control --&gt; Users to access the user page.</p> </li> <li> <p>Click the username for the user whose password you want to change. The    page for that particular user will display.</p> </li> <li> <p>Click Update password at the top right of the user page to access the    update password page.</p> </li> <li> <p>Enter the new password in to the new password field and confirm it in    the confirm new password field.</p> </li> <li> <p>Click Update in the bottom right corner of the page.</p> </li> </ol>"},{"location":"configuration/authentication/basic-authentication/#delete-users","title":"Delete users","text":"<ol> <li> <p>Log in to the MKE 4k Dashboard as an administrator.</p> </li> <li> <p>Navigate to Access Control --&gt; Users to access the user page.</p> </li> <li> <p>Tick the checkbox to the left of the username for the user you want to    delete. You can tick multiple boxes to delete multiple users at the same time.</p> </li> <li> <p>Click the actions dropdown at the far top right of the page and select    remove. A Delete confirmation dialog will display.</p> </li> <li> <p>Click Delete to remove the user from the MKE 4k system.</p> </li> </ol> <p>Alternately, you can delete a user from their dedicated user page:</p> <ol> <li> <p>Click the username for the user you want to delete to access their dedicated user page.</p> </li> <li> <p>Click Delete at the far top right of the page. A Delete confirmation    dialog will display.</p> </li> <li> <p>Click Delete to remove the user from the MKE 4k system.</p> </li> </ol>"},{"location":"configuration/backup-restore/_index/","title":"Backup and restore","text":"<p>MKE 4k supports backup and restoration of cluster data through the use of the Velero add-on. System backup is enabled by default.</p>"},{"location":"configuration/backup-restore/_index/#backup-configuration","title":"Backup configuration","text":"<p>The <code>backup</code> section of the <code>mke4.yaml</code> configuration file renders as follows:</p> <pre><code>backup:\n  enabled: true\n  storage_provider:\n    type: InCluster\n    in_cluster_options:\n      enable_ui: true\n      distributed: false\n</code></pre> <p>By default, MKE 4k supports backups that use the in-cluster storage provider, as shown in the <code>type.InCluster</code> field. In-cluster backups for MKE 4k are implemented using the MinIO add-on.</p> <p>Set <code>enable_ui</code> to <code>true</code> to expose the MinIO Console through the Ingress and make it accessible through the UI. Core backup functionality works, even if the UI is disabled.</p> <p>The <code>distributed</code> setting configures MinIO storage to run in distributed mode.</p> <p>Refer to the following list for detail on all the configuration file <code>backup</code> fields:</p> <code>enabled</code> Indicates whether backup/restore functionality is enabled. <ul> <li>Valid values: <code>true</code>, <code>false</code></li> <li>Default: <code>true</code></li> </ul> <code>storage_provider.type</code> <p>Indicates whether the storage type in use is in-cluster or external.</p> <ul> <li><code>InCluster</code>, <code>External</code></li> <li>Default: <code>InCluster</code></li> </ul> <code>storage_provider.in_cluster_options.enable_ui</code> <p>Indicates whether to expose InCluster (MinIO) storage through NodePort.</p> <ul> <li>Valid values: <code>true</code>, <code>false</code></li> <li>Default: <code>true</code></li> </ul> <code>storage_provider.in_cluster_options.distributed</code> <p>Indicates whether to run MinIO in distributed mode.</p> <ul> <li>Valid values: <code>true</code>, <code>false</code></li> <li>Default: <code>false</code></li> </ul> <code>storage_provider.external_options.provider</code> <p>Name of the external storage provider. Currently, AWS is the only available option.</p> <ul> <li>Valid values: <code>aws</code></li> <li>Default: <code>aws</code></li> </ul> <code>storage_provider.external_options.bucket</code> <p>Name of the pre-created bucket to use for backup storage.</p> <code>storage_provider.external_options.region</code> <p>Region in which the bucket exists.</p> <code>storage_provider.external_options.credentials_file_path</code> <p>Path to the Credentials file.</p> <code>storage_provider.external_options.credentials_file_profile</code> <p>Profile in the Credentials file to use</p>"},{"location":"configuration/backup-restore/_index/#create-a-backup-and-perform-a-restore","title":"Create a backup and perform a restore","text":"<p>For information on how to create backups and perform restores for both storage provider types, refer to:</p> <ul> <li>External storage provider</li> <li>In-cluster storage provider</li> </ul>"},{"location":"configuration/backup-restore/_index/#existing-limitations","title":"Existing limitations","text":"<ul> <li> <p>MKE 4k does not currently support:</p> </li> <li> <p>scheduled backups</p> </li> <li>backup to NFS storage</li> <li> <p>backup to local disks</p> </li> <li> <p>Backups must currently be restored in the same cluster in which the backup   was taken. As such, you cannot restore a backup to a new set of nodes.</p> </li> <li> <p>Backups must be downloaded and uploaded from the in-cluster storage provider   using the MinIO Console, as the CLI does not currently support these actions.</p> </li> </ul>"},{"location":"configuration/backup-restore/external/","title":"Back up using an external storage provider","text":"<p>You can configure MKE 4k to externally store backups and restores, for example, in object storage provided by a public cloud provider.</p> <p>{{&lt; callout type=\"info\" &gt;}}    AWS S3 is currently the only external backup storage supported by MKE 4k. {{&lt; /callout &gt;}}</p>"},{"location":"configuration/backup-restore/external/#configure-an-external-storage-provider","title":"Configure an external storage provider","text":"<p>{{&lt; callout type=\"info\" &gt;}}</p> <p>AWS store backups in an object storage bucket. Mirantis recommends that each of these buckets be unique for each Kubernetes cluster.</p> <p>{{&lt; /callout &gt;}}</p> <ol> <li>Create an S3 bucket:</li> </ol> <pre><code>aws s3api create-bucket \\\n    --bucket &lt;BUCKET_NAME&gt; \\\n    --region &lt;AWS_REGION&gt; \\\n    --create-bucket-configuration LocationConstraint=&lt;BUCKET_NAME&gt;\n</code></pre> <p>The <code>us-east-1</code> region does not support a <code>LocationConstraint</code> setting. As such, if your region is <code>us-east-1</code>, omit the bucket configuration.</p> <pre><code>aws s3api create-bucket \\\n    --bucket &lt;BUCKET_NAME&gt; \\\n    --region us-east-1\n</code></pre> <ol> <li>Create an IAM user:</li> </ol> <pre><code>aws iam create-user --user-name mke4backup\n</code></pre> <p>{{&lt; callout type=\"tip\" &gt;}}</p> <p>To deploy multiple MKE 4k clusters, you can create a unique    username per cluster instead of the default <code>mke4backup</code>. For more    information, refer to the official AWS documentation, What is IAM?.</p> <p>{{&lt; /callout &gt;}}</p> <ol> <li>Attach a policy to affix the necessary permissions to the IAM user.</li> </ol> <p>{{&lt; callout type=\"info\" &gt;}}</p> Click for example policy <pre><code>cat &gt; mke4backup-policy.json &lt;&lt;EOF\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:DescribeVolumes\",\n                \"ec2:DescribeSnapshots\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVolume\",\n                \"ec2:CreateSnapshot\",\n                \"ec2:DeleteSnapshot\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:PutObject\",\n                \"s3:AbortMultipartUpload\",\n                \"s3:ListMultipartUploadParts\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::${BUCKET}/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::${BUCKET}\"\n            ]\n        }\n    ]\n}\nEOF\n</code></pre> <p>{{&lt; /callout &gt;}}</p> <pre><code>aws iam put-user-policy \\\n  --user-name mke4backup \\\n  --policy-name mke4backup \\\n  --policy-document file://mke4backup-policy.json\n</code></pre> <ol> <li>Create an access key for the IAM user:</li> </ol> <pre><code>aws iam create-access-key --user-name mke4backup\n</code></pre> <p>Example output:</p> <p>```    {      \"AccessKey\": {            \"UserName\": \"mke4backup\",            \"Status\": \"Active\",            \"CreateDate\": \"2017-07-31T22:24:41.576Z\",            \"SecretAccessKey\": ,            \"AccessKeyId\":        }    } <ol> <li>Create an mke4backup-specific credentials file named <code>credentials-mke4backup</code>    in your local directory, in which the access key ID and secret are the values returned from the <code>create-access-key</code> command:</li> </ol> <pre><code>[mke4backup-profile]\naws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;\naws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;\n</code></pre> <ol> <li>Edit the <code>storage_provider</code> section of the <code>mke4.yaml</code> configuration file,    adding the AWS bucket name, bucket region, IAM credentials file path, and    IAM credentials profile.</li> </ol> <p>Example:</p> <pre><code>spec:\n  backup:\n    storage_provider:\n      type: External\n      external_options:\n        provider: aws\n        bucket: &lt;BUCKET_NAME&gt;\n        region: &lt;BUCKET_REGION&gt;\n        credentials_file_path: &lt;/PATH/TO/CREDENTIALS-MKE4BACKUP&gt;\n        credentials_file_profile: \"mke4backup-profile\"\n</code></pre> <ol> <li>Apply the configuration:</li> </ol> <pre><code>mkectl apply\n</code></pre> <ol> <li>Verify the existence of the <code>BackupStorageLocation</code> custom resource:</li> </ol> <pre><code>kubectl --kubeconfig &lt;path-to-kubeconfig&gt; get backupstoragelocation -n mke\n</code></pre> <p>Example output:</p> <pre><code>NAME      PHASE       LAST VALIDATED   AGE   DEFAULT\ndefault   Available   20s              32s   true\n</code></pre> <p>The output may require a few minutes to display.</p>"},{"location":"configuration/backup-restore/external/#create-an-external-backup","title":"Create an external backup","text":"<p>To create a backup, run:</p> <pre><code>mkectl backup create --name aws-backup\n</code></pre> <p>Example output:</p> <pre><code>INFO[0000] Creating backup aws-backup...\nBackup request \"aws-backup\" submitted successfully.\nRun `velero backup describe aws-backup` or `velero backup logs aws-backup` for more details.\nINFO[0000] Waiting for backup aws-backup to complete...\nINFO[0003] Waiting for backup to complete. Current phase: InProgress\nINFO[0006] Waiting for backup to complete. Current phase: InProgress\nINFO[0009] Waiting for backup to complete. Current phase: InProgress\nINFO[0012] Waiting for backup to complete. Current phase: InProgress\nINFO[0015] Waiting for backup to complete. Current phase: Completed\nINFO[0015] Backup aws-backup completed successfully\n</code></pre> <p>To list the backups, run:</p> <pre><code>mkectl backup list\n</code></pre> <p>Example output:</p> <pre><code>NAME         STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR\naws-backup   Completed   0        0          2024-05-08 16:17:18 -0400 EDT   29d       default            &lt;none&gt;\n</code></pre>"},{"location":"configuration/backup-restore/external/#restore-from-an-external-backup","title":"Restore from an external backup","text":"<p>A restore operation returns the Kubernetes cluster to the state it was in at the time the backup you select was created.</p> <p>To perform a restore using an external backup, run:</p> <pre><code>mkectl restore create --name aws-backup\n</code></pre> <p>Example output:</p> <pre><code>INFO[0000] Waiting for restore aws-backup-20240508161811 to complete...\nINFO[0000] Waiting for restore to complete. Current phase: InProgress\nINFO[0003] Waiting for restore to complete. Current phase: InProgress\nINFO[0006] Waiting for restore to complete. Current phase: InProgress\nINFO[0009] Waiting for restore to complete. Current phase: InProgress\nINFO[0012] Waiting for restore to complete. Current phase: InProgress\nINFO[0015] Waiting for restore to complete. Current phase: InProgress\nINFO[0018] Waiting for restore to complete. Current phase: InProgress\nINFO[0021] Waiting for restore to complete. Current phase: InProgress\nINFO[0024] Waiting for restore to complete. Current phase: Completed\nINFO[0024] Restore aws-backup-20240508161811 completed successfully\n</code></pre> <p>To list the restores, run:</p> <pre><code>mkectl restore list\n</code></pre> <p>Example output:</p> <pre><code>NAME                        BACKUP       STATUS      STARTED                         COMPLETED                       ERRORS   WARNINGS   CREATED                         SELECTOR\naws-backup-20240508161811   aws-backup   Completed   2024-05-08 16:18:11 -0400 EDT   2024-05-08 16:18:34 -0400 EDT   0        108        2024-05-08 16:18:11 -0400 EDT   &lt;none&gt;\n</code></pre>"},{"location":"configuration/backup-restore/external/#verify-backups-and-restores","title":"Verify backups and restores","text":"<p>Using your AWS console, you can verify the presence of your backups and restores in the S3 bucket.</p> <p></p>"},{"location":"configuration/backup-restore/in-cluster/","title":"Back up with an in-cluster storage provider","text":"<p>By default, MKE 4k stores backups and restores using the in-cluster storage provider, the MinIO add-on.</p> <p>{{&lt; callout type=\"info\" &gt;}}   MinIO is not currently backed by persistent storage. For persistent storage of backups, use an external storage provider or download the MinIO backups. {{&lt; /callout &gt;}}</p> <p>{{&lt; callout type=\"info\" &gt;}} The offered instructions assume that you have    created a cluster with the default MKE 4k backup configuration.  {{&lt; /callout &gt;}}</p>"},{"location":"configuration/backup-restore/in-cluster/#create-an-in-cluster-backup","title":"Create an in-cluster backup","text":"<p>To create an in-cluster backup, run:</p> <pre><code>mkectl backup create --name &lt;name&gt;\n</code></pre> <p>Example output:</p> <pre><code>mkectl backup create --name test\nINFO[0000] Creating backup test...\nBackup request \"test\" submitted successfully.\nRun `velero backup describe test` or `velero backup logs test` for more details.\nINFO[0000] Waiting for backup test to complete...\nINFO[0003] Waiting for backup to complete. Current phase: InProgress\nINFO[0006] Waiting for backup to complete. Current phase: InProgress\nINFO[0009] Waiting for backup to complete. Current phase: InProgress\nINFO[0012] Waiting for backup to complete. Current phase: InProgress\nINFO[0015] Waiting for backup to complete. Current phase: Completed\n</code></pre> <p>The backup should be present in the MinIO bucket. </p> <p>To list the backups, run:</p> <pre><code>mkectl backup list\n</code></pre> <p>Example output:</p> <pre><code>mkectl backup list\nNAME   STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR\ntest   Completed   0        0          2024-05-07 17:29:18 -0400 EDT   29d       default            &lt;none&gt;\n</code></pre> <p>To view detailed logs of a backup, access the MinIO UI.</p>"},{"location":"configuration/backup-restore/in-cluster/#restore-from-an-in-cluster-backup","title":"Restore from an in-cluster backup","text":"<p>A restore operation returns the Kubernetes cluster to the state it was in at the time the backup you selected was created.</p> <p>To perform a restore using an in-cluster backup, run:</p> <pre><code>mkectl restore create --name test\n</code></pre> <p>Example output:</p> <pre><code>mkectl restore create --name test\nINFO[0000] Waiting for restore test-20240507173309 to complete...\nINFO[0000] Waiting for restore to complete. Current phase: InProgress\nINFO[0003] Waiting for restore to complete. Current phase: InProgress\nINFO[0006] Waiting for restore to complete. Current phase: InProgress\nINFO[0009] Waiting for restore to complete. Current phase: InProgress\nINFO[0012] Waiting for restore to complete. Current phase: InProgress\nINFO[0015] Waiting for restore to complete. Current phase: InProgress\nINFO[0018] Waiting for restore to complete. Current phase: InProgress\nINFO[0021] Waiting for restore to complete. Current phase: InProgress\nINFO[0024] Waiting for restore to complete. Current phase: InProgress\nINFO[0027] Waiting for restore to complete. Current phase: Completed\nINFO[0027] Restore test-20240507173309 completed successfully\n</code></pre> <p>To list the restores, run:</p> <pre><code>mkectl restore list\n</code></pre> <p>Example output:</p> <pre><code>mkectl restore list\nNAME                  BACKUP   STATUS      STARTED                         COMPLETED                       ERRORS   WARNINGS   CREATED                         SELECTOR\ntest-20240507173309   test     Completed   2024-05-07 17:33:09 -0400 EDT   2024-05-07 17:33:34 -0400 EDT   0        121        2024-05-07 17:33:09 -0400 EDT   &lt;none&gt;\n</code></pre> <p>To view detailed logs, access the MinIO UI.</p>"},{"location":"configuration/backup-restore/in-cluster/#access-the-minio-console","title":"Access the MinIO Console","text":"<p>To access the MinIO Console:</p> <ol> <li>Obtain the username from your cluster:</li> </ol> <pre><code>kubectl --kubeconfig &lt;path_to_kubeconfig&gt; get secret -n mke minio-credentials -o jsonpath='{.data.root-user}' | base64 -d; echo;\n</code></pre> <ol> <li>Obtain the password from your cluster:</li> </ol> <p><code>shell    kubectl --kubeconfig &lt;path_to_kubeconfig&gt; get secret -n mke minio-credentials -o jsonpath='{.data.root-password}' | base64 -d; echo;</code></p> <ol> <li>Navigate to the external address for your ingress controller under <code>/minio/</code>.</li> </ol> <p>Example:    <code>https://&lt;external_address&gt;/minio/</code></p> <ol> <li>Log in using the username and password. The Velero bucket displays under the Object browser, and you can download or upload backups, using the options provided by the MinIO UI.</li> </ol> <p></p>"},{"location":"configuration/cloudproviders/_index/","title":"Cloud providers","text":"<p>With MKE 4, you can deploy a cloud provider to integrate your MKE 4k cluster with cloud provider service APIs.</p> <p>{{&lt; callout type=\"info\" &gt;}} AWS is currently the only managed cloud service provider add-on that MKE 4k supports. You can use a different cloud service provider; however, you must change the <code>provider</code> parameter under <code>cloudProvider</code> in the <code>mke4.yaml</code> configuration file to <code>external</code> prior to installing that provider:</p> <p><pre><code>  cloudProvider:\n    enabled: true\n    provider: external\n</code></pre> {{&lt; /callout &gt;}}</p>"},{"location":"configuration/cloudproviders/_index/#prerequisites","title":"Prerequisites","text":"<p>Refer to the documentation for your chosen cloud service provider to ascertain any proprietary requirements.</p> <p>To use the MKE 4k managed AWS Cloud Provider, you must first ensure that your nodes have certain IAM policies. For detailed information, refer to the official AWS Cloud Provider documentation IAM Policies.</p>"},{"location":"configuration/cloudproviders/_index/#configuration","title":"Configuration","text":"<p>To enable cloud provider support, which is disabled by default, change the <code>enabled</code> parameter under <code>cloudProvider</code> in the <code>mke4.yaml</code> configuration file to <code>true</code>:</p> <pre><code>  cloudProvider:\n    enabled: true\n    provider: aws\n</code></pre> <p>The <code>cloudProvider</code> configuration parameters are detailed in the following table:</p> Field Description Default <code>enabled</code> Enables cloud provider flags on MKE 4k components. <code>false</code> <code>provider</code> Either <code>aws</code> or <code>external</code>. If \"external\" is specified the user is responsible for installing their own cloud provider. \"\"    ``"},{"location":"configuration/cloudproviders/_index/#create-an-nlb-with-aws-cloud-provider","title":"Create an NLB with AWS Cloud Provider","text":"<p>The example below illustrates how you can use cloud provider AWS to create a Network Load Balancer (NLB) in your MKE 4k cluster. </p> <p>Once you have enabled the cloud provider through the <code>mke4.yaml</code> configuration file and have applied it, you can create an NLB as follows:</p> <ol> <li>Create a sample nginx deployment:</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl --kubeconfig ~/.mke/mke.kubeconf apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3  \n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:latest\n        ports:\n        - containerPort: 80\nEOF\n</code></pre> <ol> <li>Create a service of type <code>LoadBalancer</code>:</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl --kubeconfig ~/.mke/mke.kubeconf apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\nspec:\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\nEOF\n</code></pre> <ol> <li>Check the status of the service:</li> </ol> <pre><code>kubectl --kubeconfig ~/.mke/mke.kubeconf get service\nNAME            TYPE           CLUSTER-IP     EXTERNAL-IP                                                                        PORT(S)        AGE\nkubernetes      ClusterIP      10.96.0.1      &lt;none&gt;                                                                             443/TCP        14m\nnginx-service   LoadBalancer   10.96.177.89   afdf81e0681274c52acbb7b45add87a1-637d0d850105ea92.elb.ca-central-1.amazonaws.com   80:32927/TCP   63s\n</code></pre> <p>The load balancer should now be visible in the AWS console.</p> <p></p> \u00d7 <p>Once the load balancer finishes provisioning, you should be able to access nginx through the external IP.</p> <p></p> \u00d7"},{"location":"configuration/container-network-interface/_index/","title":"Container Network Interfaces","text":"<p>Being able to correctly configure and customize Container Network Interface (CNI) plugins is essential for setting up a reliable and scalable MKE 4k environment.</p> <p>{{&lt; callout type=\"warning\" &gt;}} To switch to a different CNI following initial CNI installation, you must first reset your MKE 4k cluster. {{&lt; /callout &gt;}}</p> <p>MKE 4k currently supports the following CNI plugins:</p> <ul> <li>Calico OSS</li> </ul> <p>{{&lt; callout type=\"important\" &gt;}} Calico OSS is the only CNI that is supported for migrating configuration during an MKE 3 to MKE 4k upgrade. {{&lt; /callout &gt;}}</p>"},{"location":"configuration/container-network-interface/cni-configuration-example/","title":"CNI Configuration Example","text":"<p>The <code>network</code> section of the <code>mke4.yaml</code> configuration file renders as follows:</p> <pre><code>network:\n    cplb:\n      disabled: true\n    kubeProxy:\n      iptables:\n        minSyncPeriod: 0s\n        syncPeriod: 0s\n      ipvs:\n        minSyncPeriod: 0s\n        syncPeriod: 0s\n        tcpFinTimeout: 0s\n        tcpTimeout: 0s\n        udpTimeout: 0s\n      metricsBindAddress: 0.0.0.0:10249\n      mode: iptables\n      nftables:\n        minSyncPeriod: 0s\n        syncPeriod: 0s\n    multus:\n      enabled: false\n    nllb:\n      disabled: true\n    nodePortRange: 32768-35535\n    providers:\n    - enabled: true\n      extraConfig:\n        cidrV4: 192.168.0.0/16\n        linuxDataplane: Iptables\n        loglevel: Info\n      provider: calico\n    - enabled: false\n      provider: custom\n    - enabled: false\n      extraConfig:\n        cidrV4: 192.168.0.0/16\n        v: \"5\"\n      provider: kuberouter\n    serviceCIDR: 10.96.0.0/16\n</code></pre>"},{"location":"configuration/container-network-interface/cni-limitations/","title":"Limitations","text":"<p>It is important to be familiar with the MKE 4k limitations related to Container Network Interfaces before deploying a CNI plugin.</p> <ul> <li>MKE 4k does not support Calico Enterprise.</li> <li>MKE 4k does not support IPVS.</li> <li>Only clusters that use the default Kubernetes proxier <code>iptables</code> can be   upgraded from MKE 3 to MKE 4k.</li> <li>Only KDD-backed MKE 3 clusters can be upgraded to MKE 4k. Refer to Upgrade   from MKE 3.7 or 3.8 for more information.</li> </ul>"},{"location":"configuration/container-network-interface/configure-cni-providers/","title":"Configure CNI Providers","text":"<p>Information is provided herein on how to configure the Container Network Interfaces that MKE 4k supports.</p>"},{"location":"configuration/container-network-interface/configure-cni-providers/#calico-oss","title":"Calico OSS","text":"Calico CNI configuration parameters  The following table includes details on the configurable settings for the Calico provider.  | Field   | Description  | Values        |  Default     | |---------|--------------|---------------|--------------| | `enabled` | Sets the name of the external storage provider. AWS is currently the only available option. | `true` | `true` | | `cidrV4` | Sets the IP pool in the Kubernetes cluster from which Pods are allocated. | Valid IPv4 CIDR | `192.168.0.0/16` You can easily modify `cidrV4` prior to cluster deployment. Contact Mirantis Support, however, if you need to modify `clusterCIDRIPv4` once your cluster has been deployed.| | `linuxDataplane` | Sets the dataplane for Calico CNI. | Iptables | Iptables| | `loglevel` | Sets the log level for the CNI components. | Info, Debug | Info|   <p>The default network configuration described herein offers a serviceable, low maintenance solution. If, however, you want more control over your network configuration environment, MKE 4k exposes maximal configuration for the Calico CNI through which you can configure your networking to the fullest extent allowed by the provider. For this, you will use the <code>values.yaml</code> key, in which case an example networking would resemble the following:</p> <pre><code> network:\n    cplb:\n      disabled: true\n    kubeProxy:\n      iptables:\n        minSyncPeriod: 0s\n        syncPeriod: 0s\n      ipvs:\n        minSyncPeriod: 0s\n        syncPeriod: 0s\n        tcpFinTimeout: 0s\n        tcpTimeout: 0s\n        udpTimeout: 0s\n      metricsBindAddress: 0.0.0.0:10249\n      mode: iptables\n      nftables:\n        minSyncPeriod: 0s\n        syncPeriod: 0s\n    multus:\n      enabled: false\n    nllb:\n      disabled: true\n    nodePortRange: 32768-35535\n    serviceCIDR: 10.96.0.0/16\n    providers:\n    - enabled: true\n      extraConfig:\n        loglevel: Info\n        values.yaml: |-\n          kubeletVolumePluginPath: &lt;mke4.yaml kubelet.kubeletRootDir setting&gt;\n          installation:\n            logging:\n              cni:\n                logSeverity: Debug\n            cni:\n              type: Calico\n            calicoNetwork:\n              linuxDataplane: Iptables\n              ipPools:\n              - cidr: 192.168.0.0/15\n                encapsulation: VXLAN\n          resources:\n            requests:\n              cpu: 250m\n          defaultFelixConfiguration:\n            enabled: true\n            wireguardEnabled: false\n            wireguardEnabledV6: false\n      provider: calico\n    - enabled: false\n      provider: custom\n    - enabled: false\n      extraConfig:\n        cidrV4: 192.168.0.0/16\n        v: \"5\"\n      provider: kuberouter\n</code></pre> <p>{{&lt; callout type=\"important\" &gt;}}</p> <ul> <li>You must choose whether to specify an exact YAML specification for the Helm installation of Tigera Operator during the initial cluster installation.</li> <li>The supplied YAML for <code>values.yaml</code> must include the exact first line <code>kubeletVolumePluginPath: &lt;mke4.yaml kubelet.kubeletRootDir setting&gt;</code>, otherwise the MKE 4k installation will fail.</li> </ul> <p>{{&lt; /callout &gt;}}</p> <p>{{&lt; callout type=\"info\" &gt;}} Refer to the official Tigera Operator documentation for:</p> <ul> <li>Information on how to prepare the required content for the <code>values.yaml</code> specification</li> <li>The <code>values.yaml</code> information content</li> <li>The [<code>defaultFelixConfiguration</code> content for the <code>values.yaml</code> specification(https://docs.tigera.io/calico/latest/reference/resources/felixconfig)</li> </ul> <p>You can view the full <code>values.yaml</code> specification for the Helm chart needed to install Tigera Operator at the Project Calico GitHub.</p> <p>{{&lt; /callout &gt;}}</p> <p>The network configuration generated as a result of upgrading to MKE 4k from an existing MKE 3 cluster always uses YAML. As such clusters have at least one existing IP pool, however, the CIDR and dataplane values are specified outside of the YAML, as illustrated below:</p> <pre><code>  network:\n    cplb:\n      disabled: true\n    kubeProxy:\n      iptables:\n        minSyncPeriod: 0s\n        syncPeriod: 0s\n      ipvs:\n        minSyncPeriod: 0s\n        syncPeriod: 0s\n        tcpFinTimeout: 0s\n        tcpTimeout: 0s\n        udpTimeout: 0s\n      metricsBindAddress: 0.0.0.0:10249\n      mode: iptables\n      nftables:\n        minSyncPeriod: 0s\n        syncPeriod: 0s\n    multus:\n      enabled: false\n    nllb:\n      disabled: true\n    nodePortRange: 30000-32768\n    serviceCIDR: 10.96.0.0/16\n    providers: \n    - enabled: true\n      extraConfig:\n        cidrV4: 192.168.0.0/15\n        linuxDataplane: Iptables\n        loglevel: DEBUG\n        values.yaml: |-\n          kubeletVolumePluginPath: &lt;mke4.yaml kubelet.kubeletRootDir setting&gt;\n          installation:\n            registry: ghcr.io/mirantiscontainers/\n            cni:\n              type: Calico\n            calicoNetwork:\n              bgp: Disabled\n              linuxDataplane: Iptables\n          resources:\n            requests:\n              cpu: 250m\n          tigeraOperator:\n            version: v1.37.1\n            registry: ghcr.io/mirantiscontainers/\n          defaultFelixConfiguration:\n            enabled: true\n            bpfConnectTimeLoadBalancing: TCP\n            bpfHostNetworkedNATWithoutCTLB: Enabled\n            bpfLogLevel: Debug\n            floatingIPs: Disabled\n            logSeverityScreen: Debug\n            logSeveritySys: Debug\n            reportingInterval: 0s\n            vxlanPort: 4789\n            vxlanVNI: 10037\n      provider: calico\n    - enabled: false\n      provider: custom\n</code></pre> <p>{{&lt; callout type=\"info\" &gt;}} - MKE 4k uses a static port range for Kubernetes NodePorts, from  <code>32768</code> to <code>35535</code>.  - Following a successful MKE 3 to MKE 4k upgrade, a list displays that details the ports that no longer need to be opened on manager or worker nodes. These ports can be blocked. {{&lt; /callout &gt;}}</p> <p>{{&lt; callout type=\"tip\" &gt;}} Refer to kubelet custom profiles for information on the <code>kubelet.kubeletRootDir</code> setting. {{&lt; /callout &gt;}}</p>"},{"location":"configuration/container-network-interface/enable-cni-providers/","title":"Enable CNI Providers","text":"<p>To enable a Container Network Interface for MKE 4k:</p> <ol> <li>Obtain the default <code>mke4.yaml</code> configuration file:</li> </ol> <pre><code>mkectl init\n</code></pre> <ol> <li> <p>In the <code>providers</code> section of the <code>mke4.yaml</code> configuration file, set the    <code>enabled</code> parameter for the CNI you want to deploy to <code>true</code> and the    <code>enabled</code> parameter for the CNIs you do not want to deploy to <code>false</code>.</p> </li> <li> <p>Apply the configuration:</p> </li> </ol> <pre><code>mkectl apply -f mke4.yaml\n</code></pre> <ol> <li>Verify the successful deployment of your chosen CNI in the MKE 4k cluster.</li> </ol> To verify Calico OSS CNI     Run the following command:     <pre><code>k0s kc get po --show-labels -A|grep -i -e tigera -e calico\n</code></pre>     Example output:     <pre><code>calico-apiserver   calico-apiserver-5f6bbbcd-9lqsk                              1/1     Running   0          34m   apiserver=true,app.kubernetes.io/name=calico-apiserver,k8s-app=calico-apiserver,pod-template-hash=5f6bbbcd\ncalico-apiserver   calico-apiserver-5f6bbbcd-mxqq7                              1/1     Running   0          34m   apiserver=true,app.kubernetes.io/name=calico-apiserver,k8s-app=calico-apiserver,pod-template-hash=5f6bbbcd\ncalico-system      calico-kube-controllers-64764dc585-xlcl7                     1/1     Running   0          34m   app.kubernetes.io/name=calico-kube-controllers,k8s-app=calico-kube-controllers,pod-template-hash=64764dc585\ncalico-system      calico-node-cx452                                            1/1     Running   0          33m   app.kubernetes.io/name=calico-node,controller-revision-hash=c9788bcc,k8s-app=calico-node,pod-template-generation=2\ncalico-system      calico-node-lfwrv                                            1/1     Running   0          33m   app.kubernetes.io/name=calico-node,controller-revision-hash=c9788bcc,k8s-app=calico-node,pod-template-generation=2\ncalico-system      calico-typha-658d6d7f94-f54t7                                1/1     Running   0          34m   app.kubernetes.io/name=calico-typha,k8s-app=calico-typha,pod-template-hash=658d6d7f94\ncalico-system      csi-node-driver-8q2g8                                        2/2     Running   0          34m   app.kubernetes.io/name=csi-node-driver,controller-revision-hash=6545d9b9d5,k8s-app=csi-node-driver,name=csi-node-driver,pod-template-generation=1\ncalico-system      csi-node-driver-nsdgr                                        2/2     Running   0          34m   app.kubernetes.io/name=csi-node-driver,controller-revision-hash=6545d9b9d5,k8s-app=csi-node-driver,name=csi-node-driver,pod-template-generation=1\ntigera-operator    tigera-operator-588c6fd5d4-wr5xc                             1/1     Running   0          34m   k8s-app=tigera-operator,name=tigera-operator,pod-template-hash=588c6fd5d4\n</code></pre>"},{"location":"configuration/container-network-interface/network-configuration/","title":"Network Configuration","text":"<p>The following table includes details on all of the configurable <code>network</code> fields.</p> Field Description Values Default <code>serviceCIDR</code> Sets the IPv4 range of IP addresses for services in a Kubernetes cluster. Valid IPv4 CIDR <code>10.96.0.0/16</code> <code>providers</code> Sets the provider for the active CNI. <code>calico</code> <code>calico</code>"},{"location":"configuration/ingress/_index/","title":"Ingress controller","text":"<p>Traffic that originates outside of your cluster, ingress traffic, is managed through the use of an ingress controller. By default, MKE 4k offers NGINX Ingress Controller, which manages ingress traffic using the Kubernetes Ingress rules.</p> <p>NGINX Ingress Controller is the only one ingress controller that MKE 4k currently supports.</p>"},{"location":"configuration/ingress/_index/#configuration","title":"Configuration","text":"<p>You can configure NGINX Ingress Controller through the <code>ingressController</code> section of the <code>mke4.yaml</code> configuration file. The function is enabled by default and must not be disabled for the cluster to function correctly.</p> <p>Ingress controller parameters that you can configure are detailed in the following table.</p> Field Description Default replicaCount Sets the number of NGINX Ingress Controller deployment replicas. 2 enableLoadBalancer Enables an external load balancer. Valid values: <code>true</code>, <code>false</code>. <code>true</code> if <code>apiServer.externalAddress</code> is set in the config file; <code>false</code> otherwise extraArgs Additional command line arguments to pass to Ingress-Nginx Controller. {} (empty) extraArgs.httpPort Sets the container port for servicing HTTP traffic. <code>80</code> extraArgs.httpsPort Sets the container port for servicing HTTPS traffic. <code>443</code> extraArgs.enableSslPassthrough Enables SSL passthrough. false preserveClientIP Enables preserving inbound traffic source IP. Valid values: <code>true</code>, <code>false</code>. <code>false</code> externalIPs Sets the list of external IPs for Ingress service. IP addresses of managers nodes are always added automatically. [] affinity Sets node affinity. Example  Affinity is always configured to schedule ingress controller pods on manager nodes. Additional rules may be added, but it's not recommended.   For more information, refer to the Kubernetes documentation Affinity and anti-affinity. {} (empty) tolerations Sets node toleration. Example  Tolerations are always configured to allow scheduling on manager nodes. Additional rules may be added, but it's not recommended.   Refer to the Kubernetes documentation Assigning Pods to Nodes for more detail. [] (empty) configMap Adds custom configuration options to Nginx.    For a complete list of available options, refer to the NGINX Ingress Controller ConfigMap. {} (empty) tcpServices Sets TCP service key-value pairs; enables TCP services. Example   Refer to the NGINX Ingress documentation Exposing TCP and UDP services for more information. for more information. {} (empty) udpServices Sets UDP service key-value pairs; enables UDP services. Example   Refer to the NGINX Ingress documentation Exposing TCP and UDP services for more information. {} (empty) nodePorts Sets the node ports for the external HTTP/HTTPS/TCP/UDP listener. You should not change the HTTPS port, but if you do so, make sure to change the target port of the MKE 4k Dashboard in your load balancer configuration. Refer to System requirements for more information. HTTP: 33000, HTTPS: 33001 ports Sets the port for the internalHTTP/HTTPS listener. HTTP: 80, HTTPS: 443 disableHttp Disables the HTTP listener. false"},{"location":"configuration/ingress/_index/#node-ports-operation-for-mke-3-to-mke-4k-upgrade","title":"Node ports operation for MKE 3 to MKE 4k upgrade","text":"<p>The handling of node ports during an upgrade from MKE 3 to MKE 4k differs, depending on several factors, as illustrated below:</p> MKE 3\u00a0NodePort\u00a0Range Ingress\u00a0Controller in MKE 3 Ingress NodePorts\u00a0in\u00a0MKE 4k Default Enabled Uses the ports set in MKE 3. Default Disabled Default ports:HTTP: 33000HTTPS: 33001 Custom Enabled Uses the ports set in MKE 3. Custom Disabled Reserves the first two static ports from the NodePort range for the Ingress Controller. <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>The calculation of the static NodePort range is calculated based on the official Kubernetes documentation How can you avoid NodePort Service port conflicts?.</p> <p>{{&lt; /callout &gt;}}</p> <p>Thus, if the NodePort range is <code>30000-32767</code>, the NodePorts for the ingress controller will be <code>30000</code> and <code>30001</code>.</p>"},{"location":"configuration/ingress/_index/#affinity","title":"Affinity","text":"<p>You can specify node affinities using the <code>ingressController.affinity.nodeAffinity</code> field in the <code>mke4.yaml</code> configuration file.</p> <p>The following example uses <code>requiredDuringSchedulingIgnoredDuringExecution</code> to schedule the ingress controller pods.</p> <pre><code>ingressController:\n  enabled: true\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: kubernetes.io/hostname\n                operator: In\n                values:\n                  - ip-172-31-42-30\n</code></pre>"},{"location":"configuration/ingress/_index/#tolerations","title":"Tolerations","text":"<p>You can set Node tolerations for server scheduling to nodes with taints using the <code>ingressController.tolerations</code> field in the <code>mke4.yaml</code> configuration file.</p> <p>The following example uses a toleration with <code>NoExecute</code> effect.</p> <pre><code>ingressController:\n  enabled: true\n  tolerations:\n  - key: \"key1\"\n    operator: \"Equal\"\n    value: \"value1\"\n    effect: \"NoExecute\"\n</code></pre>"},{"location":"configuration/ingress/_index/#example-ingress-controller-configuration","title":"Example ingress controller configuration","text":"<pre><code>ingressController:\n  enabled: true\n  enableLoadBalancer: false\n  numReplicas: 1\n  preserveClientIP: true\n  tolerations:\n    - key: \"key1\"\n      operator: \"Equal\"\n      value: \"value1\"\n      effect: \"NoExecute\"\n  extraArgs:\n    httpPort: 80\n    httpsPort: 443\n    enableSslPassthrough: false\n  configMap:\n    access-log-path: \"/var/log/nginx/access.log\"\n    generate-request-id: \"true\"\n    use-forwarded-headers: \"true\"\n    error-log-path: \"/var/log/nginx/error.log\"\n  tcpServices:\n    9000: \"default/tcp-echo:9000\"\n  udpServices:\n    5005: \"default/udp-listener:5005\"\n  nodePorts:\n    http: 33000\n    https: 33001\n    tcp:\n      9000: 33011\n    udp:\n      5005: 33012\n  ports:\n    http: 8080\n    https: 4443\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: kubernetes.io/hostname\n                operator: In\n                values:\n                  - ip-172-31-42-30\n</code></pre>"},{"location":"configuration/ingress/_index/#mke-version-comparison-ingress-configuration-parameters","title":"MKE version comparison: Ingress configuration parameters","text":"MKE 3 MKE 4k [cluster_config.ingress_controller.enabled] ingressController.enabled [cluster_config.ingress_controller.ingress_num_replicas] ingressController.numReplicas [cluster_config.ingress_controller.ingress_enable_lb] ingressController.enableLoadBalancer [cluster_config.ingress_controller.ingress_preserve_client_ip] ingressController.preserveClientIP [[cluster_config.ingress_controller.ingress_node_toleration]]  key = \"com.docker.ucp.manager\"  value = \"\"  operator = \"Exists\" effect = \"NoSchedule\" ingressController.tolerations  - key: \"key1\"  operator: \"Equal\"  value: \"value1\"  effect: \"NoExecute\" [cluster_config.ingress_controller.ingress_config_map] ingressController.configMap [cluster_config.ingress_controller.ingress_tcp_services]  9000 = \"default/tcp-echo:9000\" ingressController.tcpServices:9000: \"default/tcp-echo:9000\" [cluster_config.ingress_controller.ingress_udp_services]  5005 = \"default/udp-listener:5005\" ingressController.udpServices: 5005: \"default/udp-listener:5005\" [cluster_config.ingress_controller.ingress_extra_args]  http_port = 8080  https_port = 4443  enable_ssl_passthrough = true  default_ssl_certificate = \"\" ingressController.extraArgs:  httpPort: 0  httpsPort: 0  enableSslPassthrough: true  defaultSslCertificate: \"\" [cluster_config.ingress_controller.ingress_node_affinity] ingressController.affinity [[cluster_config.ingress_controller.ingress_exposed_ports]]  name = \"http2\"  port = 80  target_port = 8080  node_port = 33001  protocol = \"\" Deprecated in MKE 4k.  The http and https ports are enabled by default on 80 and 443 respectively.  If the user wants to change it, they can use ingressController.ports. NodePorts for http and https can be configured via ingressController.nodePorts. The default values are 33000 and 33001 respectively.  For information on how to configure TCP/UDP ports, refer to the TCP and UDP services documentation."},{"location":"configuration/ingress/tcp-udp-services/","title":"TCP and UDP services","text":"<p>The Kubernetes ingress resource only supports services over HTTP and HTTPS. Using NGINX Ingress Controller, however, you can receive external TCP/UDP traffic from non-HTTP protocols and route them to internal services using TCP/UDP port mappings.</p>"},{"location":"configuration/ingress/tcp-udp-services/#expose-a-tcp-service","title":"Expose a TCP service","text":"<p>To expose TCP services, set the following parameters in the <code>mke4.yaml</code> configuration file.</p> Field Description ingressController.tcpServices Indicates TCP service key-value pairs. ingressController.nodePorts.tcp Sets node port mapping for external TCP listeners. <p>In the example procedure, a <code>tcp-echo</code> service that is running in the default namespace on port 9000 is exposed using the port 9000, on <code>NodePort</code> 33011.</p> <ol> <li>Deploy a sample TCP service listening on port 9000, to echo back any text it    receives with the prefix <code>hello</code>:</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: tcp-echo\n  labels:\n    app: tcp-echo\n    service: tcp-echo\nspec:\n  selector:\n    app: tcp-echo\n  ports:\n    - name: tcp\n      port: 9000\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tcp-echo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tcp-echo\n  template:\n    metadata:\n      labels:\n        app: tcp-echo\n    spec:\n      containers:\n        - name: tcp-echo\n          image: docker.io/istio/tcp-echo-server:1.2\n          imagePullPolicy: IfNotPresent\n          args: [ \"9000\", \"hello\" ]\n          ports:\n            - containerPort: 9000\nEOF\n</code></pre> <ol> <li>Verify that the deployment was created correctly:</li> </ol> <pre><code>kubectl get deploy tcp-echo\n</code></pre> <p>Example output:</p> <pre><code>NAME           READY   UP-TO-DATE   AVAILABLE   AGE\ntcp-echo       1/1     1            1           39s\n</code></pre> <ol> <li>Verify that the service is running:</li> </ol> <pre><code>kubectl get service tcp-echo\n</code></pre> <p>Example output:</p> <pre><code>NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\ntcp-echo       ClusterIP   10.96.172.90   &lt;none&gt;        9000/TCP   46s\n</code></pre> <ol> <li> <p>Configure Ingress Controller to expose the TCP service:</p> </li> <li> <p>Verify that the <code>enabled</code> parameter for the <code>ingressController</code> option in       the <code>mke4.yaml</code> configuration file is set to <code>true</code>.</p> </li> <li> <p>Modify the <code>mke4.yaml</code> configuration file to expose the newly created TCP       service:</p> <pre><code>ingressController:\n  enabled: true\n  tcpServices:\n    \"9000\": default/tcp-echo:9000\n  nodePorts:\n    tcp:\n      9000: 33011\n</code></pre> </li> <li> <p>Apply the <code>mke4.yaml</code> configuration file:</p> </li> </ol> <pre><code>mkectl apply  -f mke4.yaml\n</code></pre> <ol> <li>Test the TCP service by sending the text <code>hello world</code>:</li> </ol> <pre><code>echo \"world\" | netcat &lt;WORKER_NODE_IP&gt; 33011\nhello world\n</code></pre> <p>The service should respond with <code>hello world</code>.</p> <ol> <li> <p>Check the tcp-echo logs:</p> </li> <li> <p>Obtain the Pod name:</p> <pre><code>kubectl get pods --selector=app=tcp-echo\n</code></pre> <p>Example output:</p> <pre><code>NAME                        READY   STATUS    RESTARTS   AGE\ntcp-echo-544849bd8f-6jscx   1/1     Running   0          45h\n</code></pre> </li> <li> <p>Access the log:</p> <pre><code>kubectl logs tcp-echo-544849bd8f-6jscx\n</code></pre> <p>Example output:</p> <pre><code>listening on [::]:9000, prefix: hello\nrequest: world\nresponse: hello world\n</code></pre> </li> <li> <p>Remove the Kubernetes resources, which are no longer needed:</p> </li> </ol> <pre><code>kubectl delete service tcp-echo\nkubectl delete deployment tcp-echo\n</code></pre>"},{"location":"configuration/ingress/tcp-udp-services/#expose-a-udp-service","title":"Expose a UDP service","text":"<p>To expose UDP services, set the following parameters in the <code>mke4.yaml</code> configuration file.</p> Field Description ingressController.udpServices Indicates UDP service key-value pairs. ingressController.nodePorts.udp Sets node port mapping for external UDP listeners. <p>In the example procedure, a <code>udp-listener</code> service running in the default namespace on port 5005 is exposed using the port 5005, on <code>NodePort</code> 33012.</p> <ol> <li>Deploy a sample UDP service listening on port 5005:</li> </ol> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: udp-listener\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: udp-listener\n  labels:\n    app: udp-listener\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: udp-listener\n  template:\n    metadata:\n      labels:\n        app: udp-listener\n    spec:\n      containers:\n        - name: udp-listener\n          image: mendhak/udp-listener\n          ports:\n            - containerPort: 5005\n              protocol: UDP\n              name: udp\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: udp-listener\nspec:\n  ports:\n    - port: 5005\n      targetPort: 5005\n      protocol: UDP\n      name: udp\n  selector:\n    app: udp-listener\nEOF\n</code></pre> <ol> <li>Verify that the deployment was created correctly:</li> </ol> <pre><code>kubectl get deploy udp-listener\n</code></pre> <p>Example output:</p> <pre><code>NAME           READY   UP-TO-DATE   AVAILABLE   AGE\nudp-listener   1/1     1            1           31s\n</code></pre> <ol> <li>Verify that the service is running:</li> </ol> <pre><code>kubectl get service udp-listener\n</code></pre> <p>Example output:</p> <pre><code>NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nudp-listener   ClusterIP   10.96.19.229   &lt;none&gt;        5005/UDP   37s\n</code></pre> <ol> <li> <p>Configure Ingress Controller to expose the UDP service:</p> </li> <li> <p>Verify that the <code>enabled</code> parameter for the <code>ingressController</code> option in       the <code>mke4.yaml</code> configuration file is set to <code>true</code>.</p> </li> <li> <p>Modify the <code>mke4.yaml</code> configuration file to expose the newly created UDP       service:</p> <pre><code>ingressController:\n  enabled: true\n  udpServices:\n    \"5005\": default/udp-listener:5005\n  nodePorts:\n    udp:\n      5005: 33012\n</code></pre> </li> <li> <p>Apply the <code>mke4.yaml</code> configuration file:</p> </li> </ol> <pre><code>mkectl apply  -f mke4.yaml\n</code></pre> <ol> <li>Test the UDP service by sending the UDP Datagram Message:</li> </ol> <pre><code>echo \"UDP Datagram Message\" | netcat -v -u &lt;WORKER_NODE_IP&gt; 33012\n</code></pre> <ol> <li> <p>Check the <code>udp-listener</code> logs:</p> </li> <li> <p>Obtain the Pod name:</p> <pre><code>kubectl get pods --selector=app=udp-listener\n</code></pre> <p>Example output:</p> <pre><code>NAME                           READY   STATUS    RESTARTS   AGE\nudp-listener-f768d8db4-v69jr   1/1     Running   0          44h\n</code></pre> </li> <li> <p>Access the log:</p> <pre><code>kubectl logs udp-listener-f768d8db4-v69jr\n</code></pre> <p>Example output:</p> <pre><code>Listeningon UDP port 5005\nUDP Datagram Message\n</code></pre> </li> <li> <p>Remove the Kubernetes resources, which are no longer needed:</p> </li> </ol> <pre><code>kubectl delete service udp-listener\nkubectl delete deployment udp-listener\n</code></pre>"},{"location":"configuration/kubernetes/_index/","title":"Kubernetes components","text":"<p>MKE 4k uses K0s to deploy core Kubernetes components, including:</p> <ul> <li>kubelet</li> <li>kube-apiserver</li> <li>kube-controller-manager</li> <li>kube-scheduler</li> <li>etcd</li> </ul> <p>You can configure all of the Kubernetes components through the <code>mke4.yaml</code> configuration file.</p>"},{"location":"configuration/kubernetes/etcd/","title":"etcd","text":"<p>etcd is a consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines. It handles leader elections during network partitions and can tolerate machine failure, even in the leader node.</p> <p>For MKE 4k, etcd serves as the Kubernetes backing store for all cluster data, with an etcd replica deployed on each MKE 4k manager node. This is a primary reason why Mirantis recommends that you deploy an odd number of MKE 4k manager nodes, as etcd uses the Raft consensus algorithm and thus requires that a quorum of nodes agrees on any updates to the cluster state.</p> <p>For detailed information, refer to the official etcd documentation.</p>"},{"location":"configuration/kubernetes/etcd/#configure-etcd","title":"Configure etcd","text":"<p>You can configure etcd through the <code>etcd</code> section of the <code>mke4.yaml</code> configuration file, an example of which follows:</p> <pre><code>spec:\n  etcd:\n    storageQuota: 2GB\n    tlsCipherSuites: TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\n</code></pre>"},{"location":"configuration/kubernetes/etcd/#configure-etcd-storage-quota","title":"Configure etcd storage quota","text":"<p>You can control the etcd distributed key-value storage quota using the <code>etcd.storageQuota</code> parameter in the <code>mke4.yaml</code> configuration file. By default, the value of the parameter is 2GB.</p> <p>If you choose to increase the etcd quota, be aware that this quota has a limit and such action should be used in conjunction with other strategies, such as decreasing events TTL to ensure that the etcd database does not run out of space.</p> <p>{{&lt; callout type=\"warning\" &gt;}} If a manager node virtual machine runs out of disk space, or if all of its system memory is depleted, etcd can cause the MKE 4k cluster to move into an irrecoverable state. To prevent this from happening, configure the disk space and the memory of the manager node VMs to levels that are well in excess of the set etcd storage quota. {{&lt; /callout &gt;}}</p> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>For additional information, refer to the etcd official documentation, How to debug large db size issue?</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"configuration/kubernetes/kube-apiserver/","title":"kube-apiserver","text":"<p>The Kubernetes API server validates and configures data for the API objects, which include pods, services, and replication controllers, among others. The server performs REST operations while also serving as the frontend to the shared state of a cluster, through which the other components interact.</p> <p>You can configure the Kubernetes API server for all controllers through the <code>apiServer</code> section of the <code>mke4.yaml</code> configuration file, an example of which follows:</p> <pre><code>spec:\n  apiServer:\n    audit:\n      enabled: false\n      logPath: /var/lib/k0s/audit.log\n      maxAge: 30\n      maxBackup: 10\n      maxSize: 10\n    encryptionProvider: /var/lib/k0s/encryption.cfg\n    eventRateLimit:\n      enabled: false\n    requestTimeout: 1m0s\n</code></pre> <p>You can further configure the Kubernetes API server using the <code>extraArgs</code> field to define flags. This field accepts a list of key-value pairs, which are passed directly to the kube-apiserver process at runtime.</p>"},{"location":"configuration/kubernetes/kube-controller-manager/","title":"kube-controller-manager","text":"<p>The Kubernetes controller manager is a daemon that embeds the core control loops that ship with Kubernetes. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the kube-apiserver, making changes designed to move the current state towards the desired state. Examples of controllers that ship with Kubernetes are the replication controller, endpoints controller, namespace controller, and the service accounts controller.</p> <p>You can configure all Kubernetes controllers through the <code>controllerManager</code> section of the <code>mke4.yaml</code> configuration file, an example of which follows:</p> <pre><code>spec:\n  controllerManager:\n    terminatedPodGCThreshold: 12500\n</code></pre> <p>You can further configure Kubernetes controller manager using the <code>extraArgs</code> field to define flags. This field accepts a list of key-value pairs, which are passed directly to the kube-controller-manager process at runtime.</p>"},{"location":"configuration/kubernetes/kube-scheduler/","title":"kube-scheduler","text":"<p>The Kubernetes scheduler is a control plane process that assigns pods to nodes. It first determines which nodes are valid placements for each pod in the scheduling queue, according to constraints and available resources. Next, kube-scheduler ranks each valid node and binds the pod to a node that is suitable.</p> <p>You can use multiple different schedulers within a cluster; kube-scheduler is the reference implementation.</p> <p>You can configure all Kubernetes controllers through the <code>scheduler</code> section of the <code>mke4.yaml</code> configuration file, an example of which follows:</p> <pre><code>spec:\n  scheduler:\n    profilingEnabled: true\n    bindToAll: true\n</code></pre> <p>You can further configure Kubernetes Scheduler using the <code>extraArgs</code> field to define flags. This field accepts a list of key-value pairs, which are passed directly to the kube-scheduler process at runtime.</p>"},{"location":"configuration/kubernetes/kubelet/","title":"kubelet","text":"<p>The kubelet component runs on each node in a Kubernetes cluster, which serves as the primary administrative agent for each node, monitoring application servers and routing administrative requests to servers. You can configure kubelet for all cluster nodes through the <code>kubelet</code> section of the <code>mke4.yaml</code> configuration file, an example of which follows:</p> <pre><code>spec:\n  kubelet:\n    eventRecordQPS: 50\n    kubeletRootDir: /var/lib/kubelet\n    maxPods: 110\n    podPidsLimit: -1\n    podsPerCore: 0\n    protectKernelDefaults: false\n    seccompDefault: false\n    workerKubeReserved:\n      cpu: 50m\n      ephemeral-storage: 500Mi\n      memory: 300Mi\n    managerKubeReserved:\n      cpu: 250m\n      ephemeral-storage: 4Gi\n      memory: 2Gi\n</code></pre> <p>You can further configure a kubelet using the <code>extraArgs</code> field to define flags. This field accepts a list of key-value pairs, which are passed directly to the kubelet process at runtime.</p> <p>Example extraArgs field configuration:</p> <pre><code>spec:\n  kubelet:\n    extraArgs:\n      event-burst: 100\n      event-qps: 50\n</code></pre> <p>You can also configure a kubelet with custom profiles. Such profiles offer greater control of the <code>KubeletConfiguration</code> and can be targeted to specific hosts.</p>"},{"location":"configuration/kubernetes/kubelet/#kubelet-root-directory","title":"kubelet root directory","text":"<p>The kubelet root directory is a filesystem path that kubelet uses to store its data and to manage kubelet files, such as volume mounts.</p> <p>As with MKE 3, MKE 4k follows the upstream Kubernetes default for the kubelet root directory. It offers configuration around that default, though, to provide the option of using a different location. Be aware, though, of the attendant risks that come with setting a non-default location. For example, Kubernetes uses the device plugins Unix socket under host path <code>/var/lib/kubelet/device</code>, which is a fixed location that does not vary, even when the kubelet root directory is configured to a non-default location.</p> <p>{{&lt; callout type=\"important\" &gt;}}</p> <p>For a managed CNI, if you are using an exact yaml specification, it is imperative that you specify the kubelet root directory location. Various other components may also require specification of the kubelet root directory location in their configuration, as well.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"configuration/kubernetes/kubelet/#kubelet-custom-profiles","title":"kubelet custom profiles","text":"<p>You can deploy custom profiles to configure kubelet on a per-node basis.</p> <p>A kubelet custom profile comprises a profile name and a set of values. The profile name identifies the profile and targets it to specific nodes in the cluster, while the values are merged into the final kubelet configuration that is applied to a target node.</p>"},{"location":"configuration/kubernetes/kubelet/#create-a-custom-profile","title":"Create a custom profile","text":"<p>You can specify custom profiles in the <code>kubelet.customProfiles</code> section of the <code>mke4.yaml</code> configuration file. Profiles must each have a unique name, and values can refer to fields in the kubelet configuration file.</p> <p>For detail on all possible values, refer to the official Kubernetes documentation Set Kubelet Parameters Via A Configuration File.</p> <p>The following example configuration creates a custom profile named <code>worker_profile_1</code> that specifies thresholds for the garbage collection of images and eviction:</p> <pre><code>spec:\n  kubelet:\n    customProfiles:\n      - name: worker_profile_1\n        values:\n          imageGCHighThresholdPercent: 85\n          imageGCLowThresholdPercent: 80\n          evictionHard:\n            imagefs.available: 15%\n            memory.available: 100Mi\n            nodefs.available: 10%\n            nodefs.inodesFree: 5%\n</code></pre> <p>{{&lt; callout type=\"warning\" &gt;}} To ensure your custom profile works correctly:</p> <ul> <li>Cross-check <code>featureGates</code> in the custom profile against the official   Kubernetes list of removed feature gates, as adding a removed feature gate will prevent the kubelet from starting. </li> <li>Include only namespaced <code>sysctls</code> when you configure <code>allowedUnsafeSysctls</code>, as non-namespaced <code>sysctls</code> are unsupported by the kubelet and will prevent   it from starting. {{&lt; /callout &gt;}}</li> </ul>"},{"location":"configuration/kubernetes/kubelet/#apply-a-custom-profile-to-a-node","title":"Apply a custom profile to a node","text":"<p>You can assign a custom profile through the <code>hosts</code> section of the <code>mke4.yaml</code> configuration file, whereas the profile name is an installation time argument for the host.</p> <p>The following example configuration applies the <code>worker_profile_1</code> custom profile to the <code>localhost</code> node:</p> <pre><code>hosts:\n  - role: controller+worker\n    ssh:\n      address: 52.37.200.22\n      keyPath: ~/.ssh/id_rsa\n      port: 22\n      user: ubuntu\n  - role: worker\n    ssh:\n      address: 18.236.186.188\n      keyPath: ~/.ssh/id_rsa\n      port: 22\n      user: ubuntu\n    installFlags:\n      - --profile=worker_profile_1\n</code></pre>"},{"location":"configuration/kubernetes/kubelet/#debug-worker-profiles","title":"Debug worker profiles","text":"<p>If an invalid worker profile is provided, the kubelet assigned to use the profile may fail to start. If a node takes on the <code>NotReady</code> state following the application of a worker profile, it is likely due to an incorrect worker profile configuration.</p> <p>To debug a worker profile:</p> <ol> <li>SSH into the corresponding <code>NotReady</code> node.</li> <li>Check the logs of k0sworker system service <code>journalctl -u k0sworker</code> for errors. </li> <li>If the worker node does not show any errors, SSH into a manager node.</li> <li>Check the logs of the k0scontroller system service    <code>journalctl -u k0scontroller</code> for errors. Repeat the process for every    manager node until you find errors related to kubelet or to worker profiles.</li> </ol> <p>Example of a k0scontroller error caused by an incorrect value for the <code>memoryThrottlingFactor</code> parameter in the worker profile:</p> <pre><code>k0s[1619032]: time=\"2024-11-11 22:25:03\" level=error msg=\"Failed to recover from previously failed reconciliation\" component=workerconfig.Reconciler error=\"failed to generate resources for worker configuration: failed to decode worker profile \\\"worker_profile_1\\\": error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go struct field KubeletConfiguration.memoryThrottlingFactor of type float64\"\n</code></pre>"},{"location":"configuration/kubernetes/kubelet/#precedence-of-kubelet-configuration","title":"Precedence of kubelet configuration","text":"<p>The kubelet configuration of each node is created by merging several different configuration sources. For MKE 4k, the order is as follows:</p> <ol> <li>Structured configuration values specified in the <code>kubelet</code> section of the    <code>mke4.yaml</code> configuration file, which is the lowest precedence.</li> <li>Custom profile values specified in <code>kublelet.customProfiles</code>.</li> <li>Runtime flags specified in <code>kubelet.extraArgs</code>, which is the highest    precedence.</li> </ol> <p>For more information on kubelet configuration value precedence, refer to the official Kubernetes documentation Kubelet configuration merging order.</p>"},{"location":"configuration/metallb/_index/","title":"MetalLB load balancer","text":"<p>MetalLB is a load balancer designed for bare metal Kubernetes clusters that uses standard routing protocols.</p>"},{"location":"configuration/metallb/_index/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>An MKE 4k cluster that does not already have load-balancing functionality.</p> </li> <li> <p>A MetalLB-compatible cluster network configuration.</p> </li> <li> <p><code>kube-proxy</code> running in <code>iptables</code> mode.</p> </li> <li> <p>The absence of any cloud provider configuration.</p> </li> </ul>"},{"location":"configuration/metallb/_index/#configuration","title":"Configuration","text":"<p>MetalLB is configured through the <code>addons</code> section of the <code>mke4.yaml</code> configuration file. The function is disabled by default, and thus to use MetalLB you must set the <code>enabled</code> parameter to <code>true</code>.</p> <p>MetalLB example configuration:</p> <pre><code>spec:\n  addons:\n    - chart:\n        name: metallb\n        repo: https://metallb.github.io/metallb\n        values: |\n          controller:\n            tolerations:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n                effect: NoSchedule\n          speaker:\n            frr:\n              enabled: false\n        version: 0.14.7\n      dryRun: false\n      enabled: true\n      kind: chart\n      name: metallb\n      namespace: metallb-system\n</code></pre> <p>{{&lt; callout type=\"info\" &gt;}} The example above presents a MetalLB configuration that is deployed by a Helm chart within the cluster. Free Range Routing (FRR) mode is disabled by default. {{&lt; /callout &gt;}}</p> <p>To configure MetalLB:</p> <ol> <li>Obtain the default <code>mke4.yaml</code> configuration file:</li> </ol> <pre><code>mkectl init\n</code></pre> <ol> <li> <p>Set the <code>enabled</code> parameter for <code>metallb</code> in the <code>addons</code> section to <code>true</code>.</p> </li> <li> <p>Apply the configuration:</p> </li> </ol> <pre><code>mkectl apply -f mke4.yaml\n</code></pre> <ol> <li>Verify the successful deployment of MetalLB in the cluster:</li> </ol> <pre><code>kubectl get pods,services,deployments -n metallb-system -l app.kubernetes.io/name=metallb\n</code></pre> <p>Example output:</p> <pre><code>NAME                                      READY   STATUS    RESTARTS   AGE\npod/metallb-controller-79d6b8bb85-c2hrm   1/1     Running   0          17m\npod/metallb-speaker-ccpdf                 1/1     Running   0          17m\npod/metallb-speaker-x2pgf                 1/1     Running   0          17m\n\nNAME                              TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nservice/metallb-webhook-service   ClusterIP   10.96.8.155   none          443/TCP   17m\n\nNAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/metallb-controller   1/1     1            1           17m\n</code></pre>"},{"location":"configuration/metallb/_index/#configuration-parameters","title":"Configuration parameters","text":"<p>The default configuration parameters for the MetalLB add-on are detailed in the following table:</p> Field Description Default <code>kind</code> Type of add-on. <code>chart</code> <code>enabled</code> Enablement state of add-on. <code>false</code> <code>name</code> Name of the add-on. <code>metallb</code> <code>namespace</code> Namespace used fordeploying MetalLB. <code>metallb-system</code> <code>chart.name</code> Name of the MetalLB Helm chart. <code>metallb</code> <code>chart.repo</code> MetalLB Helm repository. <code>https://metallb.github.io/metallb</code> <code>chart.version</code> Version of the MetalLB Helm chart. <code>0.14.7</code> <code>chart.values.controller.tolerations</code> Tolerations for theMetalLB controller pod. YAML:  - key: node-role.kubernetes.io/master   \u00a0 operator: Exists   \u00a0   effect: NoSchedule <code>chart.values.speaker.frr.enabled</code> Enablement state of FRRwith regard to MetalLB speaker. <code>false</code> <p>An MKE version comparison for MetalLB configuration parameters is offered in the following table:</p> MKE 3 MKE 4k <code>[cluster_config.metallb_config.enabled]</code> <code>addons.metallb.enabled</code> <code>[[cluster_config.metallb_config.metallb_ip_addr_pool]]</code> <code>Deprecated</code>"},{"location":"configuration/metallb/_index/#ip-address-pool-configuration","title":"IP address pool configuration","text":"<p> Support for IP address pool configuration is deprecated in   MKE 4k, and as such the configuration must now be managed independently. </p>"},{"location":"configuration/metallb/_index/#create-ip-address-pools-in-a-fresh-installation","title":"Create IP address pools in a fresh installation","text":"<p>For information on how to create IP address pools, refer to the official MetalLB documentation,   Layer 2 configuration.</p> <p>The following example configuration gives MetalLB control over IPs from 192.168.1.240 to 192.168.1.250, and configures Layer 2 mode.</p> <pre><code>---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: cheap\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.10.0/24\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: empty\n  namespace: metallb-system\n</code></pre> <p>As necessary, refer to the official MetalLB documentation, Advanced AddressPool.</p>"},{"location":"configuration/metallb/_index/#managing-ip-address-pools-during-upgrade","title":"Managing IP address pools during upgrade","text":"<p>During an upgrade from MKE 3 to MKE 4k, if metalLB is enabled in the former the  configured IP address pool details display in the upgrade summary.</p> <p>The following example configuration presents MetalLB enabled in MKE 3:</p> <pre><code>[cluster_config.metallb_config]\n  enabled = true\n\n  [[cluster_config.metallb_config.metallb_ip_addr_pool]]\n    name = \"example1\"\n    external_ip = [\"192.168.10.0/24\", \"192.168.1.0/24\"]\n  [[cluster_config.metallb_config.metallb_ip_addr_pool]]\n    name = \"example2\"\n    external_ip = [\"192.155.10.0/24\"]\n</code></pre> <p>The upgrade summary presents as follows:</p> <pre><code>MetalLB IP address pools\n---------------\nname: example1\nipaddress: [192.168.10.0/24 192.168.1.0/24]\n\nname: example2\nipaddress: [192.155.10.0/24]\n\nPlease make sure that you create these pools after MKE4 installation is complete.\n</code></pre> <p>Refer to the upgrade summary for guidance in the creation of IP address pools. To create the pools, you can use the template that follows or consult the official MetalLB documentation Layer 2 configuration for assistance.</p> <pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: example1\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.10.0/24\n  - 192.168.1.0/24\n---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: example2\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.155.10.0/24\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: empty\n  namespace: metallb-system\n</code></pre>"},{"location":"configuration/metallb/_index/#request-specific-ip-address-pools","title":"Request specific IP address pools","text":"<p>With MetalLB, you can request assignment from a specific address pool, if you prefer a particular IP address type but do not require an exact address. To do this, add <code>metallb.universe.tf/address-pool</code> to your service, setting the annotation value to the name of your chosen address pool. </p> <p>MetalLB example configuration:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    metallb.universe.tf/address-pool: production-public-ips\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: nginx\n  type: LoadBalancer\n</code></pre> <p>For more information, refer to the official MetalLB documentation Requesting specific IPs.</p>"},{"location":"configuration/metallb/_index/#uninstall-metallb","title":"Uninstall MetalLB","text":"<ol> <li> <p>Obtain the <code>mke4.yaml</code> configuration file.</p> </li> <li> <p>Set the <code>enabled</code> field to <code>false</code> to disable MetalLB.</p> </li> </ol> <pre><code>mkectl mke4.yaml\n</code></pre> <ol> <li>Apply the configuration:</li> </ol> <pre><code>mkectl apply -f mke4.yaml\n</code></pre>"},{"location":"configuration/node-feature-discovery/_index/","title":"Node Feature Discovery (NFD)","text":"<p>Node Feature Discovery (NFD) software detects the hardware features that are available on each node in a Kubernetes cluster, and advertises the detected features through node labels. NFD shares this information with kube-scheduler, which uses the information in its work.</p>"},{"location":"configuration/node-feature-discovery/_index/#configuration","title":"Configuration","text":"<p>MKE 4k supports the deployment of NFD as an add-on. Enabled by default, the NFD add-on deploys as a helm chart, version <code>0.16.1</code> in the <code>mke</code> namespace.</p> <p>{{&lt; callout type=\"info\" &gt;}} The <code>mke4.yaml</code> configuration file does not expose parameters for the NFD add-on. {{&lt; /callout &gt;}}</p> <ol> <li>Obtain the default configuration file:</li> </ol> <pre><code>mkectl init\n</code></pre> <ol> <li>Apply the configuration:</li> </ol> <pre><code>mkectl apply -f [config-file]\n</code></pre> <ol> <li>Verify the successful deployment of NFD in the cluster:</li> </ol> <pre><code>kubectl get pods,services,deployments -n mke -l app.kubernetes.io/name=node-feature-discovery\n</code></pre> <p>Sample output:</p> <pre><code>NAME                                                 READY   STATUS    RESTARTS   AGE\npod/node-feature-discovery-gc-5477c88f99-dltjp       1/1     Running   0          24h\npod/node-feature-discovery-master-665757d679-g8kt9   1/1     Running   0          24h\npod/node-feature-discovery-worker-vv28z              1/1     Running   0          24h\npod/node-feature-discovery-worker-zlb5s              1/1     Running   0          24h\n\nNAME                                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/node-feature-discovery-gc       1/1     1            1           24h\ndeployment.apps/node-feature-discovery-master   1/1     1            1           24h\n</code></pre> <ol> <li>Verify the labels on the nodes:</li> </ol> <pre><code>kubectl get nodes -o json | jq '.items[].metadata.labels'\n</code></pre> <p>Sample output:</p> <pre><code>{\n\"beta.kubernetes.io/arch\": \"amd64\",\n\"beta.kubernetes.io/os\": \"linux\",\n\"feature.node.kubernetes.io/cpu-cpuid.ADX\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AESNI\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX2\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512BW\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512CD\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512DQ\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512F\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512VL\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.CMPXCHG8\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.FMA3\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.FXSR\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.FXSROPT\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.HYPERVISOR\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.LAHF\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.MOVBE\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.MPX\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.OSXSAVE\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.SYSCALL\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.SYSEE\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.X87\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XGETBV1\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XSAVE\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XSAVEC\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XSAVEOPT\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XSAVES\": \"true\",\n\"feature.node.kubernetes.io/cpu-hardware_multithreading\": \"true\",\n\"feature.node.kubernetes.io/cpu-model.family\": \"6\",\n\"feature.node.kubernetes.io/cpu-model.id\": \"85\",\n\"feature.node.kubernetes.io/cpu-model.vendor_id\": \"Intel\",\n\"feature.node.kubernetes.io/kernel-config.NO_HZ\": \"true\",\n\"feature.node.kubernetes.io/kernel-config.NO_HZ_IDLE\": \"true\",\n\"feature.node.kubernetes.io/kernel-version.full\": \"5.15.0-1070-aws\",\n\"feature.node.kubernetes.io/kernel-version.major\": \"5\",\n\"feature.node.kubernetes.io/kernel-version.minor\": \"15\",\n\"feature.node.kubernetes.io/kernel-version.revision\": \"0\",\n\"feature.node.kubernetes.io/memory-swap\": \"true\",\n\"feature.node.kubernetes.io/pci-0300_1d0f.present\": \"true\",\n\"feature.node.kubernetes.io/storage-nonrotationaldisk\": \"true\",\n\"feature.node.kubernetes.io/system-os_release.ID\": \"ubuntu\",\n\"feature.node.kubernetes.io/system-os_release.VERSION_ID\": \"20.04\",\n\"feature.node.kubernetes.io/system-os_release.VERSION_ID.major\": \"20\",\n\"feature.node.kubernetes.io/system-os_release.VERSION_ID.minor\": \"04\",\n\"kubernetes.io/arch\": \"amd64\",\n\"kubernetes.io/hostname\": \"ip-172-31-87-104.us-west-2.compute.internal\",\n\"kubernetes.io/os\": \"linux\",\n\"node-role.kubernetes.io/control-plane\": \"true\",\n\"node.k0sproject.io/role\": \"control-plane\"\n}\n{\n\"beta.kubernetes.io/arch\": \"amd64\",\n\"beta.kubernetes.io/os\": \"linux\",\n\"feature.node.kubernetes.io/cpu-cpuid.ADX\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AESNI\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX2\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512BW\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512CD\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512DQ\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512F\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.AVX512VL\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.CMPXCHG8\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.FMA3\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.FXSR\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.FXSROPT\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.HYPERVISOR\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.LAHF\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.MOVBE\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.MPX\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.OSXSAVE\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.SYSCALL\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.SYSEE\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.X87\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XGETBV1\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XSAVE\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XSAVEC\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XSAVEOPT\": \"true\",\n\"feature.node.kubernetes.io/cpu-cpuid.XSAVES\": \"true\",\n\"feature.node.kubernetes.io/cpu-hardware_multithreading\": \"true\",\n\"feature.node.kubernetes.io/cpu-model.family\": \"6\",\n\"feature.node.kubernetes.io/cpu-model.id\": \"85\",\n\"feature.node.kubernetes.io/cpu-model.vendor_id\": \"Intel\",\n\"feature.node.kubernetes.io/kernel-config.NO_HZ\": \"true\",\n\"feature.node.kubernetes.io/kernel-config.NO_HZ_IDLE\": \"true\",\n\"feature.node.kubernetes.io/kernel-version.full\": \"5.15.0-1070-aws\",\n\"feature.node.kubernetes.io/kernel-version.major\": \"5\",\n\"feature.node.kubernetes.io/kernel-version.minor\": \"15\",\n\"feature.node.kubernetes.io/kernel-version.revision\": \"0\",\n\"feature.node.kubernetes.io/memory-swap\": \"true\",\n\"feature.node.kubernetes.io/pci-0300_1d0f.present\": \"true\",\n\"feature.node.kubernetes.io/storage-nonrotationaldisk\": \"true\",\n\"feature.node.kubernetes.io/system-os_release.ID\": \"ubuntu\",\n\"feature.node.kubernetes.io/system-os_release.VERSION_ID\": \"20.04\",\n\"feature.node.kubernetes.io/system-os_release.VERSION_ID.major\": \"20\",\n\"feature.node.kubernetes.io/system-os_release.VERSION_ID.minor\": \"04\",\n\"kubernetes.io/arch\": \"amd64\",\n\"kubernetes.io/hostname\": \"ip-172-31-87-179.us-west-2.compute.internal\",\n\"kubernetes.io/os\": \"linux\"\n</code></pre>"},{"location":"configuration/policycontroller/_index/","title":"Policy Controller","text":"<p>MKE 4k allows installation of third-party policy controllers for Kubernetes. OPA Gatekeeper is currently the only supported policy controller.</p>"},{"location":"configuration/policycontroller/_index/#configuration","title":"Configuration","text":"<p>You can configure the Policy Controller through the <code>policyController</code> section of the <code>mke4.yaml</code> configuration file.</p>"},{"location":"configuration/policycontroller/opagatekeeper/","title":"OPA Gatekeeper","text":"<p>MKE 4k supports the use of OPA Gatekeeper for purposes of policy control.</p> <p>Open Policy Agent (OPA) is an open source policy engine that facilitates policy-based control for cloud native environments. OPA introduces a high-level declarative language called Rego that decouples policy decisions from enforcement.</p> <p>The OPA Constraint Framework introduces the following primary resources:</p> <ul> <li>Constraint templates - OPA policy definitions, written in Rego.</li> <li>Constraints - the application of a constraint template to a given set of objects.</li> </ul> <p>Gatekeeper uses the Kubernetes API to integrate OPA into Kubernetes. Policies are defined in the form of Kubernetes CustomResourceDefinitions (CRDs) and are enforced with custom admission controller webhooks. These CRDs define constraint templates and constraints on the API server. Any time a request to create, delete, or update a resource is sent to the Kubernetes cluster API server, Gatekeeper validates that resource against the predefined policies. Gatekeeper also audits preexisting resource constraint violations against newly defined policies.</p> <p>Using OPA Gatekeeper, you can enforce a wide range of policies against your Kubernetes cluster. Policy examples include:</p> <ul> <li>Container images can only be pulled from a set of whitelisted repositories.</li> <li>New resources must be appropriately labeled.</li> <li>Deployments must specify a minimum number of replicas.</li> </ul>"},{"location":"configuration/policycontroller/opagatekeeper/#configuration","title":"Configuration","text":"<p>To configure OPA Gatekeeper in MKE 4k, set the following fields in the <code>mke4.yaml</code> configuration file:</p> <pre><code>spec:\n  policyController:\n    opaGatekeeper:\n      enabled: true\n      exemptNamespaces:\n      - &lt;Namespace1&gt;\n      - &lt;Namespace2&gt;\n</code></pre> <p>The <code>exemptNamespaces</code> field lists the namespaces that are exempt from policy enforcement. The following namespaces are added by default, and thus cannot be removed:</p> <ul> <li><code>kube-system</code></li> <li><code>kube-public</code></li> <li><code>kube-node-lease</code></li> <li><code>k0s-system</code></li> <li><code>k0s-autopilot</code></li> <li><code>flux-system</code></li> <li><code>blueprint-system</code></li> <li><code>mke</code></li> <li><code>tigera-operator</code></li> <li><code>calico-system</code></li> <li><code>calico-apiserver</code></li> </ul>"},{"location":"configuration/policycontroller/opagatekeeper/#upgrade-from-mke-3","title":"Upgrade from MKE 3","text":"<p>If OPA Gatekeeper is enabled in MKE 3, the templates, constraints and list of namespaces exempted from policy control are retained through the upgrade process.</p>"},{"location":"configuration/policycontroller/opagatekeeper/#test-opa-gatekeeper","title":"Test OPA Gatekeeper","text":"<p>Before proceeding, make sure that OPA Gatekeeper is enabled and the configuration is applied to the cluster.</p> <p>To check if the OPA Gatekeeper pods have entered the <code>Running</code> state, run:</p> <pre><code>kubectl get pod -n mke\n</code></pre> <p>Example output:</p> <pre><code>NAME                                             READY   STATUS    RESTARTS      AGE\ngatekeeper-audit-56d958d955-v6d7t                1/1     Running   2 (54s ago)   61s\ngatekeeper-controller-manager-79c4f4bfc7-8m9nq   1/1     Running   0             61s\ngatekeeper-controller-manager-79c4f4bfc7-n66mj   1/1     Running   0             61s\ngatekeeper-controller-manager-79c4f4bfc7-v8lx7   1/1     Running   0             61s\n...\n</code></pre> <p>To create constraint templates and constraints from the open source Gatekeeper library, run:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/pod-security-policy/allow-privilege-escalation/template.yaml\nkubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/pod-security-policy/allow-privilege-escalation/samples/psp-allow-privilege-escalation-container/constraint.yaml\n</code></pre> <p>To create pods that are disallowed by the newly created policies, run:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/pod-security-policy/allow-privilege-escalation/samples/psp-allow-privilege-escalation-container/example_disallowed.yaml\n</code></pre> <p>Example output:</p> <pre><code>Error from server (Forbidden): error when creating \"https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/pod-security-policy/allow-privilege-escalation/samples/psp-allow-privilege-escalation-container/example_disallowed.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [psp-allow-privilege-escalation-container] Privilege escalation container is not allowed: nginx\n</code></pre>"},{"location":"configuration/policycontroller/opagatekeeper/#mke-version-comparison","title":"MKE version comparison","text":"MKE 3 MKE 4k [cluster_config.policy_enforcement.gatekeeper.enabled] policyController.opaGatekeeper.enabled [cluster_config.policy_enforcement.gatekeeper.excluded_namespaces] policyController.opaGatekeeper.exemptNamespaces"},{"location":"getting-started/","title":"Macro Syntax Error","text":"<p>File: <code>getting-started/index.md</code></p> <p>Line 7 in Markdown file: Encountered unknown tag 'steps'. <pre><code>{{% steps %}}\n</code></pre></p>"},{"location":"getting-started/access-manage-cluster-kubectl/","title":"Access and manage the cluster with kubectl","text":"<p>{{&lt; callout type=\"warning\" &gt;}} For security purposes, ensure that kubelet is not accessible from outside of the cluster, as the certificates issued through the procedures herein can be used to access it. {{&lt; /callout &gt;}}</p> <p>In addition to the MKE 4k Dashboard, you can access and manage your MKE 4k cluster using kubectl with a kubeconfig file.</p> <p>In MKE 4k, the kubeconfig file provides everything you need, as all of the necessary certificates are embedded therein. This is counter to MKE 3, which requires that you download client certificate bundles that contain kubeconfig files, as well as the certificate files that are required to configure the Docker CLI.</p> <p>{{&lt; callout type=\"info\" &gt;}} Currently, only administrators can create kubeconfig files. As previously created kubeconfig files cannot be viewed or revoked, the expiration time of the certificates used in the kubeconfig files must be set appropriately.{{&lt; /callout &gt;}}</p> <p>The following table illustrates the differences between MKE 4k kubeconfig files and MKE 3 client bundles:</p> Function MKE 4kkubeconfig files MKE 3client bundles Create by admins for other users \u2705 \u2705 Create by admins for themselves \u2705 \u2705 Create by non-admins for other users \u274c \u274c View previously created bundles \u274c \u2705 Revoke previously created bundles \u274c \u2705 Set expiration time of certificates \u2705 \u274c Can be generated from MKE 4k UI \u274c \u2705 Non-admin certs are issued by a separate CA that is trusted by kube API server, but not trusted by other components like kubelet \u274c \u2705"},{"location":"getting-started/access-manage-cluster-kubectl/#create-a-kubeconfig-file","title":"Create a kubeconfig file","text":"<p>{{&lt; callout type=\"important\" &gt;}} Only users with admin permissions can create kubeconfig files for specific users.{{&lt; /callout &gt;}}</p> <p>Verify the installation of OpenSSL and kubectl.</p> <p>Use your terminal to run the following procedure from the MKE 4k cluster that you previously configured with the <code>mkectl apply</code> command.</p> <ol> <li>Configure the username:</li> </ol> <pre><code>USERNAME=&lt;specific-username&gt;\n</code></pre> <ol> <li>Configure the number of days until expiry:</li> </ol> <pre><code>EXPIRES_IN_DAYS=&lt;integer&gt;\n</code></pre> <p>Be sure to indicate the minimum required number of days at the very least,    for example <code>7</code>, as you cannot later revoke the certificates used by the new    kubeconfig file.</p> <ol> <li>Set the <code>EXPIRES_IN_SECONDS=</code> and <code>KUBECONFIG=</code> variables. Mirantis    recommends that you use the settings shown in the following example code block:</li> </ol> <pre><code>EXPIRES_IN_SECONDS=$((EXPIRES_IN_DAYS * 24 * 60 * 60))\nKUBECONFIG=~/.mke/mke.kubeconf\n</code></pre> <ol> <li>Run the following script to generate a kubeconfig file named    <code>&lt;username&gt;.kubeconfig</code>:</li> </ol> <pre><code>export KUBECONFIG\n\nopenssl genrsa -out $USERNAME.key 2048\nopenssl req -new -key $USERNAME.key -out $USERNAME.csr -subj \"/CN=$USERNAME\"\n\nCSR_CONTENT=$(cat $USERNAME.csr | base64 | tr -d '\\n')\nCSR_NAME=$USERNAME-csr-$(LC_ALL=C tr -dc A-Za-z0-9 &lt;/dev/urandom | head -c 10; echo)\n\ncat &lt;&lt;EOF | envsubst | kubectl apply -f -\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: $CSR_NAME\nspec:\n  request: $CSR_CONTENT\n  signerName: kubernetes.io/kube-apiserver-client\n  expirationSeconds: $EXPIRES_IN_SECONDS\n  usages:\n  - client auth\nEOF\n\nkubectl certificate approve \"$CSR_NAME\"\nkubectl get csr \"$CSR_NAME\" -o jsonpath='{.status.certificate}' | base64 --decode &gt; $USERNAME.crt\n\nkubectl config view --minify --flatten &gt; $USERNAME.kubeconfig\nkubectl config unset users --kubeconfig=$USERNAME.kubeconfig &gt; /dev/null\nkubectl config unset contexts --kubeconfig=$USERNAME.kubeconfig &gt; /dev/null\n\nkubectl config set-credentials $USERNAME --client-certificate=$USERNAME.crt --client-key=$USERNAME.key --embed-certs=true --kubeconfig=$USERNAME.kubeconfig\nkubectl config set-context mke-$USERNAME --cluster=mke --user=$USERNAME --kubeconfig=$USERNAME.kubeconfig\nkubectl config use-context mke-$USERNAME --kubeconfig=$USERNAME.kubeconfig\n\nrm $USERNAME.crt $USERNAME.csr $USERNAME.key\n</code></pre> <p>Once the <code>&lt;username&gt;.kubeconfig</code> file is generated, send it to the target user, who can thereafter use it to access the MKE 4k cluster with the <code>kubectl --kubeconfig</code> command.</p>"},{"location":"getting-started/access-manage-cluster-kubectl/#list-or-revoke-kubeconfig-files","title":"List or revoke kubeconfig files","text":"<p>Currently, it is not possible to list or revoke previously created kubeconfig files in MKE 4k.</p>"},{"location":"getting-started/add-and-remove-cluster-nodes/","title":"Add and remove cluster nodes","text":"<p>{{&lt; callout type=\"info\" &gt;}}</p> <p>To avoid unexpected complications, make sure that you have an MKE 4k cluster up and running before you follow the procedures herein for adding and removing nodes.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"getting-started/add-and-remove-cluster-nodes/#add-nodes-to-an-mke-4k-cluster","title":"Add nodes to an MKE 4k cluster","text":"<ol> <li> <p>Obtain the <code>mke4.yaml</code> configuration file for your cluster.</p> </li> <li> <p>Append the host information for the new node to the <code>hosts</code> section of the    <code>mke4.yaml</code> configuration file in the following format:</p> </li> </ol> <pre><code>- role: worker\n  ssh:\n    address: &lt;address&gt;\n    keyPath: &lt;key location&gt;\n    port: &lt;ssh port&gt;\n    user: &lt;username&gt;\n</code></pre> <ol> <li>Run <code>mkectl apply</code> command to add the new node.</li> </ol>"},{"location":"getting-started/add-and-remove-cluster-nodes/#remove-nodes-from-an-mke-4k-cluster","title":"Remove nodes from an MKE 4k cluster","text":"<p>The method for removing nodes from an MKEk cluster differs, depending on whether the node is a control plane node or a worker node.</p>"},{"location":"getting-started/add-and-remove-cluster-nodes/#remove-a-control-plane-node","title":"Remove a control plane node","text":"<p>Refer to the k0s documentation, Remove or replace a controller for information on how to remove a control plane node from an MKE 4k cluster.</p>"},{"location":"getting-started/add-and-remove-cluster-nodes/#remove-a-worker-node","title":"Remove a worker node","text":"<ol> <li>Delete the worker node from the cluster:</li> </ol> <pre><code>kubectl --kubeconfig ~/.mke/mke.kubeconf delete node &lt;worker_node_name&gt;\n</code></pre> <ol> <li>Run the following command sequence on the node itself, to uninstall    k0s:</li> </ol> <pre><code>sudo k0s stop\nsudo k0s reset\nsudo reboot\n</code></pre> <ol> <li> <p>Obtain the <code>mke4.yaml</code> configuration file for your cluster.</p> </li> <li> <p>Delete the host information for the deleted node from the <code>hosts</code> section    of the <code>mke4.yaml</code> configuration file, to circumvent any potential mkectl issues.</p> </li> <li> <p>Apply the configuration:</p> </li> </ol> <pre><code>mkectl apply -f mke4.yaml\n</code></pre>"},{"location":"getting-started/create-cluster/","title":"Create a cluster","text":"<p>{{&lt; callout type=\"info\" &gt;}}</p> <p>For information on how to create a cluster in an airgapped environment, refer to Offline installation.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"getting-started/create-cluster/#install-dependecies","title":"Install dependecies","text":"<p>Verify that you have installed <code>mkectl</code> and other dependencies on your system as described in Install MKE 4k CLI.</p>"},{"location":"getting-started/create-cluster/#configure-cluster-nodes","title":"Configure cluster nodes","text":"<p>Configure the cluster nodes in advance, in accordance with the System requirements.</p> <p>Node provisioning is managed by the cluster administrators. You can, for instance, use Terraform to create the nodes in a cloud provider. Refer to Example Terraform configuration for an example.</p>"},{"location":"getting-started/create-cluster/#initialize-deployment","title":"Initialize deployment","text":"<p>MKE 4k streamlines the cluster deployment through the use of a single YAML file, which details the desired cluster configuration. This approach simplifies the setup process and ensures consistency in cluster deployments.</p> <ol> <li>Generate the YAML file for your installation:</li> </ol> <pre><code>mkectl init &gt; mke4.yaml\n</code></pre> <ol> <li> <p>In the generated configuration file:</p> </li> <li> <p>Edit the <code>hosts</code> section to match your roster of nodes. Provide the SSH      information for each cluster node, as well as the role of the node based      on their functions within the cluster. The table below provides the list      of available node roles and their descriptions:</p> Node Role Description controller+worker A manager node that runs both control plane and data plane components. This role combines the responsibilities of managing cluster operations and executing workloads. worker A worker node that runs the data plane components. These nodes are dedicated to executing workloads and   handling the operational tasks assigned by the control plane. single A special role used when the cluster consists of a single node. This node handles both control plane and data plane components, effectively managing and executing workloads within a standalone environment. </li> <li> <p>Specify the external address in the in <code>apiServer.externalAddress</code> field.      The external address is the domain name of the load balancer configured      as described in System Requirements: Load balancer.</p> </li> </ol> <p>{{&lt; callout type=\"important\" &gt;}} You may need to add the SSH private key to your identity manager in order for mkectl to connect to it. - With ssh-agent:   <pre><code>ssh-add &lt;path_to_your_SSH_private_key&gt;\n</code></pre> - Without ssh-agent, ensure that the SSH private key is accessible at the path specified in the keyPath field of the mke4.yaml configuration file. {{&lt; /callout &gt;}}</p>"},{"location":"getting-started/create-cluster/#create-a-cluster","title":"Create a cluster","text":"<p>{{&lt; callout type=\"warning\" &gt;}}</p> <p>Before you create a new MKE 4k cluster you must first verify that a cluster does not already exist in the system. If you attempt to create a cluster where a cluster is already present, even through the use of a different configuration file, you will lose the ability to use <code>mkectl</code> to access the original cluster.</p> <p>For information on how to delete a cluster, refer to Uninstall a cluster.</p> <p>{{&lt; /callout &gt;}}</p> <p>To create a new cluster, run the <code>mkectl apply</code> command with the generated YAML configuration file:</p> <pre><code>mkectl apply -f mke4.yaml\n</code></pre> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>The <code>mkectl apply</code> command configures the <code>mke</code> context in the default kubeconfig file located at <code>~/.kube/config</code>. If the default kubeconfig is changed, and the <code>mke</code> context becomes invalid or unavailable, <code>mkectl</code> will not be able to manage the cluster until the kubeconfig is restored.</p> <p>{{&lt; /callout &gt;}}</p> <p>Once the new cluster is viable, you can start interacting with it using <code>kubectl</code> with the <code>mke</code> context.</p>"},{"location":"getting-started/get-current-mke-config/","title":"Obtain the current MKE 4k configuration file","text":"<p>To obtain the current <code>mke4.yaml</code> configuration file for your MKE 4k cluster, run:</p> <pre><code>mkectl --kubeconfig ~/.mke/mke.kubeconf config get\n</code></pre> <p>There are numerous reasons why you may need to procure the <code>mke4.yaml</code> configuration file, including:</p> <ul> <li> <p>To make changes to your <code>mke4.yaml</code> configuration file using the MKE 4k web UI or kubectl, you can obtain the current <code>mke4.yaml</code> configuration file and edit the settings as needed. Following this, you can run <code>mkectl apply</code> to apply the new settings, without the danger of the local configurations overwriting the changes.</p> </li> <li> <p>In the event that the <code>mke4.yaml</code> configuration file is lost or becomes   corrupted.</p> </li> </ul>"},{"location":"getting-started/install-MKE-4k-CLI/","title":"Install the MKE 4k CLI","text":"<p>Before you can proceed with the MKE 4k installation, you must download and install <code>mkectl</code>. You can do this automatically using an <code>install.sh</code> script, or you can do it manually.</p>"},{"location":"getting-started/install-MKE-4k-CLI/#install-automatically-with-a-script","title":"Install automatically with a script","text":"<ol> <li>Install <code>mkectl</code> by downloading and executing the following shell script:</li> </ol> <pre><code>sudo /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/MirantisContainers/mke-release/refs/heads/main/install.sh)\"\n</code></pre> <p>If you want to override default dependency versions, pass the    <code>MKECTL_VERSION</code> as required. For example:</p> <pre><code>sudo MKECTL_VERSION=v4.1.0 /bin/sh -c \"$(curl -fsSL https://raw.githubusercontent.com/MirantisContainers/mke-release/refs/heads/main/install.sh)\"\n</code></pre> <p>If you prefer to run the script in the debug mode for more detailed output and logging,    set <code>DEBUG=true</code>:</p> <pre><code>sudo DEBUG=true /bin/sh -c \"$(curl -fsSL https://raw.githubusercontent.com/MirantisContainers/mke-release/refs/heads/main/install.sh)\"\n</code></pre> <ol> <li>Confirm the installation:</li> </ol> <pre><code>mkectl version\n</code></pre> <p>Expected output:</p> <pre><code>Version: v4.1.0\n</code></pre> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>By default, the script installs mkectl v4.1.0.</p> <p>{{&lt; /callout &gt;}}</p> <p>The <code>install.sh</code> script detects the operating system and the underlying architecture, based on which it will install the <code>mkectl</code> binary in <code>/usr/local/bin</code>. Thus, you must ensure that <code>/usr/local/bin</code> is in your <code>PATH</code> environment variable.</p> <p>You can now proceed with MKE 4k cluster creation.</p>"},{"location":"getting-started/install-MKE-4k-CLI/#install-using-homebrew","title":"Install using Homebrew","text":"<ol> <li>Add the <code>mirantis</code> repository to your local taps:</li> </ol> <pre><code>brew tap mirantis/tap\n</code></pre> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>If the <code>mirantis</code> tap is already present and you want to update it, run:</p> <pre><code>brew update\n</code></pre> <p>{{&lt; /callout &gt;}}</p> <ol> <li> <p>Install <code>mkectl</code>:</p> </li> <li> <p>To install the latest <code>mkectl</code> version:</p> <pre><code>brew install mkectl\n</code></pre> </li> <li> <p>To install a specific <code>mkectl</code> version:</p> <pre><code>brew install mkectl@&lt;version-number&gt;\n</code></pre> </li> </ol>"},{"location":"getting-started/install-MKE-4k-CLI/#install-manually","title":"Install manually","text":"<ol> <li>Download the <code>mkectl</code> binary from the S3 bucket:</li> </ol> Distribution Architecture Download Linux x86_64 download MacOS x86_64 download <ol> <li>Ensure that the <code>mkectl</code> binary is executable:</li> </ol> <pre><code>chmod +x mkectl\n</code></pre> <ol> <li>Copy the <code>mkectl</code> binary to <code>/usr/local/bin/</code>:</li> </ol> <pre><code>mv mkectl /usr/local/bin/\n</code></pre>"},{"location":"getting-started/offline-installation/","title":"Offline installation","text":"<p>The installation and upgrade procedures for MKE 4k reflect those of the online scenario. While the online installation typically uses <code>registry.mirantis.com</code> as the primary OCI registry for MKE  4k materials, with the offline scenario you instead specify a private registry from which to pull the MKE 4k images and charts.</p>"},{"location":"getting-started/offline-installation/#dependencies","title":"Dependencies","text":"<ul> <li>skopeo 1.6.1 or later</li> <li>An OCI-based private registry that is accessible from all cluster nodes.</li> <li>All MKE 4k images and charts must be publicly accessible, with no required authentication.</li> <li>The registry must use HTTPS.</li> <li>The registry must support multi-level nesting. For example,     <code>registry.com/level-one/level-two/level-three/image-name:latest</code>. Some     registries only allow one level of nesting, such as     <code>registry.com/level-one/image:latest</code>, so verify that your registry     supports deeper nesting for image names.</li> </ul>"},{"location":"getting-started/offline-installation/#preparation","title":"Preparation","text":"<ol> <li> <p>Download the offline bundle from the command line as follows:</p> <pre><code>MKE_VERSION=\"4.1.0\"\nDOWNLOAD_URL=\"https://packages.mirantis.com/caas/mke_bundle_v${MKE_VERSION}.tar.gz\"\nBUNDLE_NAME=\"mke_bundle_v${MKE_VERSION}.tar.gz\"\n\ncurl -L \"$DOWNLOAD_URL\" -o \"$BUNDLE_NAME\"\n</code></pre> </li> <li> <p>Transfer the bundle file to a machine that can access your private registry.</p> </li> <li> <p>On the machine with registry access, set the environment variables:</p> </li> </ol> <pre><code>MKE_VERSION=\"4.1.0\"\nexport REGISTRY_ADDRESS='&lt;registry_address&gt;'\nexport REGISTRY_PROJECT_PATH='&lt;registry-path&gt;'\nexport REGISTRY_USERNAME='&lt;username&gt;'\nexport REGISTRY_PASSWORD='&lt;password&gt;'\nexport BUNDLE_NAME=\"mke_bundle_v${MKE_VERSION}.tar.gz\"\n</code></pre> Environment variable Description REGISTRY_ADDRESS Registry hostname (required) and port (optional). The value must not end with a slash '/'.Example: <code>private-registry.example.com:8080</code> REGISTRY_PROJECT_PATH Path to the registry project that will store all MKE 4k artifacts. The registry address and path should comprise the full registry path. The value must not end with a slash '/'.Example: <code>REGISTRY_ADDRESS + '/' + REGISTRY_PROJECT_PATH == 'private-registry.example.com:8080/mke</code> REGISTRY_USERNAME Username for the account that is allowed to push. REGISTRY_PASSWORD Password for the account that is allowed to push. BUNDLE_NAME The name of previously downloaded bundle file, which must be located in the same directory in which you run the preparation steps. <ol> <li>Upload the MKE 4k images and helm charts to your private registry:</li> </ol> <pre><code># Login to the registry\nskopeo login \"$REGISTRY_ADDRESS\" -u \"$REGISTRY_USERNAME\" -p \"$REGISTRY_PASSWORD\"\n\n# Extract the bundle\ntar -xzf \"$BUNDLE_NAME\" -C ./\n\n# Iterate over bundle artifacts and upload each one using skopeo\nfor archive in $(find ./bundle -print | grep \".tar\"); do\n  # Form the image name from the archive name\n  img=$(basename \"$archive\" | sed 's~\\.tar~~' | tr '&amp;' '/' | tr '@' ':');\n\n  echo \"Uploading $img\";\n  # Copy artifact from local oci archive to the registry\n  skopeo copy -q --retry-times 3 --multi-arch all \"oci-archive:$archive\" \"docker://$REGISTRY_ADDRESS/$REGISTRY_PROJECT_PATH/$img\";\ndone;\n</code></pre>"},{"location":"getting-started/offline-installation/#installation","title":"Installation","text":"<p>{{&lt; callout type=\"info\" &gt;}}</p> <p>For information on performing an upgrade to an existing MKE 3 installation in an airgap environment, refer to Offline upgrade.</p> <p>{{&lt; /callout &gt;}}</p> <ol> <li> <p>Refer to the Create a Cluster procedure for detail on how to create an <code>mke4.yaml</code> configuration file.</p> </li> <li> <p>Add the following additional settings to the <code>mke4.yaml</code> configuration file:</p> </li> </ol> Setting Description <code>.spec.registries.imageRegistry.URL</code> Sets your registry address with a project path that contains your MKE 4k images. For example, <code>private-registry.example.com:8080/mke</code>. The setting must not end with a slash <code>/</code>.The port is optional. <code>.spec.registries.chartRegistry.URL</code> Sets your registry address with a project path that contains your MKE 4k helm charts in OCI format. For example, <code>oci://private-registry.example.com:8080/mke</code>.The setting must always start with <code>oci://</code>, and it must not end with a slash <code>/</code> .If you uploaded the bundle as previously described, the registry address and path will be the same for chart and image registry, with the only difference being the <code>oci://</code> prefix in the chart registry URL. <code>.spec.registries.imageRegistry.caData</code> PEM encoded certificate of the Certificate Authority that issued image registry TLS certificates. Optional. Must be provided if registry is using TLS certs issued by a non-publicly trusted CA. <code>.spec.registries.chartRegistry.caData</code> PEM encoded certificate of the Certificate Authority that issued chart registry TLS certificates. Optional. Must be provided if registry is using TLS certs issued by a non-publicly trusted CA. <code>.spec.airgap.enabled = true</code> Indicates that your environment is airgapped. <ol> <li>Run the <code>mkectl apply</code> command.</li> </ol>"},{"location":"getting-started/offline-installation/#mke-4k-versus-mke-3","title":"MKE 4k versus MKE 3","text":"<p>MKE 3 requires the use of the <code>docker load</code> command to load offline bundles directly into Docker on every cluster node. While this approach does not require you to have a private registry, it also means that the cluster cannot re-pull the image should any of the loaded images go missing. As such, MKE 3 users must disable Kubernetes garbage collection, which can sometimes prune images of optional components that are not always enabled. This is not an issue with MKE 4, as images are pulled from a private registry that the customer provides, and thus there is no need to disable Kubernetes garbage collection. That said, though, users must ensure that the registry is available at all times and that it is accessible from every cluster node.</p>"},{"location":"getting-started/start-interacting-with-cluster/","title":"Start interacting with the cluster","text":"<p>To start interacting with the cluster, use <code>kubectl</code> with the <code>mke</code> context. Though to do that, you need to specify the configuration. Use <code>mkectl</code> to output the kubeconfig of the cluster to <code>~/mke/.mke.kubeconf</code>.</p> <p>You can apply <code>.mke.kubeconf</code> using any one of the following methods:</p> <ul> <li>Set the <code>KUBECONFIG</code> environment variable to point to <code>~/.mke/mke.kubeconf</code>:</li> </ul> <pre><code>export KUBECONFIG=~/.mke/&lt;cluster name&gt;.kubeconfig\n</code></pre> <ul> <li>Append the contents to the default kubeconfig:</li> </ul> <pre><code>cat ~/.mke/mke.kubeconf &gt;&gt; ~/.kube/config\n</code></pre> <ul> <li>Specify the kubeconfig as a command argument:</li> </ul> <pre><code>kubectl --kubeconfig ~/.mke/mke.kubeconf\n</code></pre> <p>Example output:</p> <pre><code>$ kubectl --context mke get nodes\n\nNAME    STATUS   ROLES           AGE   VERSION\nnode1   Ready    &lt;none&gt;          2d    v1.29.3+k0s\nnode2   Ready    control-plane   2d    v1.29.3+k0s\n</code></pre> <p>To modify the cluster configuration, edit the YAML configuration file and rerun the <code>apply</code> command:</p> <pre><code>mkectl apply -f mke4.yaml\n</code></pre>"},{"location":"getting-started/system-requirements/","title":"System requirements","text":"<p>Before you start cluster deployment, verify that your system meets the following minimum hardware and software requirements.</p>"},{"location":"getting-started/system-requirements/#hardware-requirements","title":"Hardware requirements","text":"<p>Although MKE 4k uses k0s as the underlying Kubernetes distribution, the hardware requirements for MKE 4k differ from k0s due to the higher resource requirements of various enterprise-grade components the software uses.</p> <p>To ensure stability, optimal performance, and reliable upgrades, Mirantis performed extensive internal testing to determine the optimum hardware configuration for MKE 4k:</p> <ul> <li>vCPUs: 8 vCPUs per node</li> <li> <p>Ram</p> <p>32 GB per node</p> </li> </ul> <p>To compare, the same testing showed repeated upgrade failures for MKE 3 systems with 2 vCPUs and 8 GB of RAM.</p>"},{"location":"getting-started/system-requirements/#software-requirements","title":"Software requirements","text":"<ul> <li>Operating systems:</li> <li>Ubuntu 20.04 Linux</li> <li>Ubuntu 22.04 Linux</li> <li>RHEL 8.10</li> <li>Rocky Linux 9.4</li> <li>Architecture: <code>amd64</code></li> <li> <p>Cni</p> <p>Calico</p> </li> </ul>"},{"location":"getting-started/system-requirements/#load-balancer-requirements","title":"Load balancer requirements","text":"<p>The load balancer can be implemented in many different ways. For example, you can use HAProxy, NGINX, or the load balancer of your cloud provider.</p> <p>To ensure the MKE 4k Dashboard functions properly, MKE 4k requires a TCP load balancer. This load balancer acts as a single point of contact to access the controllers. With the default MKE 4k configuration, the load balancer must allow and route traffic to each controller through the following ports:</p> Listen port Target port Purpose Configurable 6443 6443 Kubernetes API {{&lt; icon \"ban\" &gt;}} 9443 9443 Controller join API {{&lt; icon \"ban\" &gt;}} 443 33001 Ingress Controller {{&lt; icon \"check\" &gt;}} <p>You can configure the listen port of the Ingress Controller to be different from the default port <code>443</code>. However, if you change the listen port, you must append the new port number to the external address in the configuration file. For example, if you set the listen port to be the same as the target port, <code>33001</code>, the configuration should look as follows:</p> <pre><code>apiServer:\n  externalAddress: \"mke.example.com:33001\"\n</code></pre> <p>The target port must match the HTTPS node port of the Ingress Controller, which is <code>33001</code> by default, but can be adjusted as needed. See the configuration details for <code>nodePorts</code> in the  Ingress Controller configuration.</p>"},{"location":"getting-started/uninstall-cluster/","title":"Uninstall a cluster","text":"<ol> <li>Destroy the last MKE 4k cluster you created using the <code>reset</code> command:</li> </ol> <pre><code>mkectl reset --force -f mke4.yaml\n</code></pre> <ol> <li>Confirm cluster deletion.</li> </ol> <p>Example output:</p> <pre><code>INF Reset Blueprint Operator resources\nINF Resetting Blueprint\nINF Uninstalling Blueprint Operator\nINF Deleting 16 objects\nk0sctl v0.19.4 Copyright 2023, k0sctl authors.\nAnonymized telemetry of usage will be sent to the authors.\nBy continuing to use k0sctl you agree to these terms:\nhttps://k0sproject.io/licenses/eula\nINFO ==&gt; Running phase: Connect to hosts \nINFO [ssh] 54.153.41.129:22: connected            \nINFO [ssh] 54.215.249.216:22: connected           \nINFO ==&gt; Running phase: Detect host operating systems \nINFO [ssh] 54.153.41.129:22: is running Ubuntu 22.04.5 LTS \nINFO [ssh] 54.215.249.216:22: is running Ubuntu 22.04.5 LTS \nINFO ==&gt; Running phase: Acquire exclusive host lock \nINFO ==&gt; Running phase: Prepare hosts    \nINFO ==&gt; Running phase: Gather host facts \nINFO [ssh] 54.153.41.129:22: using ip-172-31-35-203.us-west-1.compute.internal as hostname \nINFO [ssh] 54.215.249.216:22: using ip-172-31-134-35.us-west-1.compute.internal as hostname \nINFO [ssh] 54.153.41.129:22: discovered ens5 as private interface \nINFO [ssh] 54.215.249.216:22: discovered ens5 as private interface \nINFO [ssh] 54.153.41.129:22: discovered 172.31.35.203 as private address \nINFO [ssh] 54.215.249.216:22: discovered 172.31.134.35 as private address \nINFO ==&gt; Running phase: Gather k0s facts \nINFO [ssh] 54.215.249.216:22: found existing configuration \nINFO [ssh] 54.215.249.216:22: is running k0s controller+worker version v1.31.2+k0s.0 \nWARN [ssh] 54.215.249.216:22: the controller+worker node will not schedule regular workloads without toleration for node-role.kubernetes.io/master:NoSchedule unless 'noTaints: true' is set \nINFO [ssh] 54.215.249.216:22: listing etcd members \nINFO [ssh] 54.153.41.129:22: is running k0s worker version v1.31.2+k0s.0 \nINFO [ssh] 54.215.249.216:22: checking if worker ip-172-31-35-203.us-west-1.compute.internal has joined \nINFO ==&gt; Running phase: Reset workers    \nINFO [ssh] 54.153.41.129:22: reset                \nINFO ==&gt; Running phase: Reset controllers \nINFO [ssh] 54.215.249.216:22: reset               \nINFO ==&gt; Running phase: Reset leader     \nINFO [ssh] 54.215.249.216:22: reset               \nINFO ==&gt; Running phase: Reload service manager \nINFO [ssh] 54.153.41.129:22: reloading service manager \nINFO [ssh] 54.215.249.216:22: reloading service manager \nINFO ==&gt; Running phase: Release exclusive host lock \nINFO ==&gt; Running phase: Disconnect from hosts \nINFO ==&gt; Finished in 26s \n</code></pre>"},{"location":"getting-started/licensing-mke4k/","title":"Licensing MKE 4k","text":"<p>{{&lt; callout type=\"warning\" &gt;}}</p> <p>You must have a valid license to lawfully run MKE 4k. For more information, refer to Mirantis Agreements and Terms.</p> <p>{{&lt; /callout &gt;}}</p> <p>To license MKE 4k:</p> <ol> <li> <p>Obtain your MKE 4k license.</p> </li> <li> <p>Set the license in your configuration.</p> </li> </ol> <p>{{&lt; callout type=\"info\" &gt;}} Mirantis recommends that you set the license in the MKE 4k configuration file prior to the creation of your MKE 4k cluster. You can, however, apply the license following cluster creation using the MKE 4k web UI. {{&lt; /callout &gt;}}</p>"},{"location":"getting-started/licensing-mke4k/apply-mke4k-license-post-installation/","title":"Apply an MKE 4k license following installation","text":"<p>{{&lt; callout type=\"warning\" &gt;}}</p> <p>You must have a valid license to lawfully run MKE 4k. For more information, refer to Mirantis Agreements and Terms.</p> <p>{{&lt; /callout &gt;}}</p> <p>If you did not set your MKE 4k license in the MKE 4k configuration file prior to installation, you can use the MKE 4k web UI to apply the license:</p> <ol> <li>Log in to the MKE web UI with an administrator account.</li> <li>In the left-side navigation panel, navigate to Admin Settings -&gt; License.</li> <li>Insert the license into the license key field. To do this, click    Choose File and navigate to the <code>.lic</code> file you previously downloaded.    Alternately, you can copy-paste the contents of the <code>.lic</code> file into the    field as a text string.</li> </ol> <p></p> \u00d7 <ol> <li>Click Save settings to update the MKE 4k license.</li> </ol>"},{"location":"getting-started/licensing-mke4k/obtain-mke4k-license/","title":"Obtain your MKE 4k license","text":"<p>{{&lt; callout type=\"warning\" &gt;}}</p> <p>You must have a valid license to lawfully run MKE 4k. For more information, refer to Mirantis Agreements and Terms.</p> <p>{{&lt; /callout &gt;}}</p> <p>Install the MKE 4k CLI prior to downloading your MKE 4k license.</p> <ol> <li> <p>Locate the Welcome to Mirantis' CloudCare Portal email sent to you from Mirantis    Support. If you do not have the email, confirm with your Designated Administrator    that you have been added as a Designated Contact.</p> </li> <li> <p>Click Environments in the top navigation bar of the MKE 4k web UI.</p> </li> <li> <p>Click the Cloud Name that is associated with the license you want to download.</p> </li> <li> <p>Scroll down to License Information and click the License File URL.     A new tab opens in your browser.</p> </li> <li> <p>Click View file to download your license file.</p> </li> </ol> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>Though MKE 4k is generally a subscription-only service, you can obtain a free trial license from Mirantis. Make your request using the Mirantis contact form.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"getting-started/licensing-mke4k/set-license-in-configuration/","title":"Set your license in the configuration","text":"<p>{{&lt; callout type=\"warning\" &gt;}}</p> <p>You must have a valid license to lawfully run MKE 4k. For more information, refer to Mirantis Agreements and Terms.</p> <p>{{&lt; /callout &gt;}}</p> <p>To ensure that your MKE 4k cluster is licensed upon installation:</p> <ol> <li> <p>Insert the license into <code>spec.license.token</code> in the <code>mke4.yaml</code>    configuration file:</p> <pre><code>spec:\n  license:\n    token: &lt;your-license-file&gt;\n</code></pre> </li> <li> <p>Apply the license:</p> </li> </ol> <pre><code>mkectl apply\n</code></pre> <ol> <li>Check the license status:</li> </ol> <pre><code>kubectl -n mke get mkeconfig mke -ojsonpath=\"{.status.licenseStatus}\" | jq \n</code></pre> <p>Example output:</p> <pre><code>{\n  \"expiration\": \"2027-10-10T07:00:00Z\",\n  \"licenseType\": \"Offline\",\n  \"maxEngines\": 10,\n  \"scanningEnabled\": true,\n  \"subject\": \"example\",\n  \"tier\": \"Production\"\n}\n</code></pre>"},{"location":"release-notes/_index/","title":"Release notes","text":"<p>The MKE 4k release notes provide a comprehensive overview of new features, enhancements, addressed issues, major middleware component versions, known issues, and security concerns.</p> <p>{{&lt; cards &gt;}}   {{&lt; card link=\"features-summary\" title=\"Features summary\" icon=\"lightning-bolt\" &gt;}}   {{&lt; card link=\"4.1.0\" title=\"4.1.0\" icon=\"lightning-bolt\" &gt;}} {{&lt; /cards &gt;}}</p>"},{"location":"release-notes/features-summary/","title":"Features Summary","text":"<p>The feature summary offers a high-level view of MKE 4k product functionality.</p> Feature Detail Learn more Authentication MKE 4k uses Dex for authentication, which serves as a proxy between  MKE 4k clusters and authentication providers. Dex supports the following authentication protocols: * Basic authentication *  OIDC * SAML * LDAPSCIM, which is supported in MKE 3, is not supported in MKE 4k. Configure OIDC service for MKE, Configure SAML service for MKE, Configure LDAP service for MKE 4k Authorization MKE 4k uses standard Kubernetes RBAC authorization. RBAC Migrations Backup and restore MKE 4k supports backup and restoration of cluster data through the use of the Velero add-on. System backup is enabled by default. Back up using an external storage provider, Back up with an in-cluster storage provider Cloud providers MKE 4k directly supports the use of AWS for managed cloud service provision. Different cloud service providers can be put to use, though doing so requires manual configuration. Cloud providers: Configuration Container Network Interface MKE 4k supports Calico OSS (operating in KDD mode) as the CNI for cluster networking. Network configuration CoreDNS Lameduck MKE 4k supports the use of lameduck mode for CoreDNS. CoreDNS Lameduck: Configuration GPU Feature Discovery MKE 4k supports running workloads on NVIDIA GPU nodes and GPU node discovery. NVIDIA MIG is not supported. NVIDIA GPU Workloads Ingress Ingress controllers abstract the complexity of Kubernetes application traffic routing and provide a bridge between Kubernetes services and external ones. Ingress controller, TCP and UDP services Kubernetes MKE 4k deploys Kubernetes 1.32. Kubernetes components Licensing MKE 4k requires the use of a license for lawful use. Licensing MKE 4k Load balancing MKE 4k supports the use of MetalLB to create Load Balancer services, offering such features as address allocation and external announcement. MetalLB load balancer Logging, Monitoring and Alerting MKE 4k monitoring setup is based on the kube-prometheus-stack, which offers a comprehensive solution for collecting, storing, and visualizing metrics. Monitoring tool: Prometheus, Monitoring tool: Grafana,  Monitoring tool: cAdvisor, Monitoring tool: OpsCare (Under development) MKE 4k CLI The MKE 4k CLI tool, mkectl is the MKE 4k CLI tool. It can be installed automatically using an install.sh script, or it can be done manually. Install the MKE 4k CLI MKE 4k Dashboard MKE 4k provides a web-based user interface that enables the management of Kubernetes resources in an MKE-managed cluster. MKE 4k Dashboard Node Feature Discovery (NFD) Node Feature Discovery (NFD) detects the hardware features that are available on each node in a Kubernetes cluster, and advertises the detected features through node labels. Node Feature Discovery: Configuration (NFD) Policy Controller MKE 4k allows installation of third-party policy controllers for Kubernetes. Currently, OPA Gatekeeper is the only supported policy controller. OPA Gatekeeper Support Bundle Support bundles for MKE 4k can be generated directly from the command line. Create a support bundle Telemetry MKE 4k can be set to automatically record and transmit data to Mirantis through an encrypted channel, for monitoring and analysis purposes. Enable telemetry through the MKE 4k CLI,   Enable telemetry through the MKE 4k web UI Airgap (offline installation) MKE 4k can be deployed in an airgap environment. Offline Installation, Upgrade from MKE 3 offline Networking: Multus Multus CNI support Multus CNI"},{"location":"release-notes/4.1.0/_index/","title":"4.1.0","text":"Release date Version Highlights 2\u2011MAY\u20112025 4.1.0 MKE 4k release introducing the following enhancements:- Introduction of Tigera Operator- Offline installation capability- k0rdent integration- Drift detection and correction- Upgrade process improvements- Telemetry data element additions- Augmentations to MKE web UI <p>{{&lt; cards &gt;}}   {{&lt; card link=\"enhancements\" title=\"Enhancements\" icon=\"support\" &gt;}}   {{&lt; card link=\"addressed-issues\" title=\"Addressed issues\" icon=\"support\" &gt;}}   {{&lt; card link=\"known-issues\" title=\"Known issues\" icon=\"support\" &gt;}}   {{&lt; card link=\"major-component-versions\" title=\"Middleware component versions\"   icon=\"support\" &gt;}} {{&lt; /cards &gt;}}</p>"},{"location":"release-notes/4.1.0/addressed-issues/","title":"Addressed issues","text":"<p>Issues addressed in the MKE 4k 4.1.0 release include:</p> <ul> <li>Fixed an issue wherein the mkectl <code>apply</code> and <code>upgrade</code>   commands could not succeed on clusters with less than 2GB of available   storage.</li> </ul> <ul> <li>Fixed an issue in the MKE web UI wherein the breadcrumbs on the   Settings &gt; License page were incorrectly linked.</li> </ul> <ul> <li>Fixed an issue wherein the OIDC client secret was not being   migrated during an mkectl upgrade.</li> </ul> <ul> <li>Fixed an issue wherein managed user passwords were not migrated as   part of an upgrade from MKE 3.</li> </ul> <ul> <li>Fixed an issue wherein Prometheus could be accessed without   authentication.</li> </ul>"},{"location":"release-notes/4.1.0/enhancements/","title":"Enhancements","text":"<p>Detail on the enhancements introduced in MKE 4k 4.1.0 includes:</p> <ul> <li>Augmented the error message the user receives when an upgrade   failure occurs due the presence of a previous configuration file in the   <code>~/.mke/mke.kubeconf</code> directory.</li> </ul> <ul> <li>Certificate data is now obscured in the MKE web UI once it has   been saved.</li> </ul> <ul> <li>Addition of a static pre-login licensing message that displays   after the first login and perpetually for unlicensed users.</li> </ul> <ul> <li>Password restrictions integrated in the MKE web UI for length and   use of white space.</li> </ul> <ul> <li>Telemetry data now contains the MKE 4k version number.</li> </ul> <p>Example:</p> <pre><code>\"app\": {\n     \"name\": \"m***\",\n     \"version\": \"v4.1.0\"\n   },\n</code></pre> <ul> <li>Telemetry data now contains the external IP address.</li> </ul> <p>Example:</p> <pre><code>\"direct\": true,\n   \"ip\": \"3***.2***.5***.1***\",\n   \"library\": {\n     \"name\": \"analytics-go\",\n     \"version\": \"3.0.0\"\n   }\n</code></pre> <ul> <li>Telemetry data now contains metrics for individual nodes and   Kubernetes resource usage.</li> </ul> <ul> <li>Removed the redundant namespace selector under <code>Namesaces &gt; New</code>   in the MKE web UI.</li> </ul> <ul> <li>Users are now required to enter their current password as part   of the process of setting a new one.</li> </ul>"},{"location":"release-notes/4.1.0/known-issues/","title":"Known issues","text":"<p>The MKE 4k known issues with available workarounds are described herein.</p>"},{"location":"release-notes/4.1.0/known-issues/#post-install-kubelet-parameter-modifications-require-k0s-restart","title":"Post-install kubelet parameter modifications require k0s restart","text":"<p>Modifications made to the kubelet parameters in the <code>mke4.yaml</code> configuration file after the initial MKE 4k installation require a restart of k0s on every cluster node. To do this:</p> <ol> <li> <p>Wait for a short time, roughly 60 seconds after the application of the    <code>mkectl apply</code> command, to give the Pods time to enter their <code>Running</code> state.</p> </li> <li> <p>Run the <code>systemctl restart k0scontroller</code> command on all manager nodes and    the  <code>systemctl restart k0scontroller</code> command on all worker nodes.</p> </li> </ol>"},{"location":"release-notes/4.1.0/known-issues/#upgrade-may-fail-on-clusters-with-two-manager-nodes","title":"Upgrade may fail on clusters with two manager nodes","text":"<p>MKE 3 upgrades to MKE 4k may fail on clusters that have only two manager nodes.</p> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>Mirantis does not sanction upgrading MKE 3 clusters that have an even number of manager nodes. In general, having an even number of manager nodes is avoided in clustering systems due to quorum and availability factors.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"release-notes/4.1.0/known-issues/#calico-ebpf-and-ipvs-modes-are-not-supported","title":"Calico eBPF and IPVS modes are not supported","text":"<p>Calico eBPF and IPVS mode are not yet supported for MKE 4k. As such, upgrading from an MKE 3 cluster using either of those networking modes results in an error:</p> <pre><code>FATA[0640] Upgrade failed due to error: failed to run step [Upgrade Tasks]:\nunable to install BOP: unable to apply MKE4 config: failed to wait for pods:\nfailed to wait for pods: failed to list pods: client rate limiter Wait returned\nan error: context deadline exceeded\n</code></pre>"},{"location":"release-notes/4.1.0/known-issues/#managed-user-passwords-are-not-migrated-during-upgrade-from-mke-3","title":"Managed user passwords are not migrated during upgrade from MKE 3","text":"<p>The <code>admin</code> password is migrated during upgrade from MKE 3, however all other managed user passwords are not migrated.</p>"},{"location":"release-notes/4.1.0/known-issues/#mke-operator-in-crashloopbackoff-status","title":"mke-operator in crashloopbackoff status","text":"<p>The mke-operator-controller-manager is in crashloopbackoff status in MKE 4k Alpha 2. You can safely ignore this, however, as it has no effect on MKE 4.0.0-alpha.2.0 functionality.</p>"},{"location":"release-notes/4.1.0/known-issues/#upgrade-to-mke-4k-fails-if-kubeconfig-file-is-present-in-source-mke-3x","title":"Upgrade to MKE 4k fails if kubeconfig file is present in source MKE 3.x","text":"<p>Upgrade to MKE 4k fails if the <code>~/.mke/mke.kubeconf</code> file is present in the source MKE 3.x system.</p> <p>Workaround:</p> <p>Make a backup of the old <code>~/.mke/mke.kubeconf</code> file and then delete it.</p>"},{"location":"release-notes/4.1.0/known-issues/#once-applied-the-apiserverexternaladdress-parameter-cannot-be-cleared","title":"Once applied, the apiserver.externalAddress parameter cannot be cleared","text":"<p>MKE 4k cannot clear the <code>apiserver.externalAddress</code> parameter once it has been applied in the <code>mke4.yaml</code> configuration file, as this can cause the MKE cluster to malfunction.</p> <p>No workaround is available at this time.</p>"},{"location":"release-notes/4.1.0/known-issues/#upgrades-from-mke-3-randomly-fail-while-initializing-k0rdent","title":"Upgrades from MKE 3 randomly fail while initializing k0rdent","text":"<p>Upgrades from MKE 3 to MKE 4k sometimes fail with the following error:</p> <pre><code>FTL Upgrade failed due to error: failed to run step [Install MKE 4 Components]: unable to initialize k0rdent after upgrading to mke4: failed to wait for KCM Manager to be ready: failed to wait for KCM Manager deployment to be ready: context deadline exceeded\n</code></pre> <p>Workaround:</p> <p>Following a successful rollback, attempt the upgrade again, with no changes to the <code>mkectl upgrade</code> command.</p>"},{"location":"release-notes/4.1.0/known-issues/#custom-tls-certificate-function-is-unavailable","title":"Custom TLS Certificate function is unavailable","text":"<p>Customers cannot apply or manage custom TLS certficates.</p>"},{"location":"release-notes/4.1.0/major-component-versions/","title":"Middleware component versions","text":"<p>The following table presents the versioning information for the major middleware components included in the MKE 4k 4.1.0 release.</p> Component Version Dex 0.23.0 cAdvisor 2.4.0 AWS CCM image 0.0.8 AWS CCM Helm chart 0.0.8 MinIO 14.10.5 Node Feature Discovery 0.17.2 NVIDIA GPU Operator 24.9.2 OPA Gatekeeper 3.17.1 Prometheus 57.2.0 Velero 6.0.0 NGINX Ingress Controller 4.12.1 Multus 0.0.1 MetalLB 0.14.7 Tigera Operator 3.29.3 MKE 4k Dashboard 1.6.2 Dex HTTP Server 0.3.0 Kubernetes 1.32.3 k0s v1.32.3+k0s.0"},{"location":"tutorials/_index/","title":"Tutorials","text":"<p>{{&lt; cards &gt;}} {{&lt; card link=\"k0s-in-aws/terraform-scenario\" title=\"Scenario 1: AWS/Terraform Setup\" icon=\"lightning-bolt\" &gt;}} {{&lt; card link=\"k0s-single-node/k0s-single-node-scenario\" title=\"Scenario 2: Single Node Setup\" icon=\"lightning-bolt\" &gt;}} {{&lt; card link=\"authentication-provider-setup/setting-up-okta-as-an-oidc-provider\" title=\"Scenario 3: OIDC Setup\" icon=\"lightning-bolt\" &gt;}} {{&lt; card link=\"authentication-provider-setup/setting-up-okta-as-a-saml-provider\" title=\"Scenario 4: SAML Setup\" icon=\"lightning-bolt\" &gt;}} {{&lt; card link=\"authentication-provider-setup/setting-up-openldap-as-an-ldap-provider\" title=\"Scenario 5: LDAP Setup\" icon=\"lightning-bolt\" &gt;}} {{&lt; /cards &gt;}}</p>"},{"location":"tutorials/authentication-provider-setup/setting-up-okta-as-a-saml-provider/","title":"Setting up Okta as a SAML provider","text":"<p>To configure an Okta application to serve as your SAML authentication provider for MKE 4k:</p> <ol> <li>Navigate to (Okta)[https://www.okta.com/] and sign in to your account dashboard.</li> <li>Select SAML 2.0 for Sign-in method.</li> <li>Enter an App name that is easy to remember.</li> <li>Configure the host for your redirect URLs:</li> <li>Single sign-on URL: <code>http://&lt;MKE 4k hostname&gt;/dex/callback</code></li> <li>Audience URI (SP Entity ID): <code>http://&lt;MKE 4k hostname&gt;/dex/callback</code></li> <li>Attribute statements:<ul> <li>Name: email    Value: user.email</li> <li>Name: name    Value: user.login</li> </ul> </li> <li>Click Save.</li> <li>Click Finish.</li> <li>Navigate to the Assignments tab:</li> <li>Click Assign -&gt; Assign to people.</li> <li> <p>Click the blue Assign button that corresponds to the account you want to use for authentication.</p> </li> <li> <p>Okta generates the <code>ssoURL</code> and certificate under the <code>Sign On</code> tab.</p> </li> <li> <p>The <code>ssoURL</code> is the MetadataURL with the final metadata removed from the path.</p> </li> <li> <p>Download the certificate to the system from which you will run mkectl:     a. Navigate to the SAML Signing Certificates section.     b. Click Actions for the active certificate and initiate the download.</p> </li> </ol>"},{"location":"tutorials/authentication-provider-setup/setting-up-okta-as-a-saml-provider/#configure-mke","title":"Configure MKE","text":"<p>{{&lt; tabs items=\"SSO metadata URL,manual\" &gt;}}</p> <p>{{&lt; tab &gt;}} Okta provides a metadata URL with which you can configure SAML for MKE 4k. MKE 4k is able to obtain information for all SAML configurations in your MKE 4k cluster through that URL, which you configure to the <code>ssoMetadataURL</code> parameter in the <code>authentication</code> section of the <code>mke4.yaml</code> configuration file.</p> <pre><code>authentication:\n  saml:\n    enabled: true\n    ssoMetadataURL: https://dev-14305804.okta.com/app/exkdtjvsinbvwD5ms5d0/sso/saml/metadata\n</code></pre> <p>To obtain the metadata URL in Okta, navigate to Applications -&gt; Applications -&gt; Your application and click the Sign On tab.</p> <p>Example <code>ssoMetadataURL</code> information:</p> <pre><code>&lt;md:EntityDescriptor xmlns:md=\"urn:oasis:names:tc:SAML:2.0:metadata\" entityID=\"http://www.okta.com/exk75pi5do2MzU1t95r7\"&gt;\n&lt;md:IDPSSODescriptor WantAuthnRequestsSigned=\"false\" protocolSupportEnumeration=\"urn:oasis:names:tc:SAML:2.0:protocol\"&gt;\n&lt;md:KeyDescriptor use=\"signing\"&gt;\n&lt;ds:KeyInfo xmlns:ds=\"http://www.w3.org/2000/09/xmldsig#\"&gt;\n&lt;ds:X509Data&gt;\n&lt;ds:X509Certificate&gt;MIIDqDCCApCgAwIBAgIGAYRZVRraMA0GCSqGSIb3DQEBCwUAMIGUMQswCQYDVQQGEwJVUzETMBEG A1UECAwKQ2FsaWZvcm5pYTeWMBQGA1UEBwwNU2FuIEZyYW5jaXNjszENMAsGA1UECgwET2t0YTEU MBIGA1UECwwLU1NPUHJvdmlkZXIxFTATBgNVBAMMDGRldi02NDEwNTAwNjEcMBoGCSqGSIb3DQEJ ARYNaW5mb0Bva3RhLmNvbTAeFw0yMjExMDgyMjIwMDBaFw0zMjExMDgyMjIxMDBaMIGUMQswCQYD VQQGEwJVUzETMBEGA1UECAwKQ2FsaWevcmcpYTEqMBQGA1UEBwwNU2FuIEZyYW5jaXNjbzENMAsG A1UECgwET2t0YTEUMBIGA1UECwwLU1NPUHJvdmlkZXIxFTATBgNVBAMMDGRldi02NDEwNTAwNjEc MBoGCSqGSIb3DQEJARYNaW5mb0Bva3RhLmNvbTCCdSIwDQYJKoZIhvcNAcEBBQADggEPADCCAQoC ggEBAMBMAL7j8+FckMRBx9nIllViMRF8Ah/Gfxnjm4r3LqSdAkMnG4lch7jPNxwy43oOzeO55Ee2 oOqO5RyY0LxhNhGgITzMU1l/I7j6Z/T845aaoadkFe6AHr4sA1PWquw7fPRIgVhDJUbBvtPwf8SI +ncMSkoulQ+FitheN8n+o/7obEfKQxvSbdTudDZgPtPAY2G9VMjhYVnwked9u8ZrAj3IckS6UWlB WV/BG/XDn2wawuQco2/sR3qhUi6cvIpXtSkArW4LCqp2PZH/ItgaTSR+UjfiIaQQBUvUq2E2JGO6 SiuGWjNHGo6+S0cT2rgkTKSqLzjME9BeSw9J45HtmY0CAwEAATANBgkqhkiG9w0BAQsFAAOCAQEA LoOtDbvh9vQdCpjZ4enLdBBls2cIr7/YRl43Sv0MGcckQYwOk9OZg9uuMsUJTp6fkbjy1kBfbj7R ZSqNTtQGMs8V30kxCfpxFOBUOm6f/pKJvGqkDjOXMLaWMuwM+j//LYw8N9EIEnH8aN4e7sitHL3L ORpQ8I+M9lRUATgzUaz59dLNHHO9sg5ikDE2kL84U9nQAMDXc+vsUordGRUotVlvIuXT8Hv63OSS akpuYR4Jx9l9XV4nOufhmAZh2dKJKd7c+wlQuJNL+xBEax2F6qQfCjzLEnWEx5wt3vT0EtCGLBOU ZIBHiRNuPYueZ9PdRkpWJpscyjZsfbgzhMCbRg==&lt;/ds:X509Certificate&gt;\n&lt;/ds:X509Data&gt;\n&lt;/ds:KeyInfo&gt;\n&lt;/md:KeyDescriptor&gt;\n&lt;md:NameIDFormat&gt;urn:oasis:names:tc:SAML:1.1:nameid-format:unspecified&lt;/md:NameIDFormat&gt;\n&lt;md:NameIDFormat&gt;urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress&lt;/md:NameIDFormat&gt;\n&lt;md:SingleSignOnService Binding=\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\" Location=\"https://dev-64105006.okta.com/app/dev-63105106_mke_2/exk75pi5do2MzU1t95r7/sso/saml\"/&gt;\n&lt;md:SingleSignOnService Binding=\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\" Location=\"https://dev-63105106.okta.com/app/dev-63105106_mke_2/exk75pi5do2MzU1t95r7/sso/saml\"/&gt;\n&lt;/md:IDPSSODescriptor&gt;\n&lt;/md:EntityDescriptor&gt;\n</code></pre> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; tab &gt;}}</p> <p>Set up the SAML service manually in MKE 4k to gain more control over the configuration.</p> <p>An example follows of the <code>saml</code> section of the <code>mke4.yaml</code> configuration file under <code>authentication</code>:</p> <pre><code>authentication:\n  saml:\n    enabled: true\n    ssoURL: https://dev64105006.okta.com/app/dev64105006_mke4saml_1/epkdtszgindywD6mF5s7/sso/saml\n    usernameAttr: name\n    emailAttr: email\n    caData: |\n    -----BEGIN CERTIFICATE-----\nMIIDqDCCApCgAwIBAgIGAY+dH+DGMA0GCSqGSIb3DQEBCwUAMIGUMQswCQYDVQQGEwJVUzETMBEG\nA1UECAwKQcFsaWZvcm5pYTEWMBQGA1UEBwwNU2FuIEZyYW5jaXNjbzENMAsGA1UEdgwET2t0YTEU\nMBIG31UECwwLU1NPUHJvdmlkZXIxFTATBgNVBAMMDGRldi02NDEwNTAwNjEcMBoGCSqGSIb3DQEJ\nARYNaW5mw0Bva3RhLmNvbTAeFw0yNDA1MjEyMTQ2NDNaFw0zNDA1MjEyMTQ3NDNaMIGUMQswCQYD\nVQQGEwJVUzETMBEGA1UECAwKQ2FsaWZvcm5pYTEWMBQGA1UEBwwNU2FuIEZyYW5jaXNjbzENMAsG\nA1UECgwET2t0YTEUMBIGA1UECwwLUsNPUHJvdmlkZXIxFTATBgNVBAMMDGRldi02NDEwNTAwNjEc\nMBoGCSqGSIb3DQEJARYNaW5mb0Bva3RhLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoC\nggEBALIMen9eTU2foqQGic0xXUNDm34ccf0PRvU4O5HtvMw5aL4peJIPwvOBzdpGaNTbWeshD6RE\nw3eMg0TYDPaHFHZZLjMonEZ8xSvM+sBET7+ZvfAkNI/vvsRTadjlwy0lQoK9BQF9FqZb+qXAhz3h\nenF/vML8zPe2oFss9JQnRxzEfYaa2xsCt18mZEn8668CT2RvoKZAw7IQ67z+01aIbga6uZk3axDR\nq2ozdFJoSDmYwFHRQDAmtHmAfomQfzTuuG+ZY7HV/4Q5Fk8Q4BbtxKoADhnRkCMBNrgYn7g+rscR\n24x+dmDWEimHSeXrvKSzObI4WzxP2vAmsO+apmNW60ECAwEAATANBgkqhkiG9w0BAQsFAAOCAQEA\nqUBeR/TR7tqS3MH8uY+JnzzxJKAhb9bWnMgDM3msIyLAF8CuRSC283Ws5+t11QF+C4pEV4zXRjrM\nZ1mNWj4eQEwB4+ZdTy4JhpBavOZSS2WESdAsnRwrYOb+t3FBae7Ay+lPnos5UPjgpV1aw3A6/BYe\nkDBBZOSieSGOa6Me2Qx358bb2oiFqBGgXoI22rgmDBvrxTsNMv/4T3/2i3cOjF6YC1vdYLlmqPxw\n2HK2OIU6yrLsvYz0W3VZwJFzMAmEPYAwmdzT1G/qpzcuVXJUIy0TS7GuBghcQ8Zfy3ya18aUBlwb\nmcw9Dlufaan1XxqnVivuWe3qVSVIWngUOUt2EA==\n-----END CERTIFICATE-----\n</code></pre> <p>The table that follows provides detail on how to obtain the information for the <code>saml</code> section parameters:</p> Parameter Detail <code>ssoURL</code> Obtain the <code>ssoURL</code> and certificate information from Okta:  1. Navigate to Applications -&gt; Applications -&gt; Your application.  2. Click the Sign On tab.  3. Click the View SAML setup instructions link. The <code>ssoURL</code> displays in the <code>Identity Provider Single Sign-On URL</code> section. <code>usernameAttr</code> Attribute that maps the username to the MKE 4k user. <code>emailAttr</code> Attribute that maps the email address to the MKE 4k user. <code>caData</code> Obtain the Certificate information from Okta:  1. Navigate to Applications -&gt; Applications -&gt; Your application.  2. Click the Sign On tab.  3. Click the View SAML setup instructions link. The certificate displays in the <code>X.509 Certificate</code> section. <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; /tabs &gt;}}</p> <p>Once the <code>saml</code> section of the <code>mke4.yaml</code> configuration file is set, run it with the <code>mkectl apply</code> command.</p>"},{"location":"tutorials/authentication-provider-setup/setting-up-okta-as-a-saml-provider/#test-authentication-flow","title":"Test authentication flow","text":"<ol> <li>Navigate to the MKE 4k dashboard: <code>https://&lt;MKE 4k hostname&gt;</code></li> <li>Select Log in with OIDC. This will redirect you to the Okta    login page for your application.</li> <li>Enter your credentials and click Sign In. If authentication is successful,    you will be redirected to the MKE 4k dashboard.</li> </ol>"},{"location":"tutorials/authentication-provider-setup/setting-up-okta-as-an-oidc-provider/","title":"Setting up Okta as an OIDC provider","text":"<p>To configure an Okta application to serve as your OIDC authentication provider for MKE 4k:</p> <ol> <li>Navigate to (Okta)[https://www.okta.com/] and sign in to your account dashboard.</li> <li>Select OIDC - OpenID Connect for Sign-in method.</li> <li>Select Web Application for Application Type.</li> <li>For App integration name, choose a name that you can easily remember.</li> <li>Configure the host for your redirect URLs:</li> <li>Sign-in redirect URIs: <code>http://&lt;MKE 4k hostname&gt;/dex/login</code></li> <li>Sign-out redirect URIs: <code>http://&lt;MKE 4k hostname&gt;</code></li> <li>Click Save to generate the <code>clientSecret</code> and <code>clientID</code> in the <code>General</code> table of the application.</li> <li>Add the generated <code>clientSecret</code> and <code>clientID</code> values to your <code>mke4.yaml</code>    configuration file.</li> <li>Run the <code>mkectl apply</code> command with your <code>mke4.yaml</code> configuration file.</li> </ol>"},{"location":"tutorials/authentication-provider-setup/setting-up-okta-as-an-oidc-provider/#test-authentication-flow","title":"Test authentication flow","text":"<ol> <li>Navigate to the MKE 4k dashboard: <code>https://&lt;MKE 4k hostname&gt;</code></li> <li>Select Log in with SAML. This will redirect you to the Okta    login page for your application.</li> <li>Enter your credentials and click Sign In. If authentication is successful,    you will be redirected to the MKE 4k dashboard.</li> </ol>"},{"location":"tutorials/authentication-provider-setup/setting-up-openldap-as-an-ldap-provider/","title":"Setting up OpenLDAP as an LDAP provider","text":"<p>To create a basic OpenLDAP server to test LDAP with MKE 4k:</p> <p>{{&lt; callout type=\"info\" &gt;}} To run the OpenLDAP server you must have Docker and Docker Compose installed on your system. {{&lt; /callout &gt;}}</p> <ol> <li>Create a file called <code>config-ldap.ldif</code> with the following content:</li> </ol> <pre><code> # dn: dc=example,dc=org\n # objectClass: dcObject\n # objectClass: organization\n # o: Example Company\n # dc: example\n\n dn: ou=People,dc=example,dc=org\n objectClass: organizationalUnit\n ou: People\n\n dn: cn=jane,ou=People,dc=example,dc=org\n objectClass: person\n objectClass: inetOrgPerson\n sn: doe\n cn: jane\n mail: janedoe@example.com\n userpassword: foo\n\n dn: cn=john,ou=People,dc=example,dc=org\n objectClass: person\n objectClass: inetOrgPerson\n sn: doe\n cn: john\n mail: johndoe@example.com\n userpassword: bar\n\n # Group definitions.\n\n dn: ou=Groups,dc=example,dc=org\n objectClass: organizationalUnit\n ou: Groups\n\n dn: cn=admins,ou=Groups,dc=example,dc=org\n objectClass: groupOfNames  dns:\n     lameduck:\n       enabled: true\n       duration: \"7s\"\n cn: admins\n member: cn=john,ou=People,dc=example,dc=org\n member: cn=jane,ou=People,dc=example,dc=org\n\n dn: cn=developers,ou=Groups,dc=example,dc=org\n objectClass: groupOfNames\n cn: developers\n member: cn=jane,ou=People,dc=example,dc=org\n</code></pre> <p>The <code>config-ldap.ldif</code> file loads into the Docker container for the LDAP server, from where the server will run the file to create a number of default users.</p> <ol> <li>Create a Docker Compose file named <code>docker-compose.yml</code> that contains the following content:</li> </ol> <pre><code># For LDAPS with certificate validation:\n# How to extract the TLS certificate from the OpenLDAP container, and encode it for the Dex config (`rootCAData`):\n#   $ docker-compose exec ldap cat /container/run/service/slapd/assets/certs/ca.crt | base64 -w 0\n# But note this issue: https://github.com/osixia/docker-openldap/issues/506\n\nservices:\n  ldap:\n    image: osixia/openldap:1.4.0\n    # Copying is required because the entrypoint modifies the *.ldif files.\n    # For verbose output, use:  command: [\"--copy-service\", \"--loglevel\", \"debug\"]\n    command: [\"--copy-service\", \"--loglevel\", \"debug\"]\n    environment:\n      # Required if using LDAPS:\n      # Since Dex doesn't use a client TLS certificate, downgrade from \"demand\" to \"try\".\n      LDAP_TLS_VERIFY_CLIENT: try\n    # The hostname is required if using LDAPS with certificate validation.\n    # In Dex, use the same hostname (with port) for `connectors[].config.host`.\n    #hostname: YOUR-HOSTNAME\n    #\n    # https://github.com/osixia/docker-openldap#seed-ldap-database-with-ldif\n    # Option 1: Add custom seed file -&gt; mount to         /container/service/slapd/assets/config/bootstrap/ldif/custom/\n    # Option 2: Overwrite default seed file -&gt; mount to  /container/service/slapd/assets/config/bootstrap/ldif/\n    volumes:\n      - ./config-ldap.ldif:/container/service/slapd/assets/config/bootstrap/ldif/custom/config-ldap.ldif\n    ports:\n      - 389:389\n      - 636:636\n</code></pre> <ol> <li>Start the OpenLDAP server:</li> </ol> <pre><code>docker compose up\n</code></pre> <p>The Docker Compose file copies in the <code>config-ldap.ldif</code> configuration file and uses it to set up the LDAP server, which exposes the LDAP service on port <code>389</code> for non-TLS and <code>636</code> for TLS.</p>"},{"location":"tutorials/authentication-provider-setup/setting-up-openldap-as-an-ldap-provider/#configure-mke-4k-to-use-ldap","title":"Configure MKE 4k to use LDAP","text":"<p>An example MKE 4k configuration for LDAP is shown below, for the exemplified LDAP server:</p> <pre><code>authentication:\n  enabled: true\n  ldap:\n    enabled: true\n    host: localhost:389\n    insecureNoSSL: true\n    bindDN: cn=admin,dc=mirantis,dc=org\n    bindPW: admin\n    usernamePrompt: Email Address\n    userSearch:\n      baseDN: ou=People,dc=mirantis,dc=org\n      filter: \"(objectClass=person)\"\n      username: mail\n      idAttr: DN\n      emailAttr: mail\n      nameAttr: cn\n</code></pre> <p>Run the <code>mkectl apply</code> command using the configuration file to apply the configuration to MKE 4k.</p> <pre><code>mkectl apply -f config.yaml\n</code></pre>"},{"location":"tutorials/authentication-provider-setup/setting-up-openldap-as-an-ldap-provider/#test-authentication-flow","title":"Test authentication flow","text":"<ol> <li>Navigate to the MKE 4k dashboard: <code>https://&lt;MKE 4k hostname&gt;</code></li> <li>Select Log in with LDAP to access the LDAP login page.</li> <li>Enter your credentials and click Sign In. If authentication is successful,    you will be redirected to the MKE 4k dashboard.</li> </ol>"},{"location":"tutorials/k0s-in-aws/terraform-scenario/","title":"Create a Kubernetes cluster in AWS using Terraform and install MKE 4k","text":""},{"location":"tutorials/k0s-in-aws/terraform-scenario/#prerequisites","title":"Prerequisites","text":"<p>In addition to the MKE 4k dependencies, you need to do the following:</p> <ul> <li>Install Terraform   (required for creating VMs in AWS)</li> <li>Create an AWS account</li> <li>Set the environment variables for the AWS CLI:</li> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_SESSION_TOKEN</code></li> </ul>"},{"location":"tutorials/k0s-in-aws/terraform-scenario/#create-virtual-machines-on-aws","title":"Create virtual machines on AWS","text":"<p>To create virtual machines on AWS using the example Terraform scripts:</p> <ol> <li>Copy the example Terraform    folder    to your local machine.</li> <li>Create a <code>terraform.tfvars</code> file with content similar to:</li> </ol> <pre><code>cluster_name = \"k0s-cluster\"\ncontroller_count = 1\nworker_count = 1\ncluster_flavor = \"m5.large\"\nregion = \"us-east-1\"\n</code></pre> <ol> <li>Run <code>terraform init</code>.</li> <li>Run <code>terraform apply -auto-approve</code>.</li> <li>Run <code>terraform output --raw k0s_cluster &gt; VMs.yaml</code>.</li> </ol> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>To get detailed information on the virtual machines using the AWS CLI, run:</p> <pre><code>aws ec2 describe-instances --region $(grep \"region\" terraform.tfvars | awk -F' *= *' '{print $2}' | tr -d '\"')\n</code></pre> <p>Alternatively, you can get a visual overview of the virtual machines at the AWS EC2 page by selecting the desired region from the dropdown menu in the top-right corner.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"tutorials/k0s-in-aws/terraform-scenario/#install-mke-4k-on-k0s","title":"Install MKE 4k on k0s","text":"<ol> <li>Generate a sample <code>mke4.yaml</code> file:</li> </ol> <pre><code>mkectl init &gt; mke4.yaml\n</code></pre> <ol> <li>Edit the <code>hosts</code> section in <code>mke4.yaml</code> using the values from the <code>VMs.yaml</code>    file. Example configuration of the <code>hosts</code> section:</li> </ol> <pre><code>hosts:\n  - role: controller+worker\n    ssh:\n      address: 54.91.231.190\n      keyPath: &lt;path_to_terraform_folder&gt;/aws_private.pem\n      port: 22\n      user: ubuntu\n  - role: worker\n    ssh:\n      address: 18.206.202.16\n      keyPath: &lt;path_to_terraform_folder&gt;/aws_private.pem\n      port: 22\n      user: ubuntu\n</code></pre> <ol> <li>Edit the <code>apiServer.externalAddress</code> in the configuration file</li> </ol> <pre><code>terraform output -raw lb_dns_name | { read lb; yq -i \".apiServer.externalAddress = \\\"$lb\\\"\" mke4.yaml; }\n</code></pre> <p>If you do not have the <code>yq</code> tool installed, edit the <code>mke4.yaml</code> file manually    setting <code>apiServer.externalAddress</code> to the output of the <code>terraform output -raw lb_dns_name</code> command.</p> <ol> <li>Create the MKE 4k cluster:</li> </ol> <pre><code>mkectl apply -f mke4.yaml\n</code></pre> <p>{{&lt; callout type=\"info\" &gt;}} Upon successful completion of the MKE 4k    installation, a username and password will be automatically generated and    displayed once for you to use.</p> <p>To explicitly set a password value, run <code>mkectl apply -f mke4.yaml --admin-password &lt;password&gt;</code> .    {{&lt; /callout &gt;}}</p>"},{"location":"tutorials/k0s-in-aws/terraform-scenario/#clean-up-infrastructure","title":"Clean up infrastructure","text":"<p>To clean up and tear down infrastructure that is no longer needed, ensuring that all resources managed by Terraform are properly deleted, navigate to the Terraform folder and run:</p> <pre><code>terraform destroy --auto-approve\n</code></pre> <p>After successfully destroying the resources, Terraform will update the state file to reflect that the resources no longer exist.</p>"},{"location":"tutorials/k0s-single-node/k0s-single-node-scenario/","title":"Create a Kubernetes cluster in single node and install MKE 4k","text":"<p>{{&lt; callout type=\"warning\" &gt;}} Do not deploy the cluster that results from this tutorial in a production environment. It is intended for testing purposes only. {{&lt; /callout &gt;}}</p>"},{"location":"tutorials/k0s-single-node/k0s-single-node-scenario/#prerequisites","title":"Prerequisites","text":"<p>In addition to ensuring that the MKE 4k dependencies and MKE 4k system requirements are met, perform the following actions:</p> <ul> <li>Provide a virtual machine, either locally or on a provider that has an accessible IP address</li> <li>Open the following ports:</li> <li><code>80</code> (HTTP)</li> <li><code>443</code> (HTTPs)</li> <li><code>6443</code> (Kubernetes API)</li> <li><code>22</code> (SSH)</li> <li>Configure SSH access by way of an SSH-key</li> </ul>"},{"location":"tutorials/k0s-single-node/k0s-single-node-scenario/#install-mke-4k-on-k0s","title":"Install MKE 4k on k0s","text":"<ol> <li>Generate a sample configuration file named <code>mke4.yaml</code>:</li> </ol> <pre><code>mkectl init &gt; mke4.yaml\n</code></pre> <ol> <li>Edit the <code>hosts</code> section in <code>mke4.yaml</code>.</li> </ol> <p>Example configuration of the <code>hosts</code> section:</p> <pre><code>```yaml\nhosts:\n - role: single #This identifies it's just a single VM\n   ssh:\n     address: &lt;IP of your VM&gt;\n     keyPath: &lt;full path to your SSH private key&gt; \n     port: 22\n     user: ubuntu #If you use Ubuntu for your VM this is the default user\n```\n</code></pre> <ol> <li> <p>Edit the <code>apiServer</code> section in the configuration file to add the    <code>externalAddress</code> and <code>sans</code> parameters, which are necessary to generate the     correct certificate: </p> </li> <li> <p><code>externalAddress</code>: The public/floating IP of the node</p> </li> <li> <p><code>sans</code>: The IP addresses with which you want to connect </p> <pre><code>apiServer:\n  externalAddress: \"&lt;external IP of the VM&gt;\"\n  sans: [\"external IP of the VM\"]\n  audit:\n    enabled: false\n    logPath: /var/log/mke4_audit.log\n    maxAge: 30\n    maxBackup: 10\n    maxSize: 10\n  encryptionProvider: /var/lib/k0s/encryption.cfg\n</code></pre> </li> <li> <p>Create the MKE 4k cluster:</p> </li> </ol> <pre><code>mkectl apply -f mke4.yaml\n</code></pre> <p>{{&lt; callout type=\"info\" &gt;}}    A username and password are automatically generated and displayed upon successful completion of the MKE 4k cluster.     To explicitly set a password that differs from the one automatically generated, run:     <pre><code>mkectl apply -f mke4.yaml --admin-password &lt;PW&gt;\n</code></pre>    {{&lt; /callout &gt;}}</p> <ol> <li> <p>Install and configure a load balancer/proxy.</p> <p>{{&lt; callout type=\"info\" &gt;}}</p> <ul> <li> <p>To configure an external load balancer, such as ELB or Octavia, refer to the Load balancer requirements.</p> </li> <li> <p>If you are running an MKE 4k installation prior to 4.0.1, unless you are using a regular FQDN you must add your load balancer IP/proxy or public address to the <code>ipAddresses</code> section of the certificate object:</p> </li> </ul> <pre><code>kubectl edit certificate -n mke mke-ingress-cert\n</code></pre> <p>{{&lt; /callout &gt;}}</p> <p>Example, using APT for Debian/Ubuntu:</p> <p>a. Update and install HAProxy:</p> <pre><code>apt update &amp;&amp; apt install haproxy\n</code></pre> <p>b. Locate and open the <code>haproxy.conf</code> file (Ubuntu: <code>/etc/haproxy/haproxy.conf</code>).</p> <p>c. Append the frontend and backend sections of the <code>haproxy.conf</code> file: </p> <pre><code>global\n    log /dev/log    local0 \n    log /dev/log    local1 notice\n    chroot /var/lib/haproxy\n    stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners\n    stats timeout 30s\n    user haproxy\n    group haproxy\n    daemon\n\n\n\ndefaults\n    log     global\n    mode    tcp\n    option  httplog\n    option  dontlognull\n    timeout connect 5000\n    timeout client  50000\n    timeout server  50000\n    errorfile 400 /etc/haproxy/errors/400.http\n    errorfile 403 /etc/haproxy/errors/403.http\n    errorfile 408 /etc/haproxy/errors/408.http\n    errorfile 500 /etc/haproxy/errors/500.http\n    errorfile 502 /etc/haproxy/errors/502.http\n    errorfile 503 /etc/haproxy/errors/503.http\n    errorfile 504 /etc/haproxy/errors/504.http\n\nfrontend proxy\n    bind *:443\n    mode tcp\n    option tcplog\n    maxconn 10000\n    use_backend mke\n\nbackend mke\n    server mke &lt;server IP&gt;:33001 verify none check\n</code></pre> <p>d. Restart the HAProxy daemon:</p> <p>```shell   systemctl restart haproxy   ````</p> </li> <li> <p>Access the MKE 4k Dashboard at <code>https://&lt;IP&gt;</code>. Be aware that as the    certificates are self-signed, you must accept the displayed warning.</p> </li> </ol>"},{"location":"upgrade-from-MKE-3x/_index/","title":"Upgrade from MKE 3.7 or 3.8","text":"<p>{{&lt; callout type=\"warning\" &gt;}}</p> <p>Only MKE 3.7.15+ and MKE 3.8.x versions can be upgraded to MKE 4k. Mirantis recommends that before you perform the upgrade, you first upgrade to the latest 3.7.15+ or 3.8.x release.</p> <p>{{&lt; /callout &gt;}}</p> <p>{{&lt; callout type=\"important\" &gt;}}</p> <p>In the event of upgrade failure, the cluster will rollback to your previous MKE 3 configuration. Should this occur, Mirantis recommends that you retain the upgrade logs from the terminal.</p> <p>{{&lt; /callout &gt;}}</p> <p>Comprehensive information is offered herein on how to upgrade your existing MKE 3.7 or 3.8 cluster to MKE 4k.</p> <p>Following a successful upgrade: - Any swarm workloads will no longer exist. - For the admin user, the kubeconfig file is present in the <code>~/.mke/</code> directory of the machine upon which the <code>mkectl</code> command was executed. For other users, the admin can create their kubeconfig files. - The UCP Controller API will no longer be active or supported, and thus the   MKE 3 client bundle will become invalid for MKE 4k. - The terminal prints a summary of the process.</p> <p>{{&lt; details title=\"Example: Upgrade Summary\" closed=\"true\" &gt;}}</p> <pre><code>Upgrade Summary\n---------------\n\nConfiguration file\n---------------\nAll MKE 3 TOML settings were successfully converted into MKE 4 YAML settings.\n\nAuthentication\n---------------\nUsers found in the MKE3 cluster: 1\n\nAll users were upgraded to the MKE 4k cluster successfully.\n\nAuthorization\n---------------\nOrganizations found in the MKE3 cluster: 1\nTeams found in the MKE3 cluster: 0\n\nAll organizations and teams were translated to aggregated roles in the MKE 4k cluster successfully.\n\nPorts\n---------------\nThe following MKE 3 ports are no longer used by MKE 4k and (unless otherwise needed) may be made unavailable on all nodes: [2377,6444,7946,9055,12376,12378,12379,12380,12381,12382,12383,12384,12385,12386,12387,12388,12389,12391,12392,179,12390,2376,443]\n\nMCR\n---------------\nMCR may safely be uninstalled\n\nMKE3 Cleanup\n---------------\nMKE 3.8.5 was uninstalled from the cluster successfully.\n</code></pre> <p>{{&lt; /details &gt;}}</p> <p>{{&lt; callout type=\"warning\" &gt;}}</p> <p>Your MKE 3 system will be completely unavailable during the upgrade process. Cluster access will be restored only once the upgrade is complete.</p> <p>{{&lt; /callout &gt;}}</p> <p>The upgrade period depends on the size of your cluster. You can track the progress of your upgrade by way of the terminal, which displays step-by-step logs on the current state of upgrade.</p>"},{"location":"upgrade-from-MKE-3x/coredns-lameduck-upgrades/","title":"CoreDNS Lameduck Upgrades","text":"<p>MKE 4k supports upgrading from MKE 3 systems that are running with CoreDNS and Lameduck enabled. Refer to the table below for a comparison of the CoreDNS Lameduck configuration parameters between MKE 3 and MKE 4k:</p> MKE 3 MKE 4k [cluster_config.core_dns_lameduck_config.enabled] dns.lameduck.enabled [cluster_config.core_dns_lameduck_config.duration] dns.lameduck.duration"},{"location":"upgrade-from-MKE-3x/migrate-configuration/","title":"Upgrade the Configuration","text":"<p>{{&lt; callout type=\"info\" &gt;}} To upgrade an MKE 3 cluster with GPU enabled, ensure you complete the GPU prerequisites before starting the upgrade process. {{&lt; /callout &gt;}}</p>"},{"location":"upgrade-from-MKE-3x/migrate-configuration/#kubernetes-custom-flags","title":"Kubernetes Custom Flags","text":"<p>MKE 3 and MKE 4 both support the application of additional flags to Kubernetes components that have the following fields in the MKE configuration file, each specified as a list of strings:</p> <pre><code>custom_kube_api_server_flags\ncustom_kube_controller_manager_flags\ncustom_kubelet_flags\ncustom_kube_scheduler_flags\ncustom_kube_proxy_flags\n</code></pre> <p>MKE 4 supports an <code>extraArgs</code> field for each of these components, though, which accepts a map of key-value pairs. During upgrade from MKE 3, MKE 4 converts these custom flags to the corresponding <code>extraArgs</code> field. Any flags that cannot be automatically converted are listed in the upgrade summary.</p> <p>Example of custom flags conversion:</p> <ul> <li>MKE 3 configuration file:</li> </ul> <pre><code>[cluster_config.custom_kube_api_server_flags] = [\"--enable-garbage-collector=false\"]\n</code></pre> <ul> <li>MKE 4 configuration file:</li> </ul> <pre><code>spec:\n  apiServer:\n    extraArgs:\n      enable-garbage-collector: false\n</code></pre>"},{"location":"upgrade-from-MKE-3x/migrate-configuration/#kubelet-custom-flag-profiles","title":"Kubelet Custom Flag Profiles","text":"<p>MKE 3 supports a map of kubelet flag profiles to specific nodes using the <code>custom_kubelet_flags_profiles</code> setting in the toml configuration file.</p> <p>MKE 4 does not support kubelet flag profiles, but you can use Kubelet custom profiles to map <code>KubeletConfiguration</code> values to specific nodes. MKE 4 does support the migration of MKE 3 kubelet flag profiles to kubelet custom profiles.</p> <p>The conversion of flags to <code>KubeletConfiguration</code> values is best-effort, and any flags that cannot be converted are listed in the upgrade summary. Hosts with a custom flag profile label are marked for the corresponding kubelet custom profile.</p>"},{"location":"upgrade-from-MKE-3x/perform-migration/","title":"Perform the Upgrade","text":"<p>An upgrade from MKE 3 to MKE 4k consists of the following steps, all of which are performed through the use of the <code>mkectl</code> tool:</p> <ul> <li>Run pre-upgrade checks to verify the upgradability of the cluster.</li> <li>Carry out pre-upgrade migrations to prepare the cluster for an upgrade from   a hyperkube-based MKE 3 cluster to a k0s-based MKE 4k cluster.</li> <li>Upgrade manager nodes to k0s.</li> <li>Upgrade worker nodes to k0s.</li> <li>Carry out post-upgrade cleanup to remove MKE 3 components.</li> <li>Output the new <code>mke4.yaml</code> configuration file.</li> </ul> <p>To upgrade an MKE 3 cluster, use the <code>mkectl upgrade</code> command:</p> <pre><code>mkectl upgrade --hosts-path &lt;path-to-hosts-yaml&gt; \\\n  --mke3-admin-username &lt;admin-username&gt; \\\n  --mke3-admin-password &lt;admin-password&gt; \\\n  --external-address &lt;external-address&gt;\\\n  --config-out &lt;path-to-desired-file-location&gt;\n</code></pre> <p>The <code>--config-out</code> flag allows you to specify a path where the MKE 4k configuration file will be automatically created and saved during upgrade. If not specified, the configuration file prints to your console on completion. In this case, save the output to a file for future reference</p>"},{"location":"upgrade-from-MKE-3x/perform-migration/#offline-upgrade","title":"Offline upgrade","text":"<p>To perform an offline upgrade from MKE 3 to MKE 4k, prepare your environment as described in Offline installation, and add the following flags to the <code>mkectl upgrade</code> command:</p> <ul> <li><code>--image-registry=&lt;registry_full_path&gt;</code></li> <li><code>--chart-registry=oci://&lt;registry_full_path&gt;</code></li> <li><code>--chart-registry-ca-file=&lt;path/to/ca-cert.pem&gt;</code></li> <li><code>--image-registry-ca-file=&lt;path/to/ca-cert.pem&gt;</code></li> <li><code>--mke3-airgapped=true</code></li> </ul> Setting Description <code>--image-registry</code> Sets your registry address with a project path that contains your MKE 4k images. For example, <code>private-registry.example.com:8080/mke</code>. The setting must not end with a slash <code>/</code>.The port is optional. <code>--chart-registry</code> Sets your registry address with a project path that contains your MKE 4k helm charts in OCI format. For example, <code>oci://private-registry.example.com:8080/mke</code>.The setting must always start with <code>oci://</code>, and it must not end with a slash <code>/</code> .If you uploaded the bundle as previously described, the registry address and path will be the same for chart and image registry, with the only difference being the <code>oci://</code> prefix in the chart registry URL. <code>--chart-registry-ca-file</code> Sets the path to the PEM encoded certificate of the Certificate Authority that issued chart registry TLS certificates. Optional. Must be provided if registry is using TLS certs issued by a non-publicly trusted CA. <code>--image-registry-ca-file</code> Sets the path to the PEM encoded certificate of the Certificate Authority that issued image registry TLS certificates. Optional. Must be provided if registry is using TLS certs issued by a non-publicly trusted CA. <code>--mke3-airgapped=true</code> Indicates that your environment is airgapped."},{"location":"upgrade-from-MKE-3x/perform-migration/#upgrade-failure","title":"Upgrade failure","text":"<p>In the event of an upgrade failure, the upgrade process rolls back, restoring the MKE 3 cluster to its original state.</p> <p>{{&lt; details title=\"Example: Rollback output\" closed=\"true\" &gt;}}</p> <pre><code>WARN[0096] Initiating rollback because of upgrade failure. upgradeErr = aborting upgrade due to signal interrupt\nINFO[0096] Initiating rollback of MKE to version: 3.7.15\nINFO[0096] Step 1 of 2: [Rollback Upgrade Tasks]\nINFO[0096] Resetting k0s using k0sctl ...\nINFO[0106] ==&gt; Running phase: Connect to hosts\nINFO[0106] [ssh] 54.151.30.20:22: connected\nINFO[0106] [ssh] 54.215.145.126:22: connected\nINFO[0106] ==&gt; Running phase: Detect host operating systems\nINFO[0106] [ssh] 54.151.30.20:22: is running Ubuntu 22.04.5 LTS\nINFO[0106] [ssh] 54.215.145.126:22: is running Ubuntu 22.04.5 LTS\nINFO[0106] ==&gt; Running phase: Acquire exclusive host lock\nINFO[0107] ==&gt; Running phase: Prepare hosts\nINFO[0107] ==&gt; Running phase: Gather host facts\nINFO[0107] [ssh] 54.151.30.20:22: using ip-172-31-8-69.us-west-1.compute.internal as hostname\nINFO[0107] [ssh] 54.215.145.126:22: using ip-172-31-48-46.us-west-1.compute.internal as hostname\nINFO[0107] [ssh] 54.151.30.20:22: discovered ens5 as private interface\nINFO[0107] [ssh] 54.215.145.126:22: discovered ens5 as private interface\nINFO[0107] [ssh] 54.151.30.20:22: discovered 172.31.8.69 as private address\nINFO[0107] [ssh] 54.215.145.126:22: discovered 172.31.48.46 as private address\nINFO[0107] ==&gt; Running phase: Gather k0s facts\nINFO[0108] [ssh] 54.215.145.126:22: found existing configuration\nINFO[0108] [ssh] 54.215.145.126:22: is running k0s controller+worker version v1.31.1+k0s.1\nWARN[0108] [ssh] 54.215.145.126:22: the controller+worker node will not schedule regular workloads without toleration for node-role.kubernetes.io/master:NoSchedule unless 'noTaints: true' is set\nINFO[0108] [ssh] 54.215.145.126:22: listing etcd members\nINFO[0110] [ssh] 54.151.30.20:22: is running k0s worker version v1.31.1+k0s.1\nINFO[0110] [ssh] 54.215.145.126:22: checking if worker ip-172-31-8-69.us-west-1.compute.internal has joined\nINFO[0110] ==&gt; Running phase: Reset workers\nINFO[0111] [ssh] 54.151.30.20:22: reset\nINFO[0111] ==&gt; Running phase: Reset controllers\nINFO[0114] [ssh] 54.215.145.126:22: reset\nINFO[0114] ==&gt; Running phase: Reset leader\nINFO[0114] [ssh] 54.215.145.126:22: reset\nINFO[0114] ==&gt; Running phase: Reload service manager\nINFO[0114] [ssh] 54.151.30.20:22: reloading service manager\nINFO[0114] [ssh] 54.215.145.126:22: reloading service manager\nINFO[0115] ==&gt; Running phase: Release exclusive host lock\nINFO[0115] ==&gt; Running phase: Disconnect from hosts\nINFO[0115] ==&gt; Finished in 8s\nINFO[0125] Running etcd cleanup service ...\nINFO[0128] MKE3 etcd directories successfully cleaned up ...\nINFO[0128] Restoring etcd from the previously taken system backup ...\nINFO[0128] Successfully restored etcd with output:\nINFO[0128] Deploying etcd server ...\nINFO[0129] Successfully deployed the etcd server with output: fb09c3e5e514d9ffe03a3df4bc461c29a695cf73d703ace5294702b7023021af\nINFO[0129] Waiting for etcd to be healthy ...\nINFO[0131] etcd health check succeeded!\nINFO[0178] [Rollback Upgrade Tasks] Completed\nINFO[0178] Step 2 of 2: [Rollback Pre Upgrade Tasks]\nINFO[0178] [Rollback Pre Upgrade Tasks] Completed\nINFO[0178] Rollback to MKE version 3.7.15 completed successfully ...\nFATA[0178] Upgrade failed due to error: aborting upgrade due to signal interrupt\n</code></pre> <p>{{&lt; /details &gt;}}</p> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>A failed upgrade can leave behind an empty or corrupted <code>~/.mke/mke.kubeconf</code> file, which will block any subsequent upgrade attempts. To resolve this issue, manually delete the file: <pre><code>rm -f ~/.mke/mke.kubeconf\n</code></pre> Following this action, during the next upgrade, the system will automatically generate a valid configuration.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"upgrade-from-MKE-3x/rbac-upgrades/","title":"RBAC Upgrades","text":"<p>As MKE 4k does not support Swarm mode, the platform uses standard Kubernetes RBAC authorization. As such, the Swarm authorization configuration that is in place for MKE 3 is not present in MKE 4.</p>"},{"location":"upgrade-from-MKE-3x/rbac-upgrades/#groups","title":"Groups","text":"<p>To enable the same RBAC hierarchy as in MKE 3 with <code>orgs</code> and <code>teams</code> groups, but without the two-level limitation, MKE 4k replaces <code>orgs</code> and <code>teams</code> with the Kubernetes <code>AggregatedRoles</code>.</p> <p>Authorization structure comparison:</p> <pre><code>MKE 3:                           MKE 4:\n\n\u251c\u2500\u2500 entire-company (org)         \u251c\u2500\u2500 entire-company-org (AggregatedRole)\n\u2502   \u251c\u2500\u2500 development (team)       \u251c\u2500\u2500 development-team (AggregatedRole)\n\u2502   \u2502   \u251c\u2500\u2500 bob (user)           \u2502   \u251c\u2500\u2500 bob (user)\n\u2502   \u251c\u2500\u2500 production (team)        \u251c\u2500\u2500 production-team (AggregatedRole)\n\u2502   \u2502   \u251c\u2500\u2500 bob (user)\u2502          \u2502   \u251c\u2500\u2500 bob (user)\n\u2502   \u2502   \u251c\u2500\u2500 bill (user)          \u2502   \u251c\u2500\u2500 bill (user)\n\u2502   \u251c\u2500\u2500 sales (team)             \u251c\u2500\u2500 sales-team (AggregatedRole)\n</code></pre>"},{"location":"upgrade-from-MKE-3x/rbac-upgrades/#roles","title":"Roles","text":"<p>Roles are bound to the aggregated roles for integration into the org, team, and user structure. Thus, what was previously an organization or a team role will have <code>-org</code> or <code>-team</code> appended to its name.</p> <p>A role can be assigned at any level in the hierarchy, with its permissions granted to all members at that same level.</p> <p>Example organization binding:</p> <pre><code>\u251c\u2500\u2500 entire-company-org (AggregatedRole) -- entire-company-org (RoleBinding) -- view (Role)\n\u2502   \u251c\u2500\u2500 development-team (AggregatedRole)\n\u2502   \u2502   \u251c\u2500\u2500 bob (user)\n\u2502   \u251c\u2500\u2500 production-team (AggregatedRole)\n\u2502   \u2502   \u251c\u2500\u2500 bob (user)\n\u2502   \u2502   \u251c\u2500\u2500 bill (user)\n\u2502   \u251c\u2500\u2500 sales-team (AggregatedRole)\n</code></pre> <p>In the example above, all members of the <code>entire-company</code> org group have <code>view</code> permissions. This includes the <code>development-team</code>, <code>production-team</code>, <code>sales-team</code>, <code>bob</code>, and <code>bill</code>.</p> <p>Example team binding:</p> <pre><code>\u2502   \u251c\u2500\u2500 development:team (AggregatedRole) -- development:team (RoleBinding) -- edit (Role)\n\u2502   \u2502   \u251c\u2500\u2500 bob (user)\n</code></pre> <p>In the example above, the binding grants <code>edit</code> permissions only to the members of the development team, which only includes <code>bob</code>.</p> <p>{{&lt; callout type=\"warning\" &gt;}}</p> <p>Swarm roles are partially translated to Kubernetes roles. During upgrade, any detected Swarm role is replicated without permissions, thus preserving the org/team/user structure. If no Swarm roles are detected, a <code>none</code> role is created as a placeholder, as Kubernetes requires each aggregated role to have at least one role. This <code>none</code> role has no permissions, with its only purpose being to maintain structural integrity.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"upgrade-from-MKE-3x/revert-upgrade/","title":"Revert the Upgrade","text":"<p>To revert a cluster upgraded to MKE 4k back to MKE 3:</p> <ol> <li> <p>Uninstall MKE 4k.</p> </li> <li> <p>Restore MKE 3 from a backup.</p> </li> </ol>"},{"location":"upgrade-from-MKE-3x/troubleshoot-upgrade/","title":"Troubleshoot the Upgrade","text":"<p>You can address various potential MKE upgrade issues using the tips and suggestions detailed herein.</p> <p>{{&lt; callout type=\"important\" &gt;}}</p> <p>The MKE 3 <code>etcdv3</code> backend is not supported for upgrade to MKE 4k.</p> <p>{{&lt; /callout &gt;}}</p> <p>During the upgrade from MKE 3 to MKE 4, which defaults to the <code>etcdv3</code> backend, you may receive the following error:</p> <pre><code>mkectl upgrade --hosts-path hosts.yaml --mke3-admin-username admin --mke3-admin-password &lt;mke_admin_password&gt; -l debug --config-out new-mke4.yaml --external-address &lt;mke4_external_address&gt;\n...\nError: unable to generate upgrade config: unsupported configuration for mke4 upgrade: mke3 cluster is using etcdv3 and not kdd backend for calico\n</code></pre> <p>To resolve the issue, ensure that:</p> <ul> <li> <p>The MKE 3 source is an eligible MKE 3 cluster:</p> </li> <li> <p>MKE 3.7.15 or later</p> </li> <li> <p>Any MKE 3.8 release</p> </li> <li> <p>The <code>calico_kdd</code> flag in the MKE 3 configuration file is set to <code>true</code>.</p> </li> <li>The configuration is applied to the MKE 3 cluster.</li> </ul> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>A KDD mode upgrade is irreversible. Thus, to reduce risk, when upgrading enterprise clusters, it is recommended that you work directly with Mirantis to plan the process and monitor it through to completion.</p> <p>{{&lt; /callout &gt;}}</p> <p>Example output:</p> <pre><code>$ AUTHTOKEN=$(curl --silent --insecure --data '{\"username\":\"'$MKE_USERNAME'\",\"password\":\"'$MKE_PASSWORD'\"}' https://$MKE_HOST/auth/login | jq --raw-output .auth_token)\n\n$ curl --silent --insecure -X PUT -H \"accept: application/toml\" -H \"Authorization: Bearer $AUTHTOKEN\" --upload-file 'mke-config.toml' https://$MKE_HOST/api/ucp/config-toml\n{\"message\":\"Calico datastore upgrade from etcd to kdd successful\"}\n</code></pre> <p>{{&lt; callout type=\"important\" &gt;}}</p> <p>Refer to the Upgrade Prerequisites for information on how to migrate an etcd-backed cluster to KDD.</p> <p>{{&lt; /callout &gt;}}</p>"},{"location":"upgrade-from-MKE-3x/upgrade-considerations/","title":"Upgrade Considerations","text":"<p>Before you upgrade to MKE 4k, confirm the existence of a backup of your MKE 3 cluster and review the Back up MKE disaster recovery documentation for MKE 3.</p> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>The Swarm cluster and all associated artifacts are not included in the upgrade from MKE 3 to MKE 4k.</p> <p>{{&lt; /callout &gt;}}</p> <p>Back up all non-MKE components separately, making sure to check both manager and worker nodes, as these are at risk of being deleted rather than migrated during the upgrade to MKE 4k.</p>"},{"location":"upgrade-from-MKE-3x/upgrade-prerequisites/","title":"Upgrade Prerequisites","text":"<p>Verify that you have the following components in place before you begin upgrading MKE 3 to MKE 4k:</p> <ul> <li> <p>An eligible MKE 3 cluster:</p> </li> <li> <p>MKE 3.7.15 or later</p> </li> <li> <p>Any MKE 3.8 release</p> </li> <li> <p>A backup of the MKE cluster. For comprehensive instruction on how to create   an MKE 3 back up, refer to Back up MKE.</p> </li> <li> <p>The latest <code>mkectl</code> binary, installed on your local environment:</p> </li> </ul> <pre><code>mkectl version\n</code></pre> <p>Example output:</p> <pre><code>Version: v4.1.0\n</code></pre> <ul> <li>A hosts.yaml file, prepared as follows:</li> </ul> <p><pre><code>hosts:\n  - address: &lt;host1-external-ip&gt;\n    port: &lt;ssh-port&gt;\n    user: &lt;ssh-user&gt;\n    keyPath: &lt;path-to-ssh-key&gt;\n  - address: &lt;host2-external-ip&gt;\n    port: &lt;ssh-port&gt;\n    user: &lt;ssh-user&gt;\n    keyPath: &lt;path-to-ssh-key&gt;\n</code></pre>   {{&lt; callout type=\"important\" &gt;}}</p> <ul> <li>Confirm that all nodes are accessible via SSH, using the specified     credentials.</li> <li>Use either external IP addresses or FQDNs. These must be resolvable and   reachable from the system from which you are running mkectl. Mirantis   recommends that you use a load balancer is recommended for high-availability   setups.</li> <li>It is not necessary to declare a node role in the <code>hosts.yaml</code> file as they are   automatically detected during upgrade.</li> </ul> <p>{{&lt; /callout &gt;}}</p> <p> <p>- Calico migration to Kubernetes Datastore Driver (KDD) from etcd</p> <p>{{&lt; callout type=\"warning\" &gt;}}   To upgrade successfully to MKE 4k, the source MKE 3 cluster must be configured to use KDD.   {{&lt; /callout &gt;}}</p> <p>To migrate Calico to KDD from etcd:</p> <ol> <li> <p>Obtain the MKE 3 configuration file:      <pre><code>export MKE_USERNAME=&lt;mke-username&gt;\nexport MKE_PASSWORD=&lt;mke-password&gt;\nexport MKE_HOST=&lt;mke-fqdn-or-ip-address&gt;\nAUTHTOKEN=$(curl --silent --insecure --data '{\"username\":\"'$MKE_USERNAME'\",\"password\":\"'$MKE_PASSWORD'\"}' https://$MKE_HOST/auth/login | jq --raw-output .auth_token)\ncurl --silent --insecure -X GET \"https://$MKE_HOST/api/ucp/config-toml\" -H \"accept: application/toml\" -H \"Authorization: Bearer $AUTHTOKEN\" &gt; mke-config.toml\n</code></pre></p> </li> <li> <p>In the <code>cluster_config</code> section of the MKE 3 configuration file, check the setting of the <code>calico_kdd</code> parameter. If it is set to <code>true</code>, skip the remaining steps. Otherwise, edit the setting to <code>true</code>.</p> </li> <li> <p>Apply the modified MKE 3 configuration file:      <pre><code>$ curl --silent --insecure -X PUT -H \"accept: application/toml\" -H \"Authorization: Bearer $AUTHTOKEN\" --upload-file 'mke-config.toml' https://$MKE_HOST/api/ucp/config-toml\n</code></pre>      On completion, the following confirmation displays:      <pre><code>  {\"message\":\"Calico datastore upgrade from etcd to kdd successful\"}\n</code></pre></p> </li> </ol> <p>{{&lt; callout type=\"important\" &gt;}}   - The conversion of the Calico datastore from etcd to   KDD typically takes about 20 seconds per node, depending on the size of the cluster.   - According to Tigera, the conversion to KDD freezes cluster networking, and thus new or replacement pods are not able to start. Existing workloads, however, continue to run and their network connectivity is not impacted.   - The steps above must be completed as a standalone procedure before beginning the MKE4k upgrade process. The upgrade itself will be covered in the following sections.   - If your MKE 3 deployment uses an unmanaged CNI, this upgrade path is not currently supported.   - Support for unmanaged CNIs will be introduced in a future version of MKE.  In particular, Calico Enterprise employs Kubernetes as Calico Datastore, and thus the steps detailed herein are not required.   {{&lt; /callout &gt;}}</p>"},{"location":"upgrade-from-MKE-3x/upgrade-verification-access/","title":"Upgrade Verification and Access","text":"<p>{{&lt; callout type=\"info\" &gt;}}</p> <p>Typical upgrade durations, determined through controlled testing in an AWS environment*:</p> Node Configuration Detail Duration 5-node cluster 3 managers, 2 workers 10:19.87 minutes 10-node cluster 3 managers, 7 workers 11:26.64 minutes <p>These estimates are offered for general guidance, however, as actual upgrade durations will vary based on hardware performance (CPU/memory/disk), workload density, network throughput, and storage backend performance. For precise planning purposes, Mirantis strongly recommends that you run a test upgrade in a staging environment that mirrors your production specifications.</p> <p>* Ubuntu 22.04 LTS, manager and worker nodes (m5.2xlarge: 8 vCPU, 32GB RAM)</p> <p>{{&lt; /callout &gt;}}</p> <p>On completion of the <code>mkectl upgrade</code> command, a kubeconfig file for the default admin user is generated and stored at <code>~/.mke/mke.kubeconf</code>.</p> <p>Set a <code>KUBECONFIG</code> environment variable.</p> <pre><code>export KUBECONFIG=~/.mke/mke.kubeconf\n</code></pre> <p>Next, verify the MKE 4k cluster node readiness, cluster health, and workload status::</p> <ol> <li>Verify node readiness:</li> </ol> <pre><code>kubectl get nodes\n</code></pre> <ul> <li> <p>Healthy nodes should report <code>STATUS=Ready</code>.</p> <pre><code>kubectl describe node &lt;node-name&gt; | grep -i conditions: -A 10\n</code></pre> </li> <li> <p>Confirm the following conditions:</p> <ul> <li><code>Ready=True</code></li> <li><code>MemoryPressure/NetworkUnavailable/DiskPressure=False</code></li> </ul> </li> <li> <p>Verify workload status:</p> </li> </ul> <pre><code>kubectl get pods --all-namespaces\n</code></pre> <ul> <li> <p>Check columns for <code>STATUS=Running</code> and <code>READY</code></p> <pre><code>kubectl get deployments,statefulsets --all-namespaces\n</code></pre> </li> <li> <p>Confirm that <code>AVAILABLE</code> matches <code>DESIRED</code> replicas.</p> </li> <li> <p>Review the logs:</p> </li> </ul> <pre><code>kubectl get pods -n mke                # MKE namespace is mke\nkubectl logs &lt;pod-name&gt; -n mke         # Check logs for mke system pods\nkubectl logs &lt;pod-name&gt; -n &lt;namespace&gt; # Or any other application pods\n</code></pre> <ol> <li>Verify cluster health:</li> </ol> <p>```bash    kubectl top nodes # Resource usage    kubectl top pods -A</p> <p>{{&lt; callout type=\"info\" &gt;}}</p> <p>The MKE 3 cluster will no longer be accessible through the previously created client bundle. The docker swarm cluster will no longer be accessible as well.</p> <p>{{&lt; /callout &gt;}}</p>"}]}